#!/usr/bin/env python3
"""
Inference Script: infer_queries_linear_corrected.py

This script performs query optimization using:
1. Inductive Matrix Completion (GNIMC) with learned U_best and V_best matrices.
2. Per-hint linear residual models to refine GNIMC predictions.
3. Polynomial feature expansion + scaling consistent with training.
4. Selection of the best hint combination based on combined GNIMC + residual predictions.
5. Execution of selected plan with PostgreSQL's EXPLAIN ANALYZE for actual latency measurements.
6. Recording and saving results to a CSV file.

Ensure that all paths, model files, and PostgreSQL credentials are correctly set.
"""

import os
import sys
import re
import logging
import psycopg2
import sqlglot
import numpy as np
import pandas as pd
import joblib
import random
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression

##############################################################################
# -------------------------- CONFIGURE ENVIRONMENT -------------------------- #
##############################################################################

def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup once here
configure_environment()

##############################################################################
# -------------------------- CONFIGURE LOGGING -------------------------------#
##############################################################################

logging.basicConfig(
    level=logging.INFO,  # Set to INFO; use DEBUG for more verbosity
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

##############################################################################
# -------------------------- HINTS SETUP -------------------------------------#
##############################################################################

HINTS = [
    "enable_hashjoin",
    "enable_indexonlyscan",
    "enable_indexscan",
    "enable_mergejoin",
    "enable_nestloop",
    "enable_seqscan"
]

# All possible hint combinations (as strings)
# Ensure that COMBO_STRS lists all valid combinations (total of 49)
COMBO_STRS = [
    "hashjoin,indexonlyscan",
    "hashjoin,indexonlyscan,indexscan",
    "hashjoin,indexonlyscan,indexscan,mergejoin",
    "hashjoin,indexonlyscan,indexscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,indexscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,indexscan,nestloop",
    "hashjoin,indexonlyscan,indexscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,indexscan,seqscan",
    "hashjoin,indexonlyscan,mergejoin",
    "hashjoin,indexonlyscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexonlyscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,nestloop",
    "hashjoin,indexonlyscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,seqscan",
    "hashjoin,indexscan",
    "hashjoin,indexscan,mergejoin",
    "hashjoin,indexscan,mergejoin,nestloop",
    "hashjoin,indexscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexscan,mergejoin,seqscan",
    "hashjoin,indexscan,nestloop",
    "hashjoin,indexscan,nestloop,seqscan",
    "hashjoin,indexscan,seqscan",
    "hashjoin,mergejoin,nestloop,seqscan",
    "hashjoin,mergejoin,seqscan",
    "hashjoin,nestloop,seqscan",
    "hashjoin,seqscan",
    "indexonlyscan,indexscan,mergejoin",
    "indexonlyscan,indexscan,mergejoin,nestloop",
    "indexonlyscan,indexscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,indexscan,mergejoin,seqscan",
    "indexonlyscan,indexscan,nestloop",
    "indexonlyscan,indexscan,nestloop,seqscan",
    "indexonlyscan,mergejoin",
    "indexonlyscan,mergejoin,nestloop",
    "indexonlyscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,mergejoin,seqscan",
    "indexonlyscan,nestloop",
    "indexonlyscan,nestloop,seqscan",
    "indexscan,mergejoin",
    "indexscan,mergejoin,nestloop",
    "indexscan,mergejoin,nestloop,seqscan",
    "indexscan,mergejoin,seqscan",
    "indexscan,nestloop",
    "indexscan,nestloop,seqscan",
    "mergejoin,nestloop,seqscan",
    "mergejoin,seqscan",
    "nestloop,seqscan"
]

def build_hint_matrix_from_combos(combo_strs, hint_list):
    """
    Convert each combo string into a binary vector indicating enabled hints.
    Returns shape (N, len(hint_list)).
    """
    N = len(combo_strs)
    D = len(hint_list)
    Y = np.zeros((N, D), dtype=int)
    for i, c_str in enumerate(combo_strs):
        items = [x.strip() for x in c_str.split(",") if x.strip()]
        for item in items:
            enable_str = "enable_" + item
            if enable_str in hint_list:
                j = hint_list.index(enable_str)
                Y[i, j] = 1
            else:
                logger.warning(f"Unknown hint '{item}' in combo '{c_str}'.")
    return Y

##############################################################################
# -------------------------- POSTGRES INTERFACE ----------------------------- #
##############################################################################

def apply_plan_hints(cursor, plan_vector):
    """
    Apply the hint vector to the PostgreSQL session.
    """
    for i, hint_name in enumerate(HINTS):
        val = plan_vector[i]
        cmd = f"SET {hint_name} TO {'ON' if val == 1 else 'OFF'};"
        logger.debug(f"Applying plan hint: {cmd}")
        cursor.execute(cmd)

def reset_all_hints(cursor):
    """
    Reset all hints to their default settings.
    """
    cursor.execute("RESET ALL;")

def parse_explain_analyze_output(explain_output):
    """
    Parse the EXPLAIN ANALYZE output to extract total execution time in seconds.
    """
    total_time = None
    for row in explain_output:
        line = row[0]
        match = re.search(r"Execution Time:\s+([\d.]+)\s+ms", line)
        if match:
            ms = float(match.group(1))
            total_time = ms / 1000.0  # Convert to seconds
            break
    if total_time is None:
        logger.warning("Could not find 'Execution Time' in EXPLAIN output.")
        return float("inf")
    return total_time

def run_query_postgres_once_explain_analyze(query_str, plan_vector,
                                            pg_host, pg_db, pg_user, pg_password,
                                            port=5432):
    """
    Execute the query with the specified hint vector and return the latency.
    """
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host,
            dbname=pg_db,
            user=pg_user,
            password=pg_password,
            port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            apply_plan_hints(cur, plan_vector)
            explain_query = f"EXPLAIN ANALYZE {query_str}"
            logger.debug(f"Running EXPLAIN ANALYZE:\n{explain_query}")
            cur.execute(explain_query)
            explain_output = cur.fetchall()
            reset_all_hints(cur)
        return parse_explain_analyze_output(explain_output)
    except Exception as e:
        logger.error(f"Error running EXPLAIN ANALYZE: {e}")
        return float("inf")
    finally:
        if conn is not None:
            conn.close()

##############################################################################
# -------------------------- IMC MODEL CLASS -------------------------------- #
##############################################################################

class InductiveMCModel:
    """
    A class for Inductive Matrix Completion predictions.
    U shape: (d1, rank), V shape: (d2, rank).
    """

    def __init__(self, U, V):
        self.U = U  # Shape: (60, 5)
        self.V = V  # Shape: (6, 5)

    def predict_cost_matrix(self, A_query, B_combined):
        """
        Predict the cost matrix for a given query and all hint combinations.
        A_query: (1, d1)
        B_combined: (n_combos, d2)
        Returns:
            X_hat: (1, n_combos)
        """
        # Compute A_query @ U -> (1,60) @ (60,5) = (1,5)
        AU = A_query @ self.U  # Shape: (1, 5)
        logger.debug(f"AU shape: {AU.shape}")  # Expected: (1, 5)

        # Compute B_combined @ V -> (49,6) @ (6,5) = (49,5)
        BV = B_combined @ self.V  # Shape: (49,5)
        logger.debug(f"BV shape: {BV.shape}")  # Expected: (49,5)

        # Compute AU @ BV.T -> (1,5) @ (5,49) = (1,49)
        X_hat = AU @ BV.T  # Shape: (1,49)
        logger.debug(f"X_hat shape: {X_hat.shape}")  # Expected: (1,49)

        return X_hat  # (1,49)

##############################################################################
# -------------------------- FEATURE PREPARATION ---------------------------- #
##############################################################################

# Define the feature schema and scaling parameters as per training
FEATURE_SCHEMA_60 = [f"feat_{i}" for i in range(60)]
MANUAL_MAX = {f"feat_{i}": 50.0 for i in range(60)}
MANUAL_MAX["feat_0"] = 10.0  # Example custom max

def manual_min_max_scale(feats_dict_60):
    """
    Apply manual min-max scaling to the features.
    """
    scaled = []
    for key in FEATURE_SCHEMA_60:
        raw_val = feats_dict_60.get(key, 0.0)
        max_val = MANUAL_MAX.get(key, 50.0)
        clipped_val = min(raw_val, max_val)
        scaled_val = clipped_val / max_val if max_val > 0 else 0.0
        scaled.append(scaled_val)
    return np.array(scaled, dtype=float)

def prepare_single_query_hint_features(a_query_1d, b_hint_1d, poly, scaler):
    """
    Prepare the combined and scaled features for a single query-hint pair.
    """
    combined = np.hstack([a_query_1d, b_hint_1d]).reshape(1, -1)  # Shape: (1,65)
    feats_poly = poly.transform(combined)  # Shape: (1, n_poly_features)
    feats_scaled = scaler.transform(feats_poly)  # Shape: (1, n_poly_features)
    return feats_scaled

##############################################################################
# -------------------------- SQL FEATURE PARSING ---------------------------- #
##############################################################################

def parse_sql_60_features(query_str):
    """
    Parse a query and return exactly 60 features as defined in FEATURE_SCHEMA_60.
    """
    feats = {}
    try:
        # Parse the SQL using sqlglot
        statements = sqlglot.parse(query_str, dialect='postgres')
        if not statements:
            raise ValueError("No statements parsed.")
        stmt = statements[0]

        # Feature 0: Number of tables
        tables = [node.alias_or_name for node in stmt.find_all(sqlglot.expressions.Table)]
        feats["feat_0"] = len(tables)

        # Feature 1: Total joins
        joins = list(stmt.find_all(sqlglot.expressions.Join))
        feats["feat_1"] = len(joins)

        # Features 2-8: Specific join types
        join_types = {
            "INNER": 0,
            "LEFT": 0,
            "RIGHT": 0,
            "FULL": 0,
            "CROSS": 0,
            "LEFT_SEMI": 0,
            "LEFT_ANTI": 0
        }
        for join in joins:
            kind = join.args.get("kind", "INNER").upper()
            if "INNER" in kind:
                join_types["INNER"] += 1
            elif "LEFT SEMI" in kind:
                join_types["LEFT_SEMI"] += 1
            elif "LEFT ANTI" in kind:
                join_types["LEFT_ANTI"] += 1
            elif "LEFT" in kind:
                join_types["LEFT"] += 1
            elif "RIGHT" in kind:
                join_types["RIGHT"] += 1
            elif "FULL" in kind:
                join_types["FULL"] += 1
            elif "CROSS" in kind:
                join_types["CROSS"] += 1
            else:
                pass  # Handle other join types if necessary

        feats["feat_2"] = join_types["INNER"]
        feats["feat_3"] = join_types["LEFT"]
        feats["feat_4"] = join_types["RIGHT"]
        feats["feat_5"] = join_types["FULL"]
        feats["feat_6"] = join_types["CROSS"]
        feats["feat_7"] = join_types["LEFT_SEMI"]
        feats["feat_8"] = join_types["LEFT_ANTI"]

        # Features 9-11: Set operations
        set_ops = {"UNION": 0, "INTERSECT": 0, "EXCEPT": 0}
        for op in stmt.find_all(sqlglot.expressions.SetOperation):
            op_type = op.__class__.__name__.upper()
            if "UNION" in op_type:
                set_ops["UNION"] += 1
            elif "INTERSECT" in op_type:
                set_ops["INTERSECT"] += 1
            elif "EXCEPT" in op_type:
                set_ops["EXCEPT"] += 1
        feats["feat_9"] = set_ops["UNION"]
        feats["feat_10"] = set_ops["INTERSECT"]
        feats["feat_11"] = set_ops["EXCEPT"]

        # Features 12-17: Aggregation functions
        agg_funcs = {"COUNT": 0, "SUM": 0, "AVG": 0, "MIN": 0, "MAX": 0, "OTHER": 0}
        for func in stmt.find_all(sqlglot.expressions.Func):
            fname = func.name.upper() if func.name else "OTHER"
            if fname in agg_funcs:
                agg_funcs[fname] += 1
            else:
                agg_funcs["OTHER"] += 1
        feats["feat_12"] = agg_funcs["COUNT"]
        feats["feat_13"] = agg_funcs["SUM"]
        feats["feat_14"] = agg_funcs["AVG"]
        feats["feat_15"] = agg_funcs["MIN"]
        feats["feat_16"] = agg_funcs["MAX"]
        feats["feat_17"] = agg_funcs["OTHER"]

        # Feature 18: Number of DISTINCT clauses
        distinct_count = 0
        for select in stmt.find_all(sqlglot.expressions.Select):
            if select.args.get("distinct"):
                distinct_count += 1
        feats["feat_18"] = distinct_count

        # Features 19-20: LIMIT and OFFSET
        limit_count = len(list(stmt.find_all(sqlglot.expressions.Limit)))
        offset_count = len(list(stmt.find_all(sqlglot.expressions.Offset)))
        feats["feat_19"] = limit_count
        feats["feat_20"] = offset_count

        # Features 21-26: Aggregation functions (redundant; ensure consistency)
        feats["feat_21"] = agg_funcs["COUNT"]
        feats["feat_22"] = agg_funcs["SUM"]
        feats["feat_23"] = agg_funcs["AVG"]
        feats["feat_24"] = agg_funcs["MIN"]
        feats["feat_25"] = agg_funcs["MAX"]
        feats["feat_26"] = agg_funcs["OTHER"]

        # Feature 27: Number of columns accessed
        columns = set()
        for col in stmt.find_all(sqlglot.expressions.Column):
            columns.add(col.name)
        feats["feat_27"] = len(columns)

        # Features 28-29: Number of predicates and join predicates
        where_clause = stmt.find(sqlglot.expressions.Where)
        num_predicates = count_predicates(where_clause)
        feats["feat_28"] = num_predicates

        # Count join predicates (unique joins)
        join_predicates = 0
        if where_clause:
            for join in stmt.find_all(sqlglot.expressions.Join):
                join_predicates += 1  # Simplistic count; adjust as needed
        feats["feat_29"] = join_predicates

        # Features 30-56: Keyword frequencies (27 total)
        keyword_freq = count_keywords(query_str)
        for kw, count in keyword_freq.items():
            feat_index = get_feature_index_for_keyword(kw)  # Implemented below
            if feat_index is not None and 0 <= feat_index < 60:
                feats[f"feat_{feat_index}"] += count  # Accumulate counts
            else:
                logger.warning(f"Keyword '{kw}' not mapped to any feature index or exceeds feature limit.")

        # Features 57-58: Query length and token count
        tokenizer = sqlglot.tokens.Tokenizer(dialect='postgres')
        tokens = tokenizer.tokenize(query_str)
        feats["feat_57"] = len(tokens)
        feats["feat_58"] = len(query_str)

        # Feature 59: Subquery flag
        # Adjusted to 'has_subquery' as a binary feature
        subqueries = sum(
            1 for node in stmt.find_all(sqlglot.expressions.Select) if node is not stmt
        )
        feats["feat_59"] = 1.0 if subqueries > 0 else 0.0

        # Ensure all 60 features are present
        for i in range(60):
            key = f"feat_{i}"
            if key not in feats:
                feats[key] = 0.0

        logger.debug(f"Parsed SQL features: {feats}")
        return feats

    except Exception as e:
        logger.error(f"Error parsing SQL features: {e}")
        # Depending on your use case, you might want to either return default features or re-raise the exception
        # Here, we'll return a default feature dictionary with zeros
        return {f"feat_{i}": 0.0 for i in range(60)}

def count_keywords(query_str):
    """
    Count the frequency of specific SQL keywords in the query string.
    """
    keywords = [
        "SELECT", "FROM", "WHERE", "JOIN", "INNER", "LEFT", "RIGHT",
        "FULL", "ON", "GROUP BY", "ORDER BY", "HAVING", "LIMIT",
        "OFFSET", "UNION", "INTERSECT", "EXCEPT", "DISTINCT", "CASE",
        "WHEN", "THEN", "ELSE", "END", "LIKE", "IN", "BETWEEN"
    ]
    keyword_counts = {kw.lower().replace(" ", "_"): 0 for kw in keywords}
    for kw in keywords:
        pattern = re.compile(r'\b' + re.escape(kw) + r'\b', re.IGNORECASE)
        matches = pattern.findall(query_str)
        kw_key = kw.lower().replace(" ", "_")
        keyword_counts[kw_key] = len(matches)
        if len(matches) > 0:
            logger.debug(f"Keyword '{kw}' occurs {len(matches)} times.")
    return keyword_counts

def get_feature_index_for_keyword(keyword):
    """
    Map a keyword to its corresponding feature index.
    Implement this based on your FEATURE_SCHEMA_60.
    """
    keyword_to_feature = {
        "select": 30,
        "from": 31,
        "where": 32,
        "join": 33,
        "inner": 34,
        "left": 35,
        "right": 36,
        "full": 37,
        "on": 38,
        "group_by": 39,
        "order_by": 40,
        "having": 41,
        "limit": 42,
        "offset": 43,
        "union": 44,
        "intersect": 45,
        "except": 46,
        "distinct": 47,
        "case": 48,
        "when": 49,
        "then": 50,
        "else": 51,
        "end": 52,
        "like": 53,
        "in": 54,
        "between": 55
        # Add more mappings as needed up to feature 59
    }
    return keyword_to_feature.get(keyword.lower(), None)

def count_predicates(where_clause):
    """
    Count the number of predicates in the WHERE clause.
    Implement based on your feature extraction needs.
    """
    if not where_clause:
        return 0
    predicates = list(where_clause.find_all(sqlglot.expressions.Boolean))
    return len(predicates)

##############################################################################
# -------------------------- MAIN INFERENCE PIPELINE ------------------------ #
##############################################################################

def main():
    logger.info("Starting infer_queries_linear_corrected.py ...")

    # Paths
    MODEL_DIR = "/Users/raahimlone/New_Data"
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results_linear_corrected.csv"

    # Ensure output directory exists
    output_dir = os.path.dirname(OUTPUT_CSV)
    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
            logger.info(f"Created output directory: {output_dir}")
        except Exception as e:
            logger.error(f"Failed to create output directory '{output_dir}': {e}")
            sys.exit(1)

    # PostgreSQL connection details
    PG_HOST = "localhost"
    PG_DB   = "IMDB"
    PG_USER = "postgres"
    PG_PASSWORD = "raahimlhere"
    PG_PORT = 6543  # Ensure this is the correct port

    # 1) Load IMC factors
    try:
        U_best = np.load(os.path.join(MODEL_DIR, "U_best.npy"))  # Shape: (60, 5)
        V_best = np.load(os.path.join(MODEL_DIR, "V_best.npy"))  # Shape: (6, 5)
        logger.info(f"Loaded U_best: {U_best.shape}, V_best: {V_best.shape}")
    except Exception as e:
        logger.error(f"Failed to load U_best or V_best: {e}")
        sys.exit(1)

    imc_model = InductiveMCModel(U_best, V_best)

    # 2) Load side info for hints (Ensure correct file and key)
    try:
        B_data = np.load(os.path.join(MODEL_DIR, "Y_scaled2.npz"))  # Should contain 'Y' key
        logger.debug(f"Available keys in Y_scaled.npz: {B_data.files}")

        if 'Y' in B_data and B_data['Y'].shape == (6, 5):
            B_full = B_data['Y']  # Shape: (6, 5)
            logger.info(f"Loaded B_full: {B_full.shape}")  # Should be (6, 5)
        else:
            # Handle other scenarios or raise an error
            raise ValueError(f"Expected 'Y' key with shape (6,5), but got {B_data['Y'].shape if 'Y' in B_data else 'No Y key'}.")
    except Exception as e:
        logger.error(f"Failed to load Y_scaled.npz correctly: {e}")
        sys.exit(1)

    # 3) Load residual models
    try:
        residual_model_list = joblib.load(os.path.join(MODEL_DIR, "residual_model_linear_list.pkl"))  # List of 6 LinearRegression models
        poly = joblib.load(os.path.join(MODEL_DIR, "residual_model_poly.pkl"))
        scaler = joblib.load(os.path.join(MODEL_DIR, "residual_model_scaler.pkl"))
        logger.info("Loaded linear residual models, poly, and scaler.")
        assert len(residual_model_list) == len(HINTS), "Residual model list length does not match number of hints."
    except Exception as e:
        logger.error(f"Failed loading residual models: {e}")
        sys.exit(1)

    # 4) Build hint combination matrix
    Y_combos = build_hint_matrix_from_combos(COMBO_STRS, HINTS)  # Shape: (49,6)
    logger.info(f"Hint combination matrix shape: {Y_combos.shape}")  # Expected: (49,6)
    assert Y_combos.shape[0] == len(COMBO_STRS), "Hint combination matrix rows do not match number of COMBO_STRS."

    # 5) Gather SQL files
    if not os.path.isdir(QUERIES_DIR):
        logger.error(f"Queries directory not found: {QUERIES_DIR}")
        sys.exit(1)

    sql_files = sorted([f for f in os.listdir(QUERIES_DIR) if f.endswith(".sql")])
    logger.info(f"Found {len(sql_files)} SQL files to process.")

    results = []
    for fname in sql_files:
        fpath = os.path.join(QUERIES_DIR, fname)
        logger.info(f"Processing {fname}...")
        try:
            with open(fpath, "r", encoding="utf-8") as ff:
                query_str = ff.read().strip()
            if not query_str:
                logger.warning(f"{fname} => Empty query. Skipping.")
                continue

            # 5a) Parse query features
            feats_60_dict = parse_sql_60_features(query_str)  # Implemented as per user
            x_feat_60 = manual_min_max_scale(feats_60_dict).reshape(1, -1)  # Shape: (1,60)
            logger.debug(f"x_feat_60 shape: {x_feat_60.shape}")  # Expected: (1,60)

            # 5b) Prepare Hint Combination Features
            # Y_combos: (49,6), B_full: (6,5) â†’ B_combined: (49,5)
            B_combined = Y_combos @ B_full  # (49,6) @ (6,5) = (49,5)
            logger.debug(f"B_combined shape: {B_combined.shape}")  # Expected: (49,5)

            # 5c) IMC baseline predictions
            # Predict X_hat = A_query @ U_best @ V_best.T @ B_combined.T
            X_hat = imc_model.predict_cost_matrix(x_feat_60, B_combined)  # Shape: (1,49)
            logger.debug(f"X_hat: {X_hat}")  # (1,49)

            # 5d) Residual predictions
            # For each of the 49 hint combinations, predict residuals based on enabled hints
            residuals_pred = np.zeros((1, Y_combos.shape[0]))  # Shape: (1,49)
            for j in range(Y_combos.shape[0]):
                enabled_hints = Y_combos[j]  # Shape: (6,)
                if not np.any(enabled_hints):
                    continue  # No hints enabled, residual is 0
                # For each enabled hint, predict residual and sum
                residual_sum = 0.0
                for k in range(len(HINTS)):
                    if enabled_hints[k]:
                        # Prepare features for this query-hint pair
                        # a_query_1d: (60,)
                        # b_hint_1d: (5,)
                        a_query_1d = x_feat_60.flatten()  # Shape: (60,)
                        b_hint_1d = B_full[k]            # Shape: (5,)
                        features_scaled = prepare_single_query_hint_features(
                            a_query_1d=a_query_1d,
                            b_hint_1d=b_hint_1d,
                            poly=poly,
                            scaler=scaler
                        )  # Shape: (1, n_poly_features)
                        residual = residual_model_list[k].predict(features_scaled)[0]
                        residual_sum += residual
                residuals_pred[0, j] = residual_sum
                logger.debug(f"Residual for combo {j}: {residual_sum}")

            logger.debug(f"Residuals_pred: {residuals_pred}")  # (1,49)

            # 5e) Combine IMC + residuals
            adjusted_X_hat = X_hat + residuals_pred  # Shape: (1,49)
            logger.debug(f"Adjusted_X_hat: {adjusted_X_hat}")  # (1,49)

            # 5f) Select the best hint combination
            # Find the minimum cost
            best_cost = adjusted_X_hat.min()
            # Define threshold within 5% of the best cost
            threshold = best_cost * 1.05
            candidate_idxs = np.where(adjusted_X_hat <= threshold)[1]  # Shape: (n_candidates,)

            if candidate_idxs.size == 0:
                logger.warning(f"No candidate combos found within 5% of the best cost for {fname}.")
                chosen_idx = np.argmin(adjusted_X_hat)
            else:
                chosen_idx = random.choice(candidate_idxs)
            chosen_cost = adjusted_X_hat[0, chosen_idx]
            chosen_plan_vector = Y_combos[chosen_idx]  # Binary vector indicating enabled hints

            logger.debug(
                f"{fname} => Chosen combo idx: {chosen_idx}, Cost: {chosen_cost:.6f}, "
                f"Combo: {COMBO_STRS[chosen_idx]}"
            )

            # 5g) Execute EXPLAIN ANALYZE with the chosen hint combination
            latency_s = run_query_postgres_once_explain_analyze(
                query_str,
                chosen_plan_vector,
                PG_HOST, PG_DB, PG_USER, PG_PASSWORD,
                port=PG_PORT
            )
            if latency_s == float("inf"):
                logger.warning(f"{fname} => Invalid or failed EXPLAIN. Skipping.")
                results.append({
                    "filename": fname,
                    "best_combo_idx": chosen_idx,
                    "cost_estimate": chosen_cost,
                    "latency_s": None,
                    "hints": COMBO_STRS[chosen_idx],
                    "status": "Failed"
                })
                continue

            # Decode chosen hints (optional: already represented in COMBO_STRS)
            enabled_hints = [hint for hint, enabled in zip(HINTS, chosen_plan_vector) if enabled]

            results.append({
                "filename": fname,
                "best_combo_idx": chosen_idx,
                "cost_estimate": chosen_cost,
                "latency_s": latency_s,
                "hints": COMBO_STRS[chosen_idx],
                "status": "Success"
            })
            logger.info(
                f"{fname} => Best Combo: {chosen_idx} ({COMBO_STRS[chosen_idx]}), "
                f"Cost Estimate: {chosen_cost:.4f}, Latency: {latency_s:.4f}s"
            )

        except Exception as e:
            logger.error(f"Error processing {fname}: {e}")
            results.append({
                "filename": fname,
                "best_combo_idx": None,
                "cost_estimate": None,
                "latency_s": None,
                "hints": None,
                "status": f"Error: {str(e)}"
            })

    # 6) Write results to CSV
    if results:
        try:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_CSV, index=False)
            logger.info(f"Saved results to {OUTPUT_CSV}")
        except Exception as e:
            logger.error(f"Failed to save CSV results: {e}")

        # Summary
        logger.info("Inference complete. Summary:")
        for r in results:
            if r["status"] == "Success":
                logger.info(
                    f"{r['filename']}: Best Combo idx={r['best_combo_idx']} "
                    f"({r['hints']}), Cost Estimate={r['cost_estimate']:.4f}, "
                    f"Latency={r['latency_s']:.4f}s"
                )
            else:
                logger.info(f"{r['filename']}: {r['status']}")

##############################################################################
# -------------------------- ENTRY POINT ------------------------------------- #
##############################################################################

if __name__ == "__main__":
    main()
