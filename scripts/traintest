#!/usr/bin/env python3
"""
Inference for LimeQO / IMC with *exact* training alignment.
Loads scalers, matrices, and hint-toggle order straight from the files created
during training (Y_scalednew.npz + y.parquet).  No hard-coded combo list.
"""

import os, sys, logging, re
import numpy as np
import pandas as pd
import psycopg2, joblib, sqlglot
from sqlglot import exp
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# ─────────────────────────────────────────────────────────────────────────────
#  0. paths & config
# ─────────────────────────────────────────────────────────────────────────────
MODEL_DIR   = "/Users/raahimlone/New_Data"
QUERY_DIR   = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
OUTPUT_CSV  = "/Users/raahimlone/rahhh/results_limeqo.csv"

PG_HOST, PG_DB  = "localhost", "IMDB"
PG_USER, PG_PW  = "postgres", "raahimhere"
PG_PORT         = 6543          # <-- keep your custom port

# ─────────────────────────────────────────────────────────────────────────────
#  1. logging
# ─────────────────────────────────────────────────────────────────────────────
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s [%(levelname)s] %(message)s",
                    handlers=[logging.StreamHandler()])
log = logging.getLogger(__name__)

# ─────────────────────────────────────────────────────────────────────────────
#  2. load ALL saved artifacts
# ─────────────────────────────────────────────────────────────────────────────
try:
    query_scaler    = joblib.load(f"{MODEL_DIR}/scaler.joblib")
    residual_scaler = joblib.load(f"{MODEL_DIR}/residual_model_scaler.pkl")
    residual_model  = joblib.load(f"{MODEL_DIR}/residual_model_xgb.pkl")
    poly            = joblib.load(f"{MODEL_DIR}/residual_model_poly.pkl")
    U               = np.load(f"{MODEL_DIR}/U_best.npy")
    V               = np.load(f"{MODEL_DIR}/V_best.npy")
    B               = np.load(f"{MODEL_DIR}/Y_scalednew.npz")["Y"]    # (n_candidates, d2)
except Exception as e:
    log.error(f"❌ cannot load core artifacts: {e}")
    sys.exit(1)

# ── load y.parquet (un-scaled hint feature rows) ────────────────────────────
try:
    hint_df = pd.read_parquet(os.path.join(MODEL_DIR, "y.parquet"))
except Exception as e:
    log.error(f"❌ cannot load y.parquet: {e}")
    sys.exit(1)

# columns that hold ON/OFF toggles (edit if your column names differ)
TOGGLE_COLS = [
    "hashjoin",
    "indexonlyscan",
    "indexscan",
    "mergejoin",
    "nestloop",
    "seqscan",
]
missing = [c for c in TOGGLE_COLS if c not in hint_df.columns]
if missing:
    log.error(f"🚫 toggle columns missing in y.parquet: {missing}")
    sys.exit(1)

# create human-readable combo strings in SAME row order as B and hint_df
def row_to_combo(row):
    enabled = [c.replace("enable_", "") for c in TOGGLE_COLS if row[c] >= 0.5]
    return ",".join(enabled) if enabled else "native"

combo_strings = [row_to_combo(r) for _, r in hint_df[TOGGLE_COLS].iterrows()]

# sanity: row counts must match
if len(combo_strings) != B.shape[0]:
    log.error("🚫 Row count mismatch between y.parquet and Y_scalednew.npz")
    sys.exit(1)

log.info(f"✅ artifacts loaded; {len(combo_strings)} candidate plans")

d1, d2 = U.shape[0], V.shape[0]             # 25 query feats,  d2 hint feature dim
n_candidates = B.shape[0]

# ─────────────────────────────────────────────────────────────────────────────
#  3. IMC + residual wrapper
# ─────────────────────────────────────────────────────────────────────────────
class IMCResidual:
    def __init__(self, U, V, poly, res_scaler, res_model):
        self.Z           = U @ V.T              # (d1,d2)
        self.poly        = poly
        self.res_scaler  = res_scaler
        self.res_model   = res_model

    def predict(self, X_query):                 # X_query : (N,d1)
        N = X_query.shape[0]
        base  = X_query @ self.Z                # (N,d2)
        init  = base @ B.T                      # (N,n_candidates)

        X_rep = np.repeat(X_query, n_candidates, axis=0)
        B_rep = np.tile(B, (N,1))
        feats = np.hstack((X_rep, B_rep))
        feats_poly   = self.poly.transform(feats)
        feats_scaled = self.res_scaler.transform(feats_poly)
        resid = self.res_model.predict(feats_scaled)

        return init.flatten() + resid           # (N*n_candidates,)

imc = IMCResidual(U, V, poly, residual_scaler, residual_model)

# ─────────────────────────────────────────────────────────────────────────────
#  4. SQL feature extraction (25 keys)
# ─────────────────────────────────────────────────────────────────────────────
feat_cols = [c.strip() for c in open(f"{MODEL_DIR}/feature_columns.txt")]

def parse_sql(q: str):
    """Return dict with exactly feat_cols; on failure returns zeros."""
    try:
        stmt = sqlglot.parse_one(q, dialect="postgres")
        amap = {t.alias_or_name: t.name for t in stmt.find_all(exp.Table)}
        where = stmt.find(exp.Where)
        select = stmt.find(exp.Select)
        sel_ex = select.expressions if select else []
        joins  = list(stmt.find_all(exp.Join))

        def pcnt():
            if where is None: return 0
            kinds=(exp.EQ,exp.GT,exp.LT,exp.GTE,exp.LTE,exp.NEQ,exp.Like,exp.ILike,exp.In,exp.Between)
            return sum(1 for _ in where.find_all(*kinds))
        def jcnt():
            if where is None: return 0
            pairs=set()
            for eq in where.find_all(exp.EQ):
                L,R=eq.left,eq.right
                if isinstance(L,exp.Column) and isinstance(R,exp.Column):
                    lt,rt=amap.get(L.table),amap.get(R.table)
                    if lt and rt and lt!=rt: pairs.add(tuple(sorted((lt,rt))))
            return len(pairs)
        def aggcnt():
            agg={"SUM","COUNT","AVG","MIN","MAX","MEDIAN","MODE"}
            c=0
            for e in sel_ex:
                if isinstance(e,exp.Alias): e=e.this
                if isinstance(e,exp.Func) and e.name.upper() in agg: c+=1
            return c
        def logic():
            return 0 if where is None else sum(1 for _ in where.find_all(exp.And,exp.Or,exp.Not))
        def comp():
            kinds=(exp.EQ,exp.GT,exp.LT,exp.GTE,exp.LTE,exp.NEQ)
            return 0 if where is None else sum(1 for _ in where.find_all(*kinds))
        def gcols(): g=stmt.find(exp.Group); return 0 if g is None else sum(1 for _ in g.find_all(exp.Column))
        def ocols(): o=stmt.find(exp.Order); return 0 if o is None else sum(1 for _ in o.find_all(exp.Ordered))
        def nested(): return max(0,len(list(stmt.find_all(exp.Select)))-1)
        def corr():
            outer={t.alias_or_name for t in stmt.find_all(exp.Table)}
            for sub in stmt.find_all(exp.Select):
                if sub is stmt: continue
                for c in sub.find_all(exp.Column):
                    if c.table in outer: return 1.0
            return 0.0
        def jkind(k): return sum(1 for j in joins if j.args.get("kind","").upper()==k)

        d = {
            "num_tables": len(list(stmt.find_all(exp.Table))),
            "num_joins": jcnt(),
            "num_predicates": pcnt(),
            "has_group_by": 1.0 if stmt.find(exp.Group) else 0.0,
            "has_order_by": 1.0 if stmt.find(exp.Order) else 0.0,
            "has_subquery": 1.0 if nested()>0 else 0.0,
            "query_length": len(q),
            "num_tokens": len(list(stmt.walk())),
            "num_select_expressions": len(sel_ex),
            "has_distinct": 1.0 if select and select.args.get("distinct") else 0.0,
            "num_aggregate_functions": aggcnt(),
            "num_logical_operators": logic(),
            "num_comparison_operators": comp(),
            "num_group_by_columns": gcols(),
            "num_order_by_columns": ocols(),
            "num_nested_subqueries": nested(),
            "has_correlated_subqueries": corr(),
            "num_inner_joins": jkind("INNER"),
            "num_left_joins":  jkind("LEFT"),
            "num_right_joins": jkind("RIGHT"),
            "num_full_outer_joins": jkind("FULL OUTER"),
            "has_limit": 1.0 if stmt.find(exp.Limit) else 0.0,
            "has_union": 1.0 if stmt.find(exp.Union) else 0.0,
            "num_union_operations": sum(1 for _ in stmt.find_all(exp.Union)),
            "num_case_statements": sum(1 for _ in stmt.find_all(exp.Case)),
        }
        return d
    except Exception as e:
        log.error(f"parse_sql error: {e}")
        return {k:0.0 for k in feat_cols}

# ─────────────────────────────────────────────────────────────────────────────
#  5. collect & scale query features
# ─────────────────────────────────────────────────────────────────────────────
feature_rows, sql_files = [], []
for fn in sorted(os.listdir(QUERY_DIR)):
    if fn.endswith(".sql"):
        sql_text = open(os.path.join(QUERY_DIR, fn)).read().strip()
        if sql_text:
            feature_rows.append(parse_sql(sql_text))
            sql_files.append((fn, sql_text))

if not feature_rows:
    log.error("❌ no .sql files found")
    sys.exit(1)

df        = pd.DataFrame(feature_rows).reindex(columns=feat_cols).fillna(0.0)
X_scaled  = query_scaler.transform(df)
N_queries = X_scaled.shape[0]

log.info(f"🛈 Parsed and scaled {N_queries} queries")

# ─────────────────────────────────────────────────────────────────────────────
#  6. predict & pick best candidate per query
# ─────────────────────────────────────────────────────────────────────────────
scores = imc.predict(X_scaled).reshape(N_queries, n_candidates)
best_idx = np.argmin(scores, axis=1)

# ─────────────────────────────────────────────────────────────────────────────
#  7. run each query in Postgres with chosen plan
# ─────────────────────────────────────────────────────────────────────────────
def exec_with_plan(sql_str, cand_idx):
    # 0/1 toggle vector from y.parquet
    plan_vec = hint_df.loc[cand_idx, TOGGLE_COLS].to_numpy(dtype=float)

    conn = psycopg2.connect(host=PG_HOST, dbname=PG_DB,
                            user=PG_USER, password=PG_PW, port=PG_PORT)
    conn.autocommit = True
    try:
        with conn.cursor() as cur:
            for v, col in zip(plan_vec, TOGGLE_COLS):
                guc = col if col.startswith("enable_") else f"enable_{col}"
                cur.execute(f"SET {guc} TO {'ON' if v >= 0.5 else 'OFF'};")
            cur.execute(f"EXPLAIN ANALYZE {sql_str}")
            out = cur.fetchall()
            cur.execute("RESET ALL;")

        # extract latency (ms or s)
        for (ln,) in out:
            if m := re.search(r"Execution Time:\s+([\d.]+)\s+ms", ln):
                return float(m.group(1)) / 1000.0
            if m := re.search(r"Execution Time:\s+([\d.]+)\s+s", ln):
                return float(m.group(1))
        return float("inf")

    except Exception as e:
        log.error(f"PG error: {e}")
        return float("inf")
    finally:
        conn.close()
results = []
for (fn, sql_text), idx in zip(sql_files, best_idx):
    lat = exec_with_plan(sql_text, int(idx))
    row = {
        "filename":   fn,
        "cand_idx":   int(idx),
        "combo":      combo_strings[idx],
        "pred_cost":  float(scores[sql_files.index((fn,sql_text)), idx]),
        "latency":    lat,
    }
    results.append(row)
    log.info(f"{fn}: {combo_strings[idx]}  →  {lat:.3f}s")

pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)
log.info(f"✅ results saved to {OUTPUT_CSV}")
