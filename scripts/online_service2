# parse_queries.py
#!/usr/bin/env python3
"""
Full Script Combining:
1) Reading queries -> extracting exactly 60 features from parse_sql.py
2) Manual minâ€“max scaling => x_feat_60 shape (60,).
3) IMC model with W(60,5) & H(6,5).
4) Polynomial feature expansion => 2,000+ features for the residual XGBoost model.
5) Query execution in PostgreSQL using EXPLAIN ANALYZE for latency measurements.
"""
# parse_queries.py

def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from `src`
configure_environment()

import os
import sys
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
import joblib
import re  # Added for parsing EXPLAIN ANALYZE output

# Import our parse_sql_60_features function
from src.data_io.sql_parser import parse_sql_60_features, FEATURE_SCHEMA_60

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

assert len(FEATURE_SCHEMA_60) == 60, "We want exactly 60 features."

class MaskedALSNonnegative:
    """
    W: shape (60,5)
    H: shape (6,5)
    """
    def __init__(self, W, H):
        self.W = np.clip(W, 0, None)  # (60,5)
        self.H = np.clip(H, 0, None)  # (6,5)
        self.logger = logging.getLogger(__name__)

    def predict_single(self, x_feat_60, y_feat_6):
        """
        x_feat_60: shape(60,)
        y_feat_6 : shape(6,)
        => q_latent: (5,) => h_latent: (5,) => cost: float
        """
        q_latent = x_feat_60 @ self.W      # (60,) x (60,5) => (5,)
        h_latent = y_feat_6 @ self.H       # (6,)  x (6,5)  => (5,)
        return float(np.dot(q_latent, h_latent))

HINTS = [
    "enable_hashjoin",
    "enable_indexonlyscan",
    "enable_indexscan",
    "enable_mergejoin",
    "enable_nestloop",
    "enable_seqscan"
]

# Custom Hint Configurations
COMBO_STRS = [
    "",
    "hashjoin,indexonlyscan",
    "hashjoin,indexonlyscan,indexscan",
    "hashjoin,indexonlyscan,indexscan,mergejoin",
    "hashjoin,indexonlyscan,indexscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,indexscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,indexscan,nestloop",
    "hashjoin,indexonlyscan,indexscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,indexscan,seqscan",
    "hashjoin,indexonlyscan,mergejoin",
    "hashjoin,indexonlyscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexonlyscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,nestloop",
    "hashjoin,indexonlyscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,seqscan",
    "hashjoin,indexscan",
    "hashjoin,indexscan,mergejoin",
    "hashjoin,indexscan,mergejoin,nestloop",
    "hashjoin,indexscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexscan,mergejoin,seqscan",
    "hashjoin,indexscan,nestloop",
    "hashjoin,indexscan,nestloop,seqscan",
    "hashjoin,indexscan,seqscan",
    "hashjoin,mergejoin,nestloop,seqscan",
    "hashjoin,mergejoin,seqscan",
    "hashjoin,nestloop,seqscan",
    "hashjoin,seqscan",
    "indexonlyscan,indexscan,mergejoin",
    "indexonlyscan,indexscan,mergejoin,nestloop",
    "indexonlyscan,indexscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,indexscan,mergejoin,seqscan",
    "indexonlyscan,indexscan,nestloop",
    "indexonlyscan,indexscan,nestloop,seqscan",
    "indexonlyscan,mergejoin",
    "indexonlyscan,mergejoin,nestloop",
    "indexonlyscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,mergejoin,seqscan",
    "indexonlyscan,nestloop",
    "indexonlyscan,nestloop,seqscan",
    "indexscan,mergejoin",
    "indexscan,mergejoin,nestloop",
    "indexscan,mergejoin,nestloop,seqscan",
    "indexscan,mergejoin,seqscan",
    "indexscan,nestloop",
    "indexscan,nestloop,seqscan",
    "mergejoin,nestloop,seqscan",
    "mergejoin,seqscan",
    "nestloop,seqscan"
]


def build_hint_matrix_from_combos(combo_strs, hint_list):
    N = len(combo_strs)
    D = len(hint_list)
    Y = np.zeros((N, D), dtype=int)
    for i, c_str in enumerate(combo_strs):
        if c_str == "":
            continue  # No hints enabled
        items = [x.strip() for x in c_str.split(",")]
        for item in items:
            enable_str = "enable_" + item
            if enable_str in hint_list:
                j = hint_list.index(enable_str)
                Y[i, j] = 1
    return Y

def apply_plan_hints(cursor, plan_vector):
    for i, hint_name in enumerate(HINTS):
        val = plan_vector[i]
        cmd = f"SET {hint_name} TO {'ON' if val >= 0.5 else 'OFF'};"
        cursor.execute(cmd)

def reset_all_hints(cursor):
    cursor.execute("RESET ALL;")

def run_query_postgres_once_explain_analyze(query_str, plan_vector, pg_host, pg_db, pg_user, pg_password, port=5432):
    """
    Executes EXPLAIN ANALYZE on the given query with specified plan hints and returns the total execution time in seconds.
    """
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host, dbname=pg_db, user=pg_user,
            password=pg_password, port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            apply_plan_hints(cur, plan_vector)
            explain_query = f"EXPLAIN ANALYZE {query_str}"
            cur.execute(explain_query)
            explain_output = cur.fetchall()
            reset_all_hints(cur)

            # Parse the execution time from EXPLAIN ANALYZE output
            execution_time = parse_explain_analyze_output(explain_output)
            return execution_time
    except Exception as e:
        logger.error(f"Error executing EXPLAIN ANALYZE: {e}")
        return float("inf")  # Return a large number to indicate failure
    finally:
        if conn:
            conn.close()

def parse_explain_analyze_output(explain_output):
    """
    Parses the EXPLAIN ANALYZE output to extract the total execution time in seconds.
    """
    total_time = None
    # EXPLAIN ANALYZE output is a list of tuples with one string element each
    for row in explain_output:
        line = row[0]
        # Look for the line that starts with "Execution Time:"
        match = re.search(r"Execution Time:\s+([\d.]+) ms", line)
        if match:
            total_time_ms = float(match.group(1))
            total_time = total_time_ms / 1000.0  # Convert to seconds
            break
    if total_time is None:
        logger.warning("Could not find Execution Time in EXPLAIN ANALYZE output.")
        return float("inf")  # Indicate failure to parse
    return total_time

def prepare_single_query_features(x_feat_60, y_feat_6, poly, scaler):
    """
    Combine => (66,) => polynomial => scaled => shape(1, 2000+) ...
    """
    combined = np.hstack([x_feat_60, y_feat_6]).reshape(1, -1)
    feats_poly = poly.transform(combined)
    feats_scaled = scaler.transform(feats_poly)
    return feats_scaled

def main():
    MODEL_DIR = "/Users/raahimlone/New_Data"
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results.csv"

    PG_HOST = "localhost"
    PG_DB = "IMDB"
    PG_USER = "postgres"
    PG_PASSWORD = "raahimlhere"
    PG_PORT = 6543

    # 1) Load IMC model: W(60,5) & H(6,5)
    try:
        W_path = os.path.join(MODEL_DIR, "U_best.npy")  
        H_path = os.path.join(MODEL_DIR, "V_best.npy")  
        W = np.load(W_path)  # shape (60,5)
        H = np.load(H_path)  # shape (6,5)
        logger.info(f"Loaded W: {W.shape}, H: {H.shape}")
    except Exception as e:
        logger.error(f"Could not load IMC model: {e}")
        return

    model_imc = MaskedALSNonnegative(W, H)

    # 2) Build Y combos => shape(N,6)
    Y = build_hint_matrix_from_combos(COMBO_STRS, HINTS)
    logger.info(f"Y combos => shape={Y.shape}")

    # 3) (Optional) Load residual pipeline
    try:
        rmodel_path = os.path.join(MODEL_DIR, "residual_model_xgb.pkl")
        poly_path   = os.path.join(MODEL_DIR, "residual_model_poly.pkl")
        scaler_path = os.path.join(MODEL_DIR, "residual_model_scaler.pkl")

        residual_model = joblib.load(rmodel_path)
        poly = joblib.load(poly_path)
        scaler = joblib.load(scaler_path)
        logger.info("Loaded XGBoost residual model & polynomial pipeline.")
    except Exception as e:
        logger.warning(f"No residual model pipeline found => using IMC only. {e}")
        residual_model, poly, scaler = None, None, None

    # 4) Manual minâ€“max for 60 features (Example only)
    MANUAL_MAX = {
        "num_tables": 10,
        "total_joins": 50,
        "num_inner_joins": 50,
        "num_left_joins": 50,
        "num_right_joins": 50,
        "num_full_joins": 50,
        "num_cross_joins": 50,
        "num_semi_joins": 50,
        "num_anti_joins": 50,
        "num_other_joins": 50,
        "num_union": 10,
        "num_intersect": 10,
        "num_except": 10,
        "num_agg_count": 50,
        "num_agg_sum": 50,
        "num_agg_avg": 50,
        "num_agg_min": 50,
        "num_agg_max": 50,
        "num_agg_other": 50,
        "num_distinct": 10,
        "num_limit": 10,
        "num_offset": 10,
        "func_unknown": 50,  # etc...
    }

    def manual_min_max_scale(feats_dict_60):
        scaled = []
        for key in FEATURE_SCHEMA_60:
            raw_val = feats_dict_60.get(key, 0)
            max_val = MANUAL_MAX.get(key, 50)  # default=50
            if max_val > 0:
                scaled.append(min(raw_val, max_val) / max_val)
            else:
                scaled.append(0.0)
        return np.array(scaled, dtype=float)

    # 5) Process queries
    if not os.path.isdir(QUERIES_DIR):
        logger.error(f"Queries dir not found: {QUERIES_DIR}")
        return

    sql_files = [f for f in os.listdir(QUERIES_DIR) if f.endswith(".sql")]
    sql_files.sort()
    logger.info(f"Found {len(sql_files)} SQL files to process.")

    results = []
    for fname in sql_files:
        fpath = os.path.join(QUERIES_DIR, fname)
        logger.info(f"Processing {fname}...")
        try:
            with open(fpath, "r", encoding="utf-8") as ff:
                query_str = ff.read().strip()
            if not query_str:
                logger.warning(f"{fname} => empty query.")
                continue

            # === Parse => EXACT 60 features
            feats_60_dict = parse_sql_60_features(query_str)

            # === Scale => x_feat_60 (shape (60,))
            x_feat_60 = manual_min_max_scale(feats_60_dict)

            # === Evaluate each hint plan
            best_idx = None
            best_cost = float("inf")
            logger.info(f"\nðŸ“Š Predicted Latencies for Query: {fname}")
            all_latencies = []

            for i in range(Y.shape[0]):
                y_feat_6 = Y[i, :]
                # 1) IMC cost
                cost_imc = model_imc.predict_single(x_feat_60, y_feat_6)
                # 2) Residual
                residual_pred = 0.0
                if residual_model and poly and scaler:
                    big_feats  = prepare_single_query_features(x_feat_60, y_feat_6, poly, scaler)
                    residual_pred = residual_model.predict(big_feats)[0]
                final_cost = cost_imc + residual_pred
                if final_cost < best_cost:
                    best_cost = final_cost
                    best_idx = i
                all_latencies.append(final_cost)
                logger.info(f"Plan {i}: Cost = {final_cost:.4f}, Hints = {Y[i]}")

            if best_idx is None:
                logger.warning(f"{fname} => no feasible plan found!")
                continue

            chosen_plan = Y[best_idx, :]  # (6,)
            
            # Execute EXPLAIN ANALYZE to get actual latency
            latency = run_query_postgres_once_explain_analyze(
                query_str=query_str,
                plan_vector=chosen_plan,
                pg_host=PG_HOST, pg_db=PG_DB,
                pg_user=PG_USER, pg_password=PG_PASSWORD,
                port=PG_PORT
            )
            
            if latency == float("inf") or latency <= 0:
                logger.warning(f"{fname} => invalid or failed latency measurement: {latency}")
                continue

            # Decode chosen hints
            enabled_hints = []
            for j, hint_name in enumerate(HINTS):
                if chosen_plan[j] == 1:
                    short = hint_name.replace("enable_", "")
                    enabled_hints.append(short)
            hint_str = ",".join(enabled_hints) if enabled_hints else "Baseline (no hints)"

            logger.info(
                f"{fname} => best_idx={best_idx}, cost={best_cost:.4f}, "
                f"latency={latency:.4f}s, hints=[{hint_str}]"
            )
            results.append({
                "filename": fname,
                "best_index": best_idx,
                "cost_estimate": best_cost,
                "latency_s": latency,
                "hints": hint_str
            })

        except Exception as e:
            logger.error(f"Error processing {fname}: {e}")
            results.append({"filename": fname, "error": str(e)})

    # 6) Save results
    if OUTPUT_CSV and OUTPUT_CSV.strip():
        try:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_CSV, index=False)
            logger.info(f"Results saved => {OUTPUT_CSV}")
        except Exception as e:
            logger.error(f"Failed saving CSV => {e}")

    # 7) Print summary
    if results:
        print("\n=== Query Optimization Results ===")
        for r in results:
            if "error" in r:
                print(f"{r['filename']} => Error: {r['error']}")
            else:
                print(
                    f"{r['filename']} => best_index={r['best_index']}, "
                    f"cost={r['cost_estimate']:.4f}, latency={r['latency_s']:.4f}s, "
                    f"hints=[{r['hints']}]"
                )
    else:
        print("No results to display.")
    logger.info("All queries processed.")
    
    import matplotlib.pyplot as plt

    def visualize_hint_matrix(Y, best_idx):
        """
        Visualize the Y matrix and highlight the selected row with minimum cost.
        """
        plt.figure(figsize=(10, 6))
        plt.imshow(Y, cmap='gray', aspect='auto')
        plt.colorbar(label='Hint Enabled (0=OFF, 1=ON)')
        plt.title('Hint Combination Matrix (Y)')
        plt.xlabel('Hint Types')
        plt.ylabel('Plan Index')

        # Highlight the best selected row
        plt.axhline(y=best_idx, color='red', linestyle='--', label='Best Plan')
        plt.legend()
        plt.show()

if __name__ == "__main__":
    main()

'''
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from `src`
configure_environment()

import os
import sys
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
import joblib

# Import our parse_sql_60_features function
from src.data_io.sql_parser import parse_sql_60_features, FEATURE_SCHEMA_60

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

assert len(FEATURE_SCHEMA_60) == 60, "We want exactly 60 features."

class MaskedALSNonnegative:
    """
    W: shape (60,5)
    H: shape (6,5)
    """
    def __init__(self, W, H):
        self.W = np.clip(W, 0, None)  # (60,5)
        self.H = np.clip(H, 0, None)  # (6,5)
        self.logger = logging.getLogger(__name__)

    def predict_single(self, x_feat_60, y_feat_6):
        """
        x_feat_60: shape(60,)
        y_feat_6 : shape(6,)
        => q_latent: (5,) => h_latent: (5,) => cost: float
        """
        q_latent = x_feat_60 @ self.W      # (60,) x (60,5) => (5,)
        h_latent = y_feat_6 @ self.H       # (6,)  x (6,5)  => (5,)
        return float(np.dot(q_latent, h_latent))

HINTS = [
    "enable_hashjoin",
    "enable_indexonlyscan",
    "enable_indexscan",
    "enable_mergejoin",
    "enable_nestloop",
    "enable_seqscan"
]

# Custom Hint Configurations
COMBO_STRS = [
    "",
    "hashjoin,indexonlyscan",
    "hashjoin,indexonlyscan,indexscan",
    "hashjoin,indexonlyscan,indexscan,mergejoin",
    "hashjoin,indexonlyscan,indexscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,indexscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,indexscan,nestloop",
    "hashjoin,indexonlyscan,indexscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,indexscan,seqscan",
    "hashjoin,indexonlyscan,mergejoin",
    "hashjoin,indexonlyscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexonlyscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,nestloop",
    "hashjoin,indexonlyscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,seqscan",
    "hashjoin,indexscan",
    "hashjoin,indexscan,mergejoin",
    "hashjoin,indexscan,mergejoin,nestloop",
    "hashjoin,indexscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexscan,mergejoin,seqscan",
    "hashjoin,indexscan,nestloop",
    "hashjoin,indexscan,nestloop,seqscan",
    "hashjoin,indexscan,seqscan",
    "hashjoin,mergejoin,nestloop,seqscan",
    "hashjoin,mergejoin,seqscan",
    "hashjoin,nestloop,seqscan",
    "hashjoin,seqscan",
    "indexonlyscan,indexscan,mergejoin",
    "indexonlyscan,indexscan,mergejoin,nestloop",
    "indexonlyscan,indexscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,indexscan,mergejoin,seqscan",
    "indexonlyscan,indexscan,nestloop",
    "indexonlyscan,indexscan,nestloop,seqscan",
    "indexonlyscan,mergejoin",
    "indexonlyscan,mergejoin,nestloop",
    "indexonlyscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,mergejoin,seqscan",
    "indexonlyscan,nestloop",
    "indexonlyscan,nestloop,seqscan",
    "indexscan,mergejoin",
    "indexscan,mergejoin,nestloop",
    "indexscan,mergejoin,nestloop,seqscan",
    "indexscan,mergejoin,seqscan",
    "indexscan,nestloop",
    "indexscan,nestloop,seqscan",
    "mergejoin,nestloop,seqscan",
    "mergejoin,seqscan",
    "nestloop,seqscan"
]


def build_hint_matrix_from_combos(combo_strs, hint_list):
    N = len(combo_strs)
    D = len(hint_list)
    Y = np.zeros((N, D), dtype=int)
    for i, c_str in enumerate(combo_strs):
        items = [x.strip() for x in c_str.split(",")]
        for item in items:
            enable_str = "enable_" + item
            if enable_str in hint_list:
                j = hint_list.index(enable_str)
                Y[i, j] = 1
    return Y

def apply_plan_hints(cursor, plan_vector):
    for i, hint_name in enumerate(HINTS):
        val = plan_vector[i]
        cmd = f"SET {hint_name} TO {'ON' if val>=0.5 else 'OFF'};"
        cursor.execute(cmd)

def reset_all_hints(cursor):
    cursor.execute("RESET ALL;")

def run_query_postgres_once(query_str, plan_vector, pg_host, pg_db, pg_user, pg_password, port=5432):
    start_time = time.time()
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host, dbname=pg_db, user=pg_user,
            password=pg_password, port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            apply_plan_hints(cur, plan_vector)
            cur.execute(query_str)
            _ = cur.fetchall()  # ensure full execution
            reset_all_hints(cur)
    except Exception as e:
        logger.error(f"Error executing query: {e}")
    finally:
        if conn:
            conn.close()
    return time.time() - start_time

def prepare_single_query_features(x_feat_60, y_feat_6, poly, scaler):
    """
    Combine => (66,) => polynomial => scaled => shape(1, 2000+) ...
    """
    combined = np.hstack([x_feat_60, y_feat_6]).reshape(1, -1)
    feats_poly = poly.transform(combined)
    feats_scaled = scaler.transform(feats_poly)
    return feats_scaled

def main():
    MODEL_DIR = "/Users/raahimlone/New_Data"
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results.csv"

    PG_HOST = "localhost"
    PG_DB = "IMDB"
    PG_USER = "postgres"
    PG_PASSWORD = "raahimlhere"
    PG_PORT = 6543

    # 1) Load IMC model: W(60,5) & H(6,5)
    try:
        W_path = os.path.join(MODEL_DIR, "U_best.npy")  
        H_path = os.path.join(MODEL_DIR, "V_best.npy")  
        W = np.load(W_path)  # shape (60,5)
        H = np.load(H_path)  # shape (6,5)
        logger.info(f"Loaded W: {W.shape}, H: {H.shape}")
    except Exception as e:
        logger.error(f"Could not load IMC model: {e}")
        return

    model_imc = MaskedALSNonnegative(W, H)

    # 2) Build Y combos => shape(N,6)
    Y = build_hint_matrix_from_combos(COMBO_STRS, HINTS)
    logger.info(f"Y combos => shape={Y.shape}")

    # 3) (Optional) Load residual pipeline
    try:
        rmodel_path = os.path.join(MODEL_DIR, "residual_model_xgb.pkl")
        poly_path   = os.path.join(MODEL_DIR, "residual_model_poly.pkl")
        scaler_path = os.path.join(MODEL_DIR, "residual_model_scaler.pkl")

        residual_model = joblib.load(rmodel_path)
        poly = joblib.load(poly_path)
        scaler = joblib.load(scaler_path)
        logger.info("Loaded XGBoost residual model & polynomial pipeline.")
    except Exception as e:
        logger.warning(f"No residual model pipeline found => using IMC only. {e}")
        residual_model, poly, scaler = None, None, None

    # 4) Manual minâ€“max for 60 features (Example only)
    MANUAL_MAX = {
        "num_tables": 10,
        "total_joins": 50,
        "num_inner_joins": 50,
        "num_left_joins": 50,
        "num_right_joins": 50,
        "num_full_joins": 50,
        "num_cross_joins": 50,
        "num_semi_joins": 50,
        "num_anti_joins": 50,
        "num_other_joins": 50,
        "num_union": 10,
        "num_intersect": 10,
        "num_except": 10,
        "num_agg_count": 50,
        "num_agg_sum": 50,
        "num_agg_avg": 50,
        "num_agg_min": 50,
        "num_agg_max": 50,
        "num_agg_other": 50,
        "num_distinct": 10,
        "num_limit": 10,
        "num_offset": 10,
        "func_unknown": 50,  # etc...
    }

    def manual_min_max_scale(feats_dict_60):
        scaled = []
        for key in FEATURE_SCHEMA_60:
            raw_val = feats_dict_60.get(key, 0)
            max_val = MANUAL_MAX.get(key, 50)  # default=50
            if max_val > 0:
                scaled.append(min(raw_val, max_val) / max_val)
            else:
                scaled.append(0.0)
        return np.array(scaled, dtype=float)

    # 5) Process queries
    if not os.path.isdir(QUERIES_DIR):
        logger.error(f"Queries dir not found: {QUERIES_DIR}")
        return

    sql_files = [f for f in os.listdir(QUERIES_DIR) if f.endswith(".sql")]
    sql_files.sort()
    logger.info(f"Found {len(sql_files)} SQL files to process.")

    results = []
    for fname in sql_files:
        fpath = os.path.join(QUERIES_DIR, fname)
        logger.info(f"Processing {fname}...")
        try:
            with open(fpath, "r", encoding="utf-8") as ff:
                query_str = ff.read().strip()
            if not query_str:
                logger.warning(f"{fname} => empty query.")
                continue

            # === Parse => EXACT 60 features
            feats_60_dict = parse_sql_60_features(query_str)

            # === Scale => x_feat_60 (shape (60,))
            x_feat_60 = manual_min_max_scale(feats_60_dict)

            # === Evaluate each hint plan
            best_idx = None
            best_cost = float("inf")
            logger.info(f"\nðŸ“Š Predicted Latencies for Query: {fname}")
            all_latencies = []

            for i in range(Y.shape[0]):
                y_feat_6 = Y[i, :]
                # 1) IMC cost
                cost_imc = model_imc.predict_single(x_feat_60, y_feat_6)
                # 2) Residual
                residual_pred = 0.0
                if residual_model and poly and scaler:
                    big_feats  = prepare_single_query_features(x_feat_60, y_feat_6, poly, scaler)
                    residual_pred = residual_model.predict(big_feats)[0]
                final_cost = cost_imc + residual_pred
                if final_cost < best_cost:
                    best_cost = final_cost
                    best_idx = i
                all_latencies.append(final_cost)
                logger.info(f"Plan {i}: Cost = {final_cost:.4f}, Hints = {Y[i]}")

            if best_idx is None:
                logger.warning(f"{fname} => no feasible plan found!")
                continue

            chosen_plan = Y[best_idx, :]  # (6,)
            latency = run_query_postgres_once(
                query_str=query_str,
                plan_vector=chosen_plan,
                pg_host=PG_HOST, pg_db=PG_DB,
                pg_user=PG_USER, pg_password=PG_PASSWORD,
                port=PG_PORT
            )
            if latency <= 0:
                logger.warning(f"{fname} => invalid latency {latency:.4f}")
                continue

            # decode chosen hints
            enabled_hints = []
            for j, hint_name in enumerate(HINTS):
                if chosen_plan[j] == 1:
                    short = hint_name.replace("enable_", "")
                    enabled_hints.append(short)
            hint_str = ",".join(enabled_hints) if enabled_hints else "Baseline (no hints)"

            logger.info(
                f"{fname} => best_idx={best_idx}, cost={best_cost:.4f}, "
                f"latency={latency:.4f}s, hints=[{hint_str}]"
            )
            results.append({
                "filename": fname,
                "best_index": best_idx,
                "cost_estimate": best_cost,
                "latency_s": latency,
                "hints": hint_str
            })

        except Exception as e:
            logger.error(f"Error processing {fname}: {e}")
            results.append({"filename": fname, "error": str(e)})

    # 6) Save results
    if OUTPUT_CSV and OUTPUT_CSV.strip():
        try:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_CSV, index=False)
            logger.info(f"Results saved => {OUTPUT_CSV}")
        except Exception as e:
            logger.error(f"Failed saving CSV => {e}")

    # 7) Print summary
    if results:
        print("\n=== Query Optimization Results ===")
        for r in results:
            if "error" in r:
                print(f"{r['filename']} => Error: {r['error']}")
            else:
                print(
                    f"{r['filename']} => best_index={r['best_index']}, "
                    f"cost={r['cost_estimate']:.4f}, latency={r['latency_s']:.4f}s, "
                    f"hints=[{r['hints']}]"
                )
    else:
        print("No results to display.")
    logger.info("All queries processed.")
    import matplotlib.pyplot as plt

    def visualize_hint_matrix(Y, best_idx):
        """
        Visualize the Y matrix and highlight the selected row with minimum cost.
        """
        plt.figure(figsize=(10, 6))
        plt.imshow(Y, cmap='gray', aspect='auto')
        plt.colorbar(label='Hint Enabled (0=OFF, 1=ON)')
        plt.title('Hint Combination Matrix (Y)')
        plt.xlabel('Hint Types')
        plt.ylabel('Plan Index')

        # Highlight the best selected row
        plt.axhline(y=best_idx, color='red', linestyle='--', label='Best Plan')
        plt.legend()
        plt.show()

if __name__ == "__main__":
    main()
'''