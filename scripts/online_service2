#!/usr/bin/env python3
"""
Build scaled feature matrices and association matrices (W, M) for IMC purely from JSON logs
and precomputed feature files, without relying on the old CSV.

- Scans a directory of JSON files for SQL, hint_list, and runtime_list.
- Loads unscaled query features from an NPZ and hint features from a Parquet/CSV.
- Scales both feature matrices to [0,1].
- Constructs W and M by mapping each (sql, hint) â†’ runtime. Only pairs with recorded runtimes
  produce nonzero entries, ensuring a sparse association.
- Randomly retains 60% of those observed entries (simulating partial observation).
- Saves dense and sparse versions of W and M with a "new" suffix.
"""
import os
import sys
import argparse
import logging
import json
import numpy as np
import pandas as pd
from scipy.sparse import coo_matrix, save_npz
from sklearn.preprocessing import MinMaxScaler

# Ensure project root is on sys.path
def configure_environment():
    import sys, os
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts = os.path.join(project_root, 'scripts')
    if scripts in sys.path:
        sys.path.remove(scripts)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config


def main():
    parser = argparse.ArgumentParser(__doc__)
    parser.add_argument('--config', default='config/default_config.yaml')
    parser.add_argument('--dataset_dir', required=True,
                        help='Path to JSON job directory with .json logs')
    parser.add_argument('--query_features', required=True,
                        help='NPZ file of unscaled query features')
    parser.add_argument('--hint_features', required=True,
                        help='Parquet/CSV of hint features')
    args = parser.parse_args()

    cfg = load_config(args.config)
    logger = setup_logger(
        __name__,
        cfg['logging']['log_file'],
        level=getattr(logging, cfg['logging']['log_level'].upper(), logging.INFO)
    )

    # Paths
    json_dir = args.dataset_dir
    qf_file = args.query_features
    hf_file = args.hint_features
    out_dir = cfg['paths']['processed_features']

    # Validate inputs
    for path, label in [(json_dir, 'JSON dir'), (qf_file, 'query features'), (hf_file, 'hint features')]:
        if not os.path.exists(path):
            logger.error(f"{label} path not found: {path}")
            sys.exit(1)
    os.makedirs(out_dir, exist_ok=True)

    # Load JSON records
    logger.info(f"Scanning JSON logs under {json_dir}")
    all_sqls = []
    records = []  # tuples: (sql, hint_list, runtime_list)
    for root, _, files in os.walk(json_dir):
        for fn in files:
            if not fn.lower().endswith('.json'):
                continue
            fp = os.path.join(root, fn)
            try:
                j = json.load(open(fp, 'r'))
            except Exception as e:
                logger.warning(f"Failed to load {fp}: {e}")
                continue
            recs = j if isinstance(j, list) else [j]
            for r in recs:
                sql = r.get('sql', '').strip()
                hints = r.get('hint_list', []) or []
                runtimes = r.get('runtime_list', []) or []
                if not sql:
                    continue
                all_sqls.append(sql)
                records.append((sql, hints, runtimes))
    if not all_sqls:
        logger.error("No SQL entries found in JSON logs.")
        sys.exit(1)

    # Unique SQL indexing
    unique_sqls = list(dict.fromkeys(all_sqls))
    q2i = {q: i for i, q in enumerate(unique_sqls)}
    n_q = len(unique_sqls)
    logger.info(f"Found {n_q} unique SQL queries.")

    # Load unscaled features
    try:
        arr = np.load(qf_file)
        X_raw = arr[arr.files[0]]
        logger.info(f"Loaded query features from {qf_file}, shape {X_raw.shape}")
    except Exception as e:
        logger.error(f"Error loading query features: {e}")
        sys.exit(1)
    try:
        Y_df = pd.read_parquet(hf_file) if hf_file.lower().endswith('.parquet') else pd.read_csv(hf_file)
        Y_raw = Y_df.values
        logger.info(f"Loaded hint features from {hf_file}, shape {Y_raw.shape}")
    except Exception as e:
        logger.error(f"Error loading hint features: {e}")
        sys.exit(1)

    # Scale features
    scaler = MinMaxScaler(feature_range=(0, 1))
    X = scaler.fit_transform(X_raw)
    Y = scaler.fit_transform(Y_raw)
    logger.info("Scaled X and Y to [0,1]")

    # Initialize association matrices
    n_h = Y.shape[0]
    W = np.zeros((n_q, n_h), dtype=float)
    M = np.zeros((n_q, n_h), dtype=int)

    # Populate only observed (sql, hint) pairs
    for sql, hints, runtimes in records:
        i = q2i[sql]
        for idx, hint in enumerate(hints):
            if idx < len(runtimes) and 0 <= hint < n_h:
                W[i, hint] = runtimes[idx]
                M[i, hint] = 1

    # --- randomly retain only 60% of these observed entries ---
    obs_positions = np.argwhere(M == 1)
    num_obs = obs_positions.shape[0]
    num_keep = int(num_obs * 0.6)
    np.random.seed(42)
    selected_ids = np.random.choice(num_obs, size=num_keep, replace=False)
    W_masked = np.zeros_like(W)
    M_masked = np.zeros_like(M)
    for sid in selected_ids:
        i, j = obs_positions[sid]
        W_masked[i, j] = W[i, j]
        M_masked[i, j] = 1
    W, M = W_masked, M_masked
    logger.info(f"Retained {num_keep}/{num_obs} ({num_keep/num_obs:.0%}) of observed entries")

    # Save feature matrices with 'new' suffix
    np.savez(os.path.join(out_dir, 'X_scalednew.npz'), X=X)
    np.savez(os.path.join(out_dir, 'Y_scalednew.npz'), Y=Y)
    logger.info("Saved X_scalednew.npz and Y_scalednew.npz")

    # Save W/M dense & sparse with 'new' suffix
    save_npz(os.path.join(out_dir, 'W_sparsenew.npz'), coo_matrix(W))
    np.save(os.path.join(out_dir, 'Wnew.npy'), W)
    save_npz(os.path.join(out_dir, 'M_sparsenew.npz'), coo_matrix(M))
    np.save(os.path.join(out_dir, 'Mnew.npy'), M)
    logger.info("Saved W_sparsenew.npz, Wnew.npy, M_sparsenew.npz, Mnew.npy")

    total = n_q * n_h
    observed = int(M.sum())
    logger.info(f"Observed entries after masking: {observed}/{total}")
    logger.info("Done.")

if __name__ == '__main__':
    main()
