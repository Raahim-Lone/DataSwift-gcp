#!/usr/bin/env python3
"""
Gauss-Newton based algorithm for inductive matrix completion with enhanced residual modeling.
### Enhanced with Weighted Residuals, Residual Tracking, Iterative Refinement, and LIME-QO ###
### Written by Pini Zilber and Boaz Nadler, 2022 ###
"""
    
import numpy as np
from scipy import sparse
from scipy.sparse import linalg as sp_linalg
import random
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
import xgboost as xgb  # For residual modeling
import joblib  # For model persistence
from sklearn.model_selection import train_test_split
    
# Initialization Options
INIT_WITH_SVD = 0
INIT_WITH_RANDOM = 1
INIT_WITH_USER_DEFINED = 2
    
def generate_product_matrix(A, B):
    """
    Returns M such that M @ vec(C) = vec(A @ C @ B)
    """
    assert A.shape[0] == B.shape[1], 'Error: dimension mismatch'
        
    m = A.shape[0]
    M = np.zeros((m, A.shape[1] * B.shape[0]))
    for i in range(m):
        AB = np.outer(A[i, :], B[:, i])
        M[i, :] = AB.flatten()
    return M
    
def GNIMC(X, omega, rank, A, B, verbose=True, alpha=0.1, perform_qr=True, max_outer_iter=100,
          max_inner_iter_init=20, max_inner_iter_final=1000, lsqr_inner_init_tol=1e-15, 
          lsqr_smart_tol=True, lsqr_smart_obj_min=1e-5,
          init_option=INIT_WITH_SVD, init_U=None, init_V=None,
          stop_relRes=1e-14, stop_relDiff=-1, stop_relResDiff=-1):
    """
    Run GNIMC algorithm for inductive matrix completion.
    
    Parameters:
    ----------
    X : numpy.ndarray
        Input matrix with observed entries set and unobserved entries set to 0.
    omega : scipy.sparse.csr_matrix
        Matrix indicating observed entries (1) and unobserved entries (0).
    rank : int
        Rank for the matrix completion.
    A : numpy.ndarray
        Feature matrix for queries.
    B : numpy.ndarray
        Feature matrix for hints.
    Other Parameters:
        Detailed explanations as needed.
    
    Returns:
    -------
    tuple
        (X_hat, iter_num, convergence_flag, all_relRes, U_best, V_best)
    """
    n1, n2 = X.shape
    d1 = A.shape[1]
    d2 = B.shape[1]
    m = omega.count_nonzero()
    p = m / (n1 * n2)
    I, J, _ = sparse.find(omega)

    # Initial estimate
    if init_option == INIT_WITH_SVD:
        L, S, R = sp_linalg.svds(X / p, k=rank, tol=1e-16)
        U = A.T @ L @ np.diag(np.sqrt(S))
        V = B.T @ R.T @ np.diag(np.sqrt(S))
    elif init_option == INIT_WITH_RANDOM:
        U = np.linalg.qr(np.random.randn(d1, rank))[0]
        V = np.linalg.qr(np.random.randn(d2, rank))[0]
    else:  # INIT_WITH_USER_DEFINED
        U = init_U
        V = init_V

    AU_omega_rows = A[I, :] @ U
    BV_omega_cols = B[J, :] @ V

    # Before iterations
    x = X[I, J]
    X_norm = np.linalg.norm(x)
    early_stopping_flag = False
    relRes = float("inf")
    all_relRes = [relRes]
    best_relRes = float("inf")
    U_best = U.copy()
    V_best = V.copy()
    x_hat = np.sum(AU_omega_rows * BV_omega_cols, axis=1)
    x_hat_prev = x_hat.copy()

    # Iterations
    iter_num = 0
    while iter_num < max_outer_iter and not early_stopping_flag:
        iter_num += 1

        # Construct variables for LSQR
        if perform_qr:
            U_Q, U_R = np.linalg.qr(U)
            V_Q, V_R = np.linalg.qr(V)
            AU_for_use = A[I, :] @ U_Q
            BV_for_use = B[J, :] @ V_Q
        else:
            AU_for_use = AU_omega_rows
            BV_for_use = BV_omega_cols

        L1 = generate_product_matrix(A[I, :], BV_for_use.T)
        L2 = generate_product_matrix(AU_for_use, B[J, :].T)
        L = sparse.csr_matrix(np.concatenate((L1, L2), axis=1))
            
        # Update b of LSQR (as in ||Ax-b||^2)
        update = alpha * np.sum(AU_omega_rows * BV_for_use, axis=1)
        b = x + update

        # Determine LSQR tolerance and #iterations
        lsqr_tol = lsqr_inner_init_tol
        lsqr_iters = max_inner_iter_init
        if lsqr_smart_tol and relRes < lsqr_smart_obj_min:
            lsqr_tol = min(lsqr_tol, relRes**2)
            lsqr_iters = max_inner_iter_final

        # Solve the least squares problem
        z = sp_linalg.lsqr(L, b, atol=lsqr_tol, btol=lsqr_tol, iter_lim=lsqr_iters)[0]

        # Construct Utilde and Vtilde from the solution z
        U_tilde = np.reshape(z[:d1 * rank], (d1, rank))
        V_tilde = np.reshape(z[d1 * rank:], (rank, d2)).T
        if perform_qr:
            U_tilde = U_tilde @ np.linalg.inv(V_R).T
            V_tilde = V_tilde @ np.linalg.inv(U_R).T

        # Calculate new U, V
        U = 0.5 * (1 - alpha) * U + U_tilde
        V = 0.5 * (1 - alpha) * V + V_tilde
        AU_omega_rows = A[I, :] @ U
        BV_omega_cols = B[J, :] @ V
            
        # Get new estimate and calculate corresponding error
        x_hat = np.sum(AU_omega_rows * BV_omega_cols, axis=1)

        relRes = np.linalg.norm(x_hat - x) / X_norm
        all_relRes.append(relRes)
        if relRes < best_relRes:
            best_relRes = relRes
            U_best = U.copy()
            V_best = V.copy()
        x_hat_diff = np.linalg.norm(x_hat - x_hat_prev) / np.linalg.norm(x_hat) if np.linalg.norm(x_hat) != 0 else 0
        x_hat_prev = x_hat.copy()  # Update previous estimate

        # Report
        if verbose:
            print(f"[INSIDE GNIMC] iter: {iter_num}, relRes: {relRes:.6e}")

        # Check early stopping criteria
        if stop_relRes > 0 and relRes < stop_relRes:
            early_stopping_flag = True
        if stop_relDiff > 0 and x_hat_diff < stop_relDiff:
            early_stopping_flag = True
        if stop_relResDiff > 0 and len(all_relRes) >= 2:
            rel_res_diff = np.abs(relRes / all_relRes[-2] - 1)
            if rel_res_diff < stop_relResDiff:
                early_stopping_flag = True
        if verbose and early_stopping_flag:
            print("[INSIDE GNIMC] Early stopping")

    # Return including U_best and V_best
    convergence_flag = iter_num < max_outer_iter
    X_hat = A @ U_best @ V_best.T @ B.T 
    return X_hat, iter_num, convergence_flag, all_relRes, U_best, V_best
    
def prepare_features(A, B, M, poly=None):
    """
    Prepare and engineer features for the residual model.
    """
    # Expand features
    n_queries, n_hints = M.shape
    A_expanded = np.repeat(A, n_hints, axis=0)  # Shape: (n_queries * n_hints, d1)
    B_expanded = np.tile(B, (n_queries, 1))      # Shape: (n_queries * n_hints, d2)
    
    features = np.hstack((A_expanded, B_expanded))  # Shape: (n_queries * n_hints, d1 + d2)
    
    # Handle missing values if any
    # Assuming no missing values; otherwise, uncomment the following:
    # from sklearn.impute import SimpleImputer
    # imputer = SimpleImputer(strategy='mean')
    # features = imputer.fit_transform(features)
    
    # Polynomial Features
    if poly is None:
        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
    features_poly = poly.fit_transform(features)
    
    print(f"🔄 Feature matrix shape after polynomial expansion: {features_poly.shape}")
    
    return features_poly, poly
    
def initialize_residual_tracker(n_queries, n_hints):
    """
    Initialize a tracker for residuals per entry.
    """
    # Initialize as a 3D array to track residuals over iterations
    return np.zeros((n_queries, n_hints, 0))  # Shape: (queries, hints, iterations)
    
def update_residual_tracker(tracker, residuals):
    """
    Update the residual tracker with current residuals.
    """
    residuals_expanded = residuals[:, :, np.newaxis]
    return np.concatenate((tracker, residuals_expanded), axis=2)
    
def train_residual_model(W_true, X_hat, A, B, M, scaler=None, poly=None, residual_model=None, train_observed_only=True):
    """
    Train a residual model with weighted samples to emphasize positive residuals.
    
    Parameters:
    ----------
    W_true : numpy.ndarray
        True latency matrix.
    X_hat : numpy.ndarray
        Predicted latency matrix from GNIMC.
    A : numpy.ndarray
        Feature matrix for queries.
    B : numpy.ndarray
        Feature matrix for hints.
    M : numpy.ndarray
        Mask matrix indicating observed entries.
    scaler : sklearn.preprocessing object, optional
        Fitted scaler for feature scaling.
    poly : sklearn.preprocessing object, optional
        Fitted polynomial feature transformer.
    residual_model : xgb.XGBRegressor object, optional
        Pre-trained residual model.
    train_observed_only : bool, optional
        Whether to train the residual model only on observed entries. Default is True.
    
    Returns:
    -------
    tuple
        (adjusted_X_hat, residual_model, scaler, poly)
    """
    print("\n=== Training Residual Model with Weighted Samples ===")
    
    # Compute residuals
    residuals = W_true - X_hat  # Shape: (n_queries, n_hints)
    
    n_queries, n_hints = W_true.shape
    
    # Prepare feature matrix for residuals
    features_poly, poly = prepare_features(A, B, M, poly)
    
    residuals_flat = residuals.flatten()             # Shape: (n_queries * n_hints,)
    
    # Determine indices based on mask if training only on observed data
    if train_observed_only:
        observed_indices = M.flatten() == 1
        features_train = features_poly[observed_indices]
        residuals_train = residuals_flat[observed_indices]
    else:
        features_train = features_poly
        residuals_train = residuals_flat
    
    # Create sample weights: Assign higher weight to positive residuals
    alpha_weight = 1  # Adjust alpha as needed
    sample_weights = 1 + alpha_weight * (residuals_train > 0).astype(float)
    
    # Initialize and train the XGBoost regressor with sample weights
    print("🔄 Initializing and training XGBoost regressor with sample weights...")
    if residual_model is None:
        residual_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=2000,        # Number of boosting rounds
            learning_rate=0.02,       # Learning rate
            max_depth=10,             # Maximum tree depth
            subsample=0.8,            # Subsample ratio of the training instances
            colsample_bytree=0.8,     # Subsample ratio of columns when constructing each tree
            verbosity=1,
            n_jobs=-1,                # Use all available cores
            random_state=42
        )
    
    # Scale features
    print("🔄 Scaling features...")
    if scaler is None:
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_train)
    else:
        features_scaled = scaler.transform(features_train)
    
    # Train the model
    print("🔄 Training residual model...")
    residual_model.fit(features_scaled, residuals_train, sample_weight=sample_weights)
    print("✅ Residual Model Trained with Weighted Samples.")
    
    # Predict residuals for all entries
    print("🔄 Predicting residuals for all entries...")
    features_all_scaled = scaler.transform(features_poly)
    residuals_pred = residual_model.predict(features_all_scaled)
    residuals_pred = residuals_pred.reshape(n_queries, n_hints)
    
    # Adjust X_hat
    print("🔄 Adjusting X_hat with predicted residuals...")
    adjusted_X_hat = X_hat + residuals_pred
    
    # Ensure non-negativity
    adjusted_X_hat = np.maximum(adjusted_X_hat, 0)
    print("✅ X_hat Adjusted with Residuals.")
    
    # Save the residual model and transformers
    print("\n✅ Saving Residual Model and Preprocessing Objects...")
    joblib.dump(residual_model, '/Users/raahimlone/New_Data/residual_model_xgb.pkl')
    joblib.dump(scaler, '/Users/raahimlone/New_Data/residual_model_scaler.pkl')  # Ensure correct filename
    joblib.dump(poly, '/Users/raahimlone/New_Data/residual_model_poly.pkl')      # Ensure correct filename
    print("✅ Residual model and preprocessing objects saved successfully.")
    
    return adjusted_X_hat, residual_model, scaler, poly
    
def LIMEQO(W_true, M, A, B, k, lambda_, t, m_select=3):
    """
    Perform the LIME-QO algorithm for hint selection.

    Parameters:
    ----------
    W_true : numpy.ndarray
        True latency matrix of shape (n_queries, n_hints).
    M : numpy.ndarray
        Mask matrix of the same shape as W_true, where M[i, j] = 1 indicates 
        that W_true[i, j] is observed, and 0 otherwise.
    A : numpy.ndarray
        Feature matrix for queries of shape (n_queries, d1).
    B : numpy.ndarray
        Feature matrix for hints of shape (n_hints, d2).
    k : int
        Rank parameter for the matrix completion algorithm.
    lambda_ : float
        Regularization parameter for the matrix completion algorithm.
    t : int
        Number of iterations for the hint selection process.
    m_select : int, optional
        Number of top hints to select in each iteration. Default is 3.

    Returns:
    -------
    tuple
        (hint_selections, W_tilde, M, U_best, V_best, all_selected_hints)
    """
    # Initialize W_tilde with observed entries; unobserved entries set to 0
    W_tilde = np.copy(W_true)
    W_tilde[M == 0] = 0  # Unobserved entries set to 0
    n_queries, n_hints = W_true.shape
    hint_selections = [None] * n_queries
    all_selected_hints = []

    for iteration in range(t):
        print(f"\n--- Iteration {iteration+1} ---")

        # Step 1: First Hint Selection based on W_tilde
        for i in range(n_queries):
            # Select the hint with the minimum observed latency
            observed_indices = np.where(M[i] == 1)[0]
            if observed_indices.size == 0:
                continue  # Skip if no observations
            j_min = observed_indices[np.argmin(W_tilde[i, observed_indices])]
            hint_selections[i] = j_min  # Store the index of the selected hint

        # Step 2: Matrix Completion using GNIMC
        omega = sparse.csr_matrix(M)
        print("🔄 Performing GNIMC matrix completion...")

        W_hat, _, _, _, U_best, V_best = GNIMC(
            X=W_tilde,
            omega=omega,
            rank=k,
            A=A,
            B=B,
            verbose=False,
            alpha=0.04,
            perform_qr=True,
            max_outer_iter=100,  # Adjust as needed
        )
        print("✅ GNIMC completed.")

        # Step 3 & 4: Second Hint Selection and Cost Improvement Calculation
        S = []
        for i in range(n_queries):
            # Select the hint with the minimum predicted latency
            j_hat_min = np.argmin(W_hat[i, :])

            # Compute Delta W_i
            if np.any(M[i] == 1):
                min_W_tilde_i = np.min(W_tilde[i, M[i] == 1])
            else:
                # If no observed entries, set to a high value to encourage selection
                min_W_tilde_i = np.inf
            delta_Wi = min_W_tilde_i - W_hat[i, j_hat_min]

            if delta_Wi > 0 and M[i, j_hat_min] == 0:
                S.append((i, j_hat_min, delta_Wi))

        print(f"🔍 Selected {len(S)} potential improvements based on Delta W.")

        # Step 5: Select Top m_select Improvements
        S_sorted = sorted(S, key=lambda x: x[2], reverse=True)
        top_S = S_sorted[:m_select]
        print(f"Number of selected hints in top_S: {len(top_S)}")

        # If fewer than m_select pairs, randomly select additional unobserved pairs
        if len(top_S) < m_select:
            needed = m_select - len(top_S)
            # Find all unobserved (i, j) pairs
            unobserved = np.argwhere(M == 0)
            np.random.shuffle(unobserved)
            for idx in unobserved:
                i, j = idx
                # Avoid selecting pairs already in top_S
                if not any((i == pair[0] and j == pair[1]) for pair in top_S):
                    top_S.append((i, j, 0))  # delta_Wi set to 0 for random selection
                    needed -= 1
                    if needed == 0:
                        break

        # Step 6: Update Mask Matrix M and W_tilde with Selected Hints
        print("📝 Updating M and W_tilde with selected hints...")
        for (i, j, delta_Wi) in top_S:
            if M[i, j] == 0:
                M[i, j] = 1  # Update mask to indicate observation
                W_tilde[i, j] = W_true[i, j]  # Assign true latency
                if delta_Wi > 0:
                    print(f"🔧 Hint selected at (i={i}, j={j}) with Delta_W={delta_Wi:.4f}")
                else:
                    print(f"🔀 Randomly selected hint at (i={i}, j={j})")

        print(f"Selected {len(top_S)} hints to observe.")

        # Accumulate all selected hints
        all_selected_hints.extend([(i, j) for (i, j, _) in top_S])

        # Optional: Early stopping if no significant improvements
        if len(S_sorted) == 0:
            print("🔒 No potential improvements found. Stopping iterations.")
            break

    # Final Hint Selection after all observations
    for i in range(n_queries):
        observed_indices = np.where(M[i] == 1)[0]
        if observed_indices.size == 0:
            hint_selections[i] = None  # Or assign a default hint
        else:
            j_min = observed_indices[np.argmin(W_tilde[i, observed_indices])]
            hint_selections[i] = j_min  # Store the index of the selected hint

    return hint_selections, W_tilde, M, U_best, V_best, all_selected_hints
    
if __name__ == "__main__":
    # Load W_true (Full Matrix)
    try:
        W_true = np.load('/Users/raahimlone/New_Data/W_low_rank.npy')  # Shape: (n_queries, n_hints)
        print("✅ W_true Loaded:")
        print(W_true)
        print(f"Shape of W_true: {W_true.shape}")
    except FileNotFoundError:
        print("Error: W_low_rank.npy not found at the specified path.")
        exit(1)

    # Load Mask Matrix (M)
    try:
        M = np.load('/Users/raahimlone/New_Data/M.npy')  # Shape: (n_queries, n_hints), with 1s and 0s
        print("\n✅ Mask Matrix (M) Loaded:")
        print(M)
        print(f"Shape of M: {M.shape}")
    except FileNotFoundError:
        print("Error: M.npy not found at the specified path.")
        exit(1)

    # Load Side Information Matrices (A and B)
    try:
        X_data = np.load('/Users/raahimlone/New_Data/X_scaled.npz')
        A = X_data['data']  # Shape: (n_queries, d1)
        print("\n✅ Side Information Matrix A Loaded:")
        print(A)
        print(f"Shape of A: {A.shape}")
    except FileNotFoundError:
        print("Error: X_scaled.npz not found at the specified path.")
        exit(1)
    except KeyError:
        print("Error: 'data' key not found in X_scaled.npz.")
        exit(1)

    try:
        Y_data = np.load('/Users/raahimlone/New_Data/Y_scaled.npz')
        B = Y_data['Y']  # Shape: (n_hints, d2)
        print("\n✅ Side Information Matrix B Loaded:")
        print(B)
        print(f"Shape of B: {B.shape}")
    except FileNotFoundError:
        print("Error: Y_scaled.npz not found at the specified path.")
        exit(1)
    except KeyError:
        print("Error: 'Y' key not found in Y_scaled.npz.")
        exit(1)

    # Define Parameters
    rank = 5
    lambda_ = 0.05  # Regularization parameter
    t = 5  # Number of iterations for LIME-QO
    m = 5  # Number of hints to select per iteration

    # Initialize W_tilde and M_copy for iterative runs
    W_tilde_current = np.copy(W_true)
    W_tilde_current[M == 0] = 0  # Unobserved entries set to 0

    M_copy_current = np.copy(M)

    # Initialize list to store all selected hints
    all_selected_hints = []

    # Initialize residual tracker
    n_queries, n_hints = W_true.shape
    residual_tracker = initialize_residual_tracker(n_queries, n_hints)

    # Define maximum number of alterations
    initial_alterations = 1
    additional_alterations = 1

    # Initial Phase: Perform alterations using LIME-QO
    print("\n=== Initial Phase: Performing Initial Alterations ===")
    for i in range(1, initial_alterations + 1):
        print(f"\n--- Initial Alteration {i} ---")
        hints, W_tilde_current, M_copy_current, U_best, V_best, selected_hints = LIMEQO(
            W_true=W_true,
            M=M_copy_current,
            A=A,
            B=B,
            k=rank,
            lambda_=lambda_,
            t=1,  # Perform one iteration per alteration
            m_select=m
        )
        all_selected_hints.extend(selected_hints)

        # Update residual tracker after each alteration
        X_hat_temp = A @ U_best @ V_best.T @ B.T
        residuals_temp = W_true - X_hat_temp
        residual_tracker = update_residual_tracker(residual_tracker, residuals_temp)

    # Compute X_hat after initial alterations
    print("\n=== Computing X_hat after Initial Alterations ===")
    X_hat_final = A @ U_best @ V_best.T @ B.T
    print("✅ X_hat after Initial Alterations:")
    print(X_hat_final)

    # Check for negative entries
    negative_indices = np.where(X_hat_final < 0)
    num_negatives = len(negative_indices[0])
    print(f"\nNumber of negative entries in X_hat_final after Initial Alterations: {num_negatives}")

    # Train and Apply Enhanced Residual Model
    print("\n=== Training Initial Residual Model ===")
    X_hat_final, residual_model, scaler, poly = train_residual_model(
        W_true=W_true, 
        X_hat=X_hat_final, 
        A=A, 
        B=B, 
        M=M_copy_current,
        train_observed_only=True  # Ensuring no data leakage
    )

    # Recompute residuals and relRes after residual model adjustment
    residuals = W_true - X_hat_final
    residual_tracker = update_residual_tracker(residual_tracker, residuals)
    X_norm = np.linalg.norm(W_true)
    relRes = np.linalg.norm(residuals) / X_norm
    print(f"\nRelative Residual after Residual Model Adjustment: {relRes:.6e}")

    # Iterative Residual Correction Phase
    target_relRes = 0.1  # Desired relative residual
    max_iterations = 1  # Maximum number of residual correction iterations
    current_iteration = 0

    while relRes > target_relRes and current_iteration < max_iterations:
        current_iteration += 1
        print(f"\n=== Iterative Residual Correction Phase: Iteration {current_iteration} ===")

        # Train residual model with updated X_hat
        X_hat_final, residual_model, scaler, poly = train_residual_model(
            W_true=W_true, 
            X_hat=X_hat_final, 
            A=A, 
            B=B, 
            M=M_copy_current,
            scaler=scaler, 
            poly=poly, 
            residual_model=residual_model, 
            train_observed_only=True  # Ensuring no data leakage
        )

        # Update residuals and tracker
        residuals = W_true - X_hat_final
        residual_tracker = update_residual_tracker(residual_tracker, residuals)
        relRes = np.linalg.norm(residuals) / X_norm
        print(f"Relative Residual after Iteration {current_iteration}: {relRes:.6e}")

    if relRes > target_relRes:
        print(f"\n❗ Warning: Desired relative residual of {target_relRes} not achieved after {max_iterations} iterations.")
    else:
        print(f"\n✅ Desired relative residual of {target_relRes} achieved after {current_iteration} iterations.")

    # Verification Phase: Perform additional alterations if relRes is still high
    if relRes > target_relRes:
        print("\n=== Verification Phase: Performing Additional Alterations to Enforce Low Relative Residual ===")
        for j in range(1, additional_alterations + 1):
            print(f"\n--- Additional Alteration {j} ---")
            hints, W_tilde_current, M_copy_current, U_best, V_best, selected_hints = LIMEQO(
                W_true=W_true,
                M=M_copy_current,
                A=A, 
                B=B,
                k=rank,
                lambda_=lambda_,
                t=1,  # Perform one iteration per alteration
                m_select=m
            )
            all_selected_hints.extend(selected_hints)

            # Compute X_hat after each additional alteration
            X_hat_final = A @ U_best @ V_best.T @ B.T
            print("✅ X_hat after Additional Alteration:")
            print(X_hat_final)

            # Train and apply residual model again
            X_hat_final, residual_model, scaler, poly = train_residual_model(
                W_true=W_true, 
                X_hat=X_hat_final, 
                A=A, 
                B=B, 
                M=M_copy_current,
                scaler=scaler, 
                poly=poly, 
                residual_model=residual_model, 
                train_observed_only=True
            )

            # Recompute residuals and relRes
            residuals = W_true - X_hat_final
            residual_tracker = update_residual_tracker(residual_tracker, residuals)
            relRes = np.linalg.norm(residuals) / X_norm
            print(f"Relative Residual after Alteration {j}: {relRes:.6e}")

            if relRes <= target_relRes:
                print("\n✅ Desired relative residual achieved. Stopping additional alterations.")
                break  # Exit the loop if desired relRes is achieved

        if relRes > target_relRes:
            print(f"\nReached maximum of {additional_alterations} additional alterations with relative residual: {relRes:.6e}")
    else:
        print("\nAll values in X_hat_final are sufficiently accurate after Initial Alterations and Residual Model. No additional alterations needed.")

    # Final Adjustments: Ensure non-negativity (safety)
    negative_indices = np.where(X_hat_final < 0)
    if len(negative_indices[0]) > 0:
        X_hat_final[negative_indices] = 0
        print("\n✅ Set remaining negative entries in X_hat_final to zero.")

    # Display Selected Hints
    print("\n✅ Selected Hint Pairs (Row, Column):")
    for hint in all_selected_hints:
        print(hint)

    # Display Final X_hat
    print("\n✅ Final X_hat Values:")
    print(X_hat_final)  # This will print all values of X_hat

    # Save the final X_hat matrix to disk
    np.save('/Users/raahimlone/New_Data/X_hat_final.npy', X_hat_final)
    print("\n✅ Final X_hat matrix saved to '/Users/raahimlone/New_Data/X_hat_final.npy'.")

    # Save the final U and V matrices to disk
    np.save('/Users/raahimlone/New_Data/U_best.npy', U_best)
    np.save('/Users/raahimlone/New_Data/V_best.npy', V_best)
    print("\n✅ U_best and V_best matrices saved successfully.")

    # Save the residual model and preprocessing objects for future use
    print("\n✅ Saving Residual Model and Preprocessing Objects...")
    joblib.dump(residual_model, '/Users/raahimlone/New_Data/residual_model_xgb.pkl')
    joblib.dump(scaler, '/Users/raahimlone/New_Data/residual_model_scaler.pkl')
    joblib.dump(poly, '/Users/raahimlone/New_Data/residual_model_poly.pkl')
    print("✅ Residual model and preprocessing objects saved successfully.")
