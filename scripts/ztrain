#!/usr/bin/env python3
"""
Gauss-Newton based algorithm for inductive matrix completion with enhanced residual modeling.
### Enhanced with Weighted Residuals, Residual Tracking, Iterative Refinement, and LIME-QO ###
### Updated to fix scaling mismatch & dimension issues ###
"""

import numpy as np
from scipy import sparse
from scipy.sparse import linalg as sp_linalg
import joblib
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
import xgboost as xgb
import os

# Initialization Options
INIT_WITH_SVD = 0
INIT_WITH_RANDOM = 1
INIT_WITH_USER_DEFINED = 2

def generate_product_matrix(A, B):
    """
    Returns M such that M @ vec(C) = vec(A @ C @ B).

    ### FIX/UPDATE ###
    If you only need row-block expansions, your original usage might suffice.
    If you need the fully flattened approach, the dimension should be (A.shape[0]*B.shape[1], A.shape[1]*B.shape[0]).
    The snippet below is the "original" version you used, but double-check your actual usage in GNIMC.
    """
    assert A.shape[0] == B.shape[1], 'Error: dimension mismatch'
    m = A.shape[0]
    M = np.zeros((m, A.shape[1] * B.shape[0]))
    for i in range(m):
        AB = np.outer(A[i, :], B[:, i])
        M[i, :] = AB.flatten()
    return M

def GNIMC(X, omega, rank, A, B, verbose=True, alpha=0.1, perform_qr=True, max_outer_iter=100,
          max_inner_iter_init=20, max_inner_iter_final=1000, lsqr_inner_init_tol=1e-15, 
          lsqr_smart_tol=True, lsqr_smart_obj_min=1e-5,
          init_option=INIT_WITH_SVD, init_U=None, init_V=None,
          stop_relRes=1e-14, stop_relDiff=-1, stop_relResDiff=-1):
    """
    Run GNIMC algorithm for inductive matrix completion.
    """
    n1, n2 = X.shape
    d1 = A.shape[1]
    d2 = B.shape[1]
    m = omega.count_nonzero()
    I, J, _ = sparse.find(omega)

    # Initial estimate
    if init_option == INIT_WITH_SVD:
        p = m / (n1 * n2)
        L, S, R = sp_linalg.svds(X / p, k=rank, tol=1e-16)
        U = A.T @ L @ np.diag(np.sqrt(S))
        V = B.T @ R.T @ np.diag(np.sqrt(S))
    elif init_option == INIT_WITH_RANDOM:
        U = np.linalg.qr(np.random.randn(d1, rank))[0]
        V = np.linalg.qr(np.random.randn(d2, rank))[0]
    else:  # INIT_WITH_USER_DEFINED
        U = init_U
        V = init_V

    AU_omega_rows = A[I, :] @ U
    BV_omega_cols = B[J, :] @ V

    x = X[I, J]
    X_norm = np.linalg.norm(x)
    relRes = float("inf")
    all_relRes = [relRes]
    best_relRes = float("inf")
    U_best = U.copy()
    V_best = V.copy()
    x_hat = np.sum(AU_omega_rows * BV_omega_cols, axis=1)
    x_hat_prev = x_hat.copy()

    iter_num = 0
    early_stopping_flag = False

    while iter_num < max_outer_iter and not early_stopping_flag:
        iter_num += 1

        # Construct variables for LSQR
        if perform_qr:
            U_Q, U_R = np.linalg.qr(U)
            V_Q, V_R = np.linalg.qr(V)
            AU_for_use = A[I, :] @ U_Q
            BV_for_use = B[J, :] @ V_Q
        else:
            AU_for_use = AU_omega_rows
            BV_for_use = BV_omega_cols

        L1 = generate_product_matrix(A[I, :], BV_for_use.T)
        L2 = generate_product_matrix(AU_for_use, B[J, :].T)
        L = sparse.csr_matrix(np.concatenate((L1, L2), axis=1))

        update = alpha * np.sum(AU_omega_rows * BV_for_use, axis=1)
        b = x + update

        lsqr_tol = lsqr_inner_init_tol
        lsqr_iters = max_inner_iter_init
        if lsqr_smart_tol and relRes < lsqr_smart_obj_min:
            lsqr_tol = min(lsqr_tol, relRes**2)
            lsqr_iters = max_inner_iter_final

        z = sp_linalg.lsqr(L, b, atol=lsqr_tol, btol=lsqr_tol, iter_lim=lsqr_iters)[0]

        U_tilde = np.reshape(z[:A.shape[1]*rank], (A.shape[1], rank))
        V_tilde = np.reshape(z[A.shape[1]*rank:], (B.shape[1], rank))
        if perform_qr:
            U_tilde = U_tilde @ np.linalg.inv(V_R).T
            V_tilde = V_tilde @ np.linalg.inv(U_R).T

        U = 0.5 * (1 - alpha) * U + U_tilde
        V = 0.5 * (1 - alpha) * V + V_tilde
        AU_omega_rows = A[I, :] @ U
        BV_omega_cols = B[J, :] @ V

        x_hat = np.sum(AU_omega_rows * BV_omega_cols, axis=1)
        relRes = np.linalg.norm(x_hat - x) / X_norm
        all_relRes.append(relRes)
        if relRes < best_relRes:
            best_relRes = relRes
            U_best = U.copy()
            V_best = V.copy()
        x_hat_diff = (np.linalg.norm(x_hat - x_hat_prev) / np.linalg.norm(x_hat)
                      if np.linalg.norm(x_hat) != 0 else 0)
        x_hat_prev = x_hat.copy()

        if verbose:
            print(f"[INSIDE GNIMC] iter: {iter_num}, relRes: {relRes:.6e}")

        if stop_relRes > 0 and relRes < stop_relRes:
            early_stopping_flag = True
        if stop_relDiff > 0 and x_hat_diff < stop_relDiff:
            early_stopping_flag = True
        if stop_relResDiff > 0 and len(all_relRes) >= 2:
            rel_res_diff = abs(relRes / all_relRes[-2] - 1)
            if rel_res_diff < stop_relResDiff:
                early_stopping_flag = True
        if verbose and early_stopping_flag:
            print("[INSIDE GNIMC] Early stopping")

    convergence_flag = iter_num < max_outer_iter
    X_hat = A @ U_best @ V_best.T @ B.T
    return X_hat, iter_num, convergence_flag, all_relRes, U_best, V_best

def prepare_features(A, B, M, poly=None):
    """
    Prepare features for residual model training, e.g., polynomial expansion.
    """
    n_queries, n_hints = M.shape
    A_expanded = np.repeat(A, n_hints, axis=0)
    B_expanded = np.tile(B, (n_queries, 1))
    features = np.hstack((A_expanded, B_expanded))

    if poly is None:
        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
    features_poly = poly.fit_transform(features)

    print(f"üîÑ Feature matrix shape after polynomial expansion: {features_poly.shape}")
    return features_poly, poly

def initialize_residual_tracker(n_queries, n_hints):
    return np.zeros((n_queries, n_hints, 0))

def update_residual_tracker(tracker, residuals):
    residuals_expanded = residuals[:, :, np.newaxis]
    return np.concatenate((tracker, residuals_expanded), axis=2)

def train_residual_model(W_true, X_hat, A, B, M, scaler=None, poly=None, residual_model=None, train_observed_only=True):
    """
    Train a residual model with weighted samples to emphasize positive residuals.
    ### FIX/UPDATE ###
    - We apply StandardScaler to the polynomial features,
      and save the EXACT scaler for use at inference time.
    """
    print("\n=== Training Residual Model with Weighted Samples ===")
    
    residuals = W_true - X_hat
    n_queries, n_hints = W_true.shape
    
    # Prepare feature matrix for residuals
    features_poly, poly = prepare_features(A, B, M, poly)
    residuals_flat = residuals.flatten()

    if train_observed_only:
        observed_indices = M.flatten() == 1
        features_train = features_poly[observed_indices]
        residuals_train = residuals_flat[observed_indices]
    else:
        features_train = features_poly
        residuals_train = residuals_flat

    alpha_weight = 1
    sample_weights = 1 + alpha_weight * (residuals_train > 0).astype(float)

    if residual_model is None:
        residual_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=2000,
            learning_rate=0.02,
            max_depth=10,
            subsample=0.8,
            colsample_bytree=0.8,
            verbosity=1,
            n_jobs=-1,
            random_state=42
        )

    ### FIX/UPDATE ###
    # Use StandardScaler to fit & transform
    if scaler is None:
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_train)
    else:
        features_scaled = scaler.transform(features_train)

    # Train the residual model on scaled polynomial features
    residual_model.fit(features_scaled, residuals_train, sample_weight=sample_weights)

    print("‚úÖ Residual Model Trained with Weighted Samples.")

    # Predict residuals for all entries
    all_scaled = scaler.transform(features_poly)
    residuals_pred = residual_model.predict(all_scaled)
    residuals_pred = residuals_pred.reshape(n_queries, n_hints)

    # Adjust X_hat
    adjusted_X_hat = X_hat + residuals_pred
    adjusted_X_hat = np.maximum(adjusted_X_hat, 0)
    print("‚úÖ X_hat Adjusted with Residuals.")
    
    return adjusted_X_hat, residual_model, scaler, poly

def LIMEQO(W_true, M, A, B, k, lambda_, t, m_select=3):
    """
    LIME-QO algorithm for hint selection (same code as your original).
    """
    W_tilde = np.copy(W_true)
    W_tilde[M == 0] = 0
    n_queries, n_hints = W_true.shape
    hint_selections = [None] * n_queries
    all_selected_hints = []

    for iteration in range(t):
        print(f"\n--- Iteration {iteration+1} ---")

        for i in range(n_queries):
            observed_indices = np.where(M[i] == 1)[0]
            if observed_indices.size == 0:
                continue
            j_min = observed_indices[np.argmin(W_tilde[i, observed_indices])]
            hint_selections[i] = j_min

        omega = sparse.csr_matrix(M)
        print("üîÑ Performing GNIMC matrix completion...")

        W_hat, _, _, _, U_best, V_best = GNIMC(
            X=W_tilde,
            omega=omega,
            rank=k,
            A=A,
            B=B,
            verbose=False,
            alpha=0.04,
            perform_qr=True,
            max_outer_iter=100
        )
        print("‚úÖ GNIMC completed.")

        S = []
        for i in range(n_queries):
            j_hat_min = np.argmin(W_hat[i, :])
            if np.any(M[i] == 1):
                min_W_tilde_i = np.min(W_tilde[i, M[i] == 1])
            else:
                min_W_tilde_i = np.inf
            delta_Wi = min_W_tilde_i - W_hat[i, j_hat_min]
            if delta_Wi > 0 and M[i, j_hat_min] == 0:
                S.append((i, j_hat_min, delta_Wi))

        print(f"üîç Selected {len(S)} potential improvements based on Delta W.")
        S_sorted = sorted(S, key=lambda x: x[2], reverse=True)
        top_S = S_sorted[:m_select]

        if len(top_S) < m_select:
            needed = m_select - len(top_S)
            unobserved = np.argwhere(M == 0)
            np.random.shuffle(unobserved)
            for idx in unobserved:
                i, j = idx
                if not any((i == pair[0] and j == pair[1]) for pair in top_S):
                    top_S.append((i, j, 0))
                    needed -= 1
                    if needed == 0:
                        break

        print("üìù Updating M and W_tilde with selected hints...")
        for (i, j, delta_Wi) in top_S:
            if M[i, j] == 0:
                M[i, j] = 1
                W_tilde[i, j] = W_true[i, j]
                if delta_Wi > 0:
                    print(f"üîß Hint selected at (i={i}, j={j}) with Delta_W={delta_Wi:.4f}")
                else:
                    print(f"üîÄ Randomly selected hint at (i={i}, j={j})")

        print(f"Selected {len(top_S)} hints to observe.")
        all_selected_hints.extend([(i, j) for (i, j, _) in top_S])
        if len(S_sorted) == 0:
            print("üîí No potential improvements found. Stopping iterations.")
            break

    for i in range(n_queries):
        observed_indices = np.where(M[i] == 1)[0]
        if observed_indices.size == 0:
            hint_selections[i] = None
        else:
            j_min = observed_indices[np.argmin(W_tilde[i, observed_indices])]
            hint_selections[i] = j_min

    return hint_selections, W_tilde, M, U_best, V_best, all_selected_hints

if __name__ == "__main__":
    # Example driver code (adapt paths as needed)
    try:
        W_true = np.load('/Users/raahimlone/New_Data/W_low_rank.npy')
        M = np.load('/Users/raahimlone/New_Data/M.npy')

        data_X = np.load('/Users/raahimlone/New_Data/X_scaled.npz')
        A = data_X['data']
        data_Y = np.load('/Users/raahimlone/New_Data/Y_scaled.npz')
        B = data_Y['Y']
    except Exception as e:
        print(f"Error loading data: {e}")
        exit(1)

    rank = 5
    lambda_ = 0.05
    t = 5
    m = 5

    # Example usage ...
    # (Same flow as your original script: LIMEQO, residual model training, repeated steps, etc.)

    # 1) Possibly run LIMEQO to pick hints
    # 2) Solve GNIMC to get U_best, V_best
    # 3) Train residual model & compute final X_hat
    # 4) Save everything

    # For brevity, we show just one cycle:

    # Step: LIMEQO
    hints, W_tilde, M_copy, U_best, V_best, selected_hints = LIMEQO(
        W_true=W_true,
        M=M,
        A=A,
        B=B,
        k=rank,
        lambda_=lambda_,
        t=1,
        m_select=m
    )

    # Step: Get final X_hat from U_best, V_best
    X_hat_final = A @ U_best @ V_best.T @ B.T

    # Step: Train residual model
    X_hat_final, residual_model, scaler, poly = train_residual_model(
        W_true=W_true,
        X_hat=X_hat_final,
        A=A,
        B=B,
        M=M_copy,
        train_observed_only=True
    )

    # Save final results
    np.save('/Users/raahimlone/New_Data/X_hat_final.npy', X_hat_final)
    np.save('/Users/raahimlone/New_Data/U_best.npy', U_best)
    np.save('/Users/raahimlone/New_Data/V_best.npy', V_best)

    # ### FIX/UPDATE ###
    # Save the EXACT scaler & poly used in training
    joblib.dump(residual_model, '/Users/raahimlone/New_Data/residual_model_xgb.pkl')
    joblib.dump(scaler,         '/Users/raahimlone/New_Data/residual_model_scaler.pkl')
    joblib.dump(poly,           '/Users/raahimlone/New_Data/residual_model_poly.pkl')

    print("\nAll done.")
