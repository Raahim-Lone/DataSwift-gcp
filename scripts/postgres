#!/usr/bin/env python3
"""
Script to execute SQL queries against PostgreSQL and track their latencies using EXPLAIN ANALYZE.
"""

import os
import sys
import logging
import psycopg2
import pandas as pd
import re

def configure_environment():
    """
    Sets up the environment by configuring the Python path.
    Modify this function if your project structure requires specific path configurations.
    """
    import sys
    import os

    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before any imports that depend on it
configure_environment()

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def run_query_postgres_explain_analyze(query_str, pg_host, pg_db, pg_user, pg_password, port=5432):
    """
    Executes EXPLAIN ANALYZE on the given query and returns the total execution time in seconds.
    """
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host,
            dbname=pg_db,
            user=pg_user,
            password=pg_password,
            port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            explain_query = f"EXPLAIN ANALYZE {query_str}"
            cur.execute(explain_query)
            explain_output = cur.fetchall()

            # Parse the execution time from EXPLAIN ANALYZE output
            execution_time = parse_explain_analyze_output(explain_output)
            return execution_time
    except Exception as e:
        logger.error(f"Error executing EXPLAIN ANALYZE: {e}")
        return float("inf")  # Indicate failure
    finally:
        if conn:
            conn.close()

def parse_explain_analyze_output(explain_output):
    """
    Parses the EXPLAIN ANALYZE output to extract the total execution time in seconds.
    """
    total_time = None
    for row in explain_output:
        line = row[0]
        match = re.search(r"Execution Time:\s+([\d.]+) ms", line)
        if match:
            total_time_ms = float(match.group(1))
            total_time = total_time_ms / 1000.0  # Convert to seconds
            break
    if total_time is None:
        logger.warning("Could not find Execution Time in EXPLAIN ANALYZE output.")
        return float("inf")  # Indicate failure to parse
    return total_time

def main():
    # Configuration
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"      # <-- Update this path
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results.csv"        # <-- Update this path

    PG_HOST = "localhost"                           # Update if different
    PG_DB = "IMDB"                         # <-- Update this
    PG_USER = "postgres"                       # <-- Update this
    PG_PASSWORD = "raahimlhere"                   # <-- Update this
    PG_PORT = 6543                                  # Update if different

    # 1) Verify Queries Directory
    if not os.path.isdir(QUERIES_DIR):
        logger.error(f"Queries directory not found: {QUERIES_DIR}")
        sys.exit(1)

    # 2) Gather SQL Files
    sql_files = [f for f in os.listdir(QUERIES_DIR) if f.endswith(".sql")]
    sql_files.sort()
    logger.info(f"Found {len(sql_files)} SQL files to process.")

    if not sql_files:
        logger.warning("No SQL files found to process.")
        sys.exit(0)

    # 3) Process Each Query
    results = []
    for fname in sql_files:
        fpath = os.path.join(QUERIES_DIR, fname)
        logger.info(f"Processing {fname}...")
        try:
            with open(fpath, "r", encoding="utf-8") as ff:
                query_str = ff.read().strip()
            if not query_str:
                logger.warning(f"{fname} => Empty query.")
                results.append({
                    "filename": fname,
                    "latency_s": None,
                    "status": "Empty Query"
                })
                continue

            # Execute EXPLAIN ANALYZE
            latency = run_query_postgres_explain_analyze(
                query_str=query_str,
                pg_host=PG_HOST,
                pg_db=PG_DB,
                pg_user=PG_USER,
                pg_password=PG_PASSWORD,
                port=PG_PORT
            )

            if latency == float("inf"):
                logger.warning(f"{fname} => Failed to measure latency.")
                results.append({
                    "filename": fname,
                    "latency_s": None,
                    "status": "Execution Failed"
                })
                continue

            logger.info(f"{fname} => Latency: {latency:.4f} seconds")
            results.append({
                "filename": fname,
                "latency_s": latency,
                "status": "Success"
            })

        except Exception as e:
            logger.error(f"Error processing {fname}: {e}")
            results.append({
                "filename": fname,
                "latency_s": None,
                "status": f"Error: {str(e)}"
            })

    # 4) Save Results to CSV
    if OUTPUT_CSV and OUTPUT_CSV.strip():
        try:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_CSV, index=False)
            logger.info(f"Results saved to {OUTPUT_CSV}")
        except Exception as e:
            logger.error(f"Failed to save results to CSV: {e}")

    # 5) Print Summary
    if results:
        print("\n=== Query Execution Summary ===")
        for r in results:
            if r["status"] != "Success":
                print(f"{r['filename']} => {r['status']}")
            else:
                print(f"{r['filename']} => Latency: {r['latency_s']:.4f} seconds")
    else:
        print("No results to display.")
    logger.info("All queries processed.")

if __name__ == "__main__":
    main()
