#!/usr/bin/env python3
"""
Gauss-Newton based algorithm for inductive matrix completion with enhanced residual modeling.
Enhanced with Weighted Residuals, Residual Tracking, Iterative Refinement, and LIME-QO.
This version includes debugging messages and forces output flushing.
It also pre-processes W_true to obtain a low-rank approximation.
"""

import sys
import numpy as np
from scipy import sparse
from scipy.sparse import linalg as sp_linalg
import joblib  # For model persistence
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
import xgboost as xgb  # For residual modeling
import os

# ---------------- Debugging helper ----------------
DEBUG = True
def dprint(msg):
    if DEBUG:
        print(msg, flush=True)

# ---------------- Low-Rank Preprocessing ----------------
def make_low_rank(W, r):
    """
    Returns a rank-r approximation of W using truncated SVD.
    """
    U, s, Vt = np.linalg.svd(W, full_matrices=False)
    return U[:, :r] @ np.diag(s[:r]) @ Vt[:r, :]

# -----------------------------------------------------------------------------
# Initialization Options
# -----------------------------------------------------------------------------
INIT_WITH_SVD = 0
INIT_WITH_RANDOM = 1
INIT_WITH_USER_DEFINED = 2

def generate_product_matrix(A, B):
    """
    Returns M such that M @ vec(C) = vec(A @ C @ B)
    """
    assert A.shape[0] == B.shape[1], 'Error: dimension mismatch'
    m = A.shape[0]
    M = np.zeros((m, A.shape[1] * B.shape[0]))
    for i in range(m):
        AB = np.outer(A[i, :], B[:, i])
        M[i, :] = AB.flatten()
    return M

def GNIMC(X, omega, rank, A, B, verbose=True, alpha=0.04, perform_qr=True,
          max_outer_iter=500, max_inner_iter_init=1000, max_inner_iter_final=100, 
          lsqr_inner_init_tol=1e-6, lsqr_smart_tol=True, lsqr_smart_obj_min=1e-5,
          init_option=INIT_WITH_SVD, init_U=None, init_V=None,
          stop_relRes=1e-2, stop_relDiff=-1, stop_relResDiff=-1):
    """
    Run GNIMC algorithm for inductive matrix completion.
    X is the partial matrix with observed entries, and 'omega' is its mask in sparse format.
    A ‚àà ‚Ñù^(n1√ód1) and B ‚àà ‚Ñù^(n2√ód2) are the side information matrices.
    """
    n1, n2 = X.shape
    d1 = A.shape[1]
    d2 = B.shape[1]
    m = omega.nnz  # Use the sparse matrix attribute for number of nonzeros
    p = m / (n1 * n2)
    I, J, _ = sparse.find(omega)

    # Initial estimate
    if init_option == INIT_WITH_SVD:
        L, S, R = sp_linalg.svds(X / p, k=rank, tol=1e-16)
        U = A.T @ L @ np.diag(np.sqrt(S))
        V = B.T @ R.T @ np.diag(np.sqrt(S))
    elif init_option == INIT_WITH_RANDOM:
        U = np.linalg.qr(np.random.randn(d1, rank))[0]
        V = np.linalg.qr(np.random.randn(d2, rank))[0]
    else:  # INIT_WITH_USER_DEFINED
        U = init_U
        V = init_V

    AU_omega_rows = A[I, :] @ U
    BV_omega_cols = B[J, :] @ V

    # x = vector of observed values
    x = X[I, J]
    X_norm = np.linalg.norm(x)
    early_stopping_flag = False
    relRes = float("inf")
    all_relRes = [relRes]
    best_relRes = float("inf")
    U_best = U.copy()
    V_best = V.copy()
    x_hat = np.sum(AU_omega_rows * BV_omega_cols, axis=1)
    x_hat_prev = x_hat.copy()

    iter_num = 0
    while iter_num < max_outer_iter and not early_stopping_flag:
        iter_num += 1

        if perform_qr:
            U_Q, U_R = np.linalg.qr(U)
            V_Q, V_R = np.linalg.qr(V)
            AU_for_use = A[I, :] @ U_Q
            BV_for_use = B[J, :] @ V_Q
        else:
            AU_for_use = AU_omega_rows
            BV_for_use = BV_omega_cols

        # Build L for the normal equation
        L1 = generate_product_matrix(A[I, :], BV_for_use.T)
        L2 = generate_product_matrix(AU_for_use, B[J, :].T)
        L = sparse.csr_matrix(np.concatenate((L1, L2), axis=1))
        
        # Build b vector
        update = alpha * np.sum(AU_omega_rows * BV_for_use, axis=1)
        b = x + update

        lsqr_tol = lsqr_inner_init_tol
        lsqr_iters = max_inner_iter_init
        if lsqr_smart_tol and relRes < lsqr_smart_obj_min:
            lsqr_tol = min(lsqr_tol, relRes**2)
            lsqr_iters = max_inner_iter_final

        z = sp_linalg.lsqr(L, b, atol=lsqr_tol, btol=lsqr_tol, iter_lim=lsqr_iters)[0]

        U_tilde = np.reshape(z[:d1 * rank], (d1, rank))
        V_tilde = np.reshape(z[d1 * rank:], (rank, d2)).T
        if perform_qr:
            U_tilde = U_tilde @ np.linalg.inv(V_R).T
            V_tilde = V_tilde @ np.linalg.inv(U_R).T

        # Exponential moving average update
        U = (1 - alpha) * U + alpha * U_tilde
        V = (1 - alpha) * V + alpha * V_tilde
        AU_omega_rows = A[I, :] @ U
        BV_omega_cols = B[J, :] @ V
        
        x_hat = np.sum(AU_omega_rows * BV_omega_cols, axis=1)
        relRes = np.linalg.norm(x_hat - x) / X_norm
        all_relRes.append(relRes)
        if relRes < best_relRes:
            best_relRes = relRes
            U_best = U.copy()
            V_best = V.copy()

        x_hat_diff = (np.linalg.norm(x_hat - x_hat_prev) / np.linalg.norm(x_hat)
                      if np.linalg.norm(x_hat) != 0 else 0)
        x_hat_prev = x_hat.copy()

        if verbose:
            dprint(f"[INSIDE GNIMC] iter: {iter_num}, relRes: {relRes:.6e}")

        if stop_relRes > 0 and relRes < stop_relRes:
            early_stopping_flag = True
        if stop_relDiff > 0 and x_hat_diff < stop_relDiff:
            early_stopping_flag = True
        if stop_relResDiff > 0 and len(all_relRes) >= 2:
            rel_res_diff = np.abs(relRes / all_relRes[-2] - 1)
            if rel_res_diff < stop_relResDiff:
                early_stopping_flag = True
        if verbose and early_stopping_flag:
            dprint("[INSIDE GNIMC] Early stopping")

    convergence_flag = iter_num < max_outer_iter
    X_hat = A @ U_best @ V_best.T @ B.T
    return X_hat, iter_num, convergence_flag, all_relRes, U_best, V_best

# ---------------- Residual Modeling and Feature Engineering ----------------
def prepare_features(A, B, M, poly=None):
    """
    Prepare and engineer features for the residual model.
    Concatenates an expanded version of A and B for each observed entry.
    """
    n_queries, n_hints = M.shape
    A_expanded = np.repeat(A, n_hints, axis=0)
    B_expanded = np.tile(B, (n_queries, 1))
    features = np.hstack((A_expanded, B_expanded))
    if poly is None:
        poly = PolynomialFeatures(degree=1, interaction_only=True, include_bias=False)
    features_poly = poly.fit_transform(features)
    dprint(f"üîÑ Feature matrix shape after polynomial expansion: {features_poly.shape}")
    return features_poly, poly

def initialize_residual_tracker(n_queries, n_hints):
    """Initialize a tracker for residuals per entry."""
    return np.zeros((n_queries, n_hints, 0))

def update_residual_tracker(tracker, residuals):
    """Update the residual tracker with current residuals."""
    residuals_expanded = residuals[:, :, np.newaxis]
    return np.concatenate((tracker, residuals_expanded), axis=2)

def train_residual_model(W_true, X_hat, A, B, M,
                         scaler=None, poly=None, residual_model=None,
                         train_observed_only=True):
    """
    Train a residual model with weighted samples.
    """
    dprint("\n=== Training Residual Model with Weighted Samples ===")
    residuals = W_true - X_hat
    n_queries, n_hints = W_true.shape
    features_poly, poly = prepare_features(A, B, M, poly)
    residuals_flat = residuals.flatten()

    if train_observed_only:
        observed_indices = (M.flatten() == 1)
        features_train = features_poly[observed_indices]
        residuals_train = residuals_flat[observed_indices]
    else:
        features_train = features_poly
        residuals_train = residuals_flat

    alpha_weight = 1
    sample_weights = 1 + alpha_weight * (residuals_train > 0).astype(float)
    
    dprint("üîÑ Initializing and training XGBoost regressor with sample weights...")
    if residual_model is None:
        residual_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=2000,
            learning_rate=0.005,
            max_depth=10,
            subsample=0.7,
            colsample_bytree=0.8,
            verbosity=1,
            n_jobs=-1,
            random_state=42
        )

    dprint("üîÑ Scaling features with StandardScaler...")
    if scaler is None:
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_train)
    else:
        features_scaled = scaler.transform(features_train)

    dprint("üîÑ Training residual model...")
    residual_model.fit(features_scaled, residuals_train, sample_weight=sample_weights)
    dprint("‚úÖ Residual Model Trained with Weighted Samples.")

    dprint("üîÑ Predicting residuals for all entries...")
    features_all_scaled = scaler.transform(features_poly)
    residuals_pred = residual_model.predict(features_all_scaled)
    residuals_pred = residuals_pred.reshape(n_queries, n_hints)

    dprint("üîÑ Adjusting X_hat with predicted residuals...")
    adjusted_X_hat = X_hat + residuals_pred
    adjusted_X_hat = np.maximum(adjusted_X_hat, 0)
    dprint("‚úÖ X_hat Adjusted with Residuals.")
    
    return adjusted_X_hat, residual_model, scaler, poly

def build_observed_matrix(W, M):
    """
    Construct a matrix W_tilde such that:
    W_tilde[i,j] = W[i,j] if M[i,j] == 1, else 0.
    """
    W_tilde = np.zeros_like(W)
    W_tilde[M == 1] = W[M == 1]
    return W_tilde

def LIMEQO(W_true, M, A, B, k, lambda_, t, m_select=3):
    """
    Perform the LIME-QO algorithm for hint selection.
    Builds an "observed-only" matrix W_tilde from W_true and M.
    """
    n_queries, n_hints = W_true.shape
    hint_selections = [None] * n_queries
    all_selected_hints = []

    for iteration in range(t):
        dprint(f"\n--- Iteration {iteration+1} ---")
        W_tilde = build_observed_matrix(W_true, M)

        # For each row, pick the best observed hint (lowest cost)
        for i in range(n_queries):
            observed_indices = np.where(M[i] == 1)[0]
            if observed_indices.size == 0:
                continue
            j_min = observed_indices[np.argmin(W_tilde[i, observed_indices])]
            hint_selections[i] = j_min

        omega = sparse.csr_matrix(M)
        dprint("üîÑ Performing GNIMC matrix completion...")
        W_hat, _, _, _, U_best, V_best = GNIMC(
            X=W_tilde,
            omega=omega,
            rank=k,
            A=A,
            B=B,
            verbose=True,
            alpha=0.2,
            perform_qr=True,
            max_outer_iter=1000,
        )
        dprint("‚úÖ GNIMC completed.")

        S = []
        for i in range(n_queries):
            j_hat_min = np.argmin(W_hat[i, :])
            if np.any(M[i] == 1):
                min_W_tilde_i = np.min(W_tilde[i, M[i] == 1])
            else:
                min_W_tilde_i = np.inf
            delta_Wi = min_W_tilde_i - W_hat[i, j_hat_min]
            if delta_Wi > 0 and M[i, j_hat_min] == 0:
                S.append((i, j_hat_min, delta_Wi))

        dprint(f"üîç Selected {len(S)} potential improvements based on Delta W.")
        S_sorted = sorted(S, key=lambda x: x[2], reverse=True)
        top_S = S_sorted[:m_select]
        dprint(f"Number of selected hints in top_S: {len(top_S)}")

        if len(top_S) < m_select:
            needed = m_select - len(top_S)
            unobserved = np.argwhere(M == 0)
            np.random.shuffle(unobserved)
            for idx in unobserved:
                i, j = idx
                if not any((i == pair[0] and j == pair[1]) for pair in top_S):
                    top_S.append((i, j, 0))
                    needed -= 1
                    if needed == 0:
                        break

        dprint("üìù Updating M with selected hints...")
        for (i, j, delta_Wi) in top_S:
            if M[i, j] == 0:
                M[i, j] = 1
                if delta_Wi > 0:
                    dprint(f"üîß Hint selected at (i={i}, j={j}) with Delta_W={delta_Wi:.4f}")
                else:
                    dprint(f"üîÄ Randomly selected hint at (i={i}, j={j})")
        dprint(f"Selected {len(top_S)} hints to observe.")
        all_selected_hints.extend([(i, j) for (i, j, _) in top_S])

        if len(S_sorted) == 0:
            dprint("üîí No potential improvements found. Stopping iterations.")
            break

    W_tilde_final = build_observed_matrix(W_true, M)
    for i in range(n_queries):
        observed_indices = np.where(M[i] == 1)[0]
        if observed_indices.size == 0:
            hint_selections[i] = None
        else:
            j_min = observed_indices[np.argmin(W_tilde_final[i, observed_indices])]
            hint_selections[i] = j_min

    return hint_selections, W_tilde_final, M, U_best, V_best, all_selected_hints

# -------------------------------------------------------------------------
# Main Script / Example Usage
# -------------------------------------------------------------------------
if __name__ == "__main__":
    try:
        W_true = np.load('/Users/raahimlone/New_Data/W.npy')
        dprint("‚úÖ W_true Loaded:")
        dprint(W_true)
        dprint(f"Shape of W_true: {W_true.shape}")
    except FileNotFoundError:
        dprint("Error: W.npy not found at the specified path.")
        sys.exit(1)

    try:
        M = np.load('/Users/raahimlone/New_Data/omega.npy')
        dprint("\n‚úÖ Mask Matrix (M) Loaded:")
        dprint(M)
        dprint(f"Shape of M: {M.shape}")
    except FileNotFoundError:
        dprint("Error: omega.npy not found at the specified path.")
        sys.exit(1)

    try:
        data_X = np.load('/Users/raahimlone/New_Data/X.npz')
        A = data_X['X']
        dprint("\n‚úÖ Side Information Matrix A Loaded from X_scalednew.npz:")
        dprint(A)
        dprint(f"Shape of A: {A.shape}")
    except (FileNotFoundError, KeyError) as e:
        dprint("Error loading A from X_scalednew.npz.")
        sys.exit(1)

    try:
        data_Y = np.load('/Users/raahimlone/New_Data/Y_expanded.npz')
        B = data_Y['Y']
        dprint("\n‚úÖ Side Information Matrix B Loaded from Y_expanded.npz:")
        dprint(B)
        dprint(f"Shape of B: {B.shape}")
    except (FileNotFoundError, KeyError) as e:
        dprint("Error loading B from Y_expanded.npz.")
        sys.exit(1)

    # ------------------- Scaling Step -------------------
    W_scale = 100.0
    # Scale W_true by constant factor.
    W_true_scaled = W_true / W_scale

    # Instead of constant division for A and B, use StandardScaler.
    from sklearn.preprocessing import StandardScaler
    scaler_A = StandardScaler()
    scaler_B = StandardScaler()
    A_scaled = scaler_A.fit_transform(A)
    B_scaled = scaler_B.fit_transform(B)

    dprint("‚úÖ W_true has been replaced with its low-rank approximation after scaling.")
    rank = 12  # Must be ‚â§ number of columns in B (which is 12)
    W_true_scaled = make_low_rank(W_true_scaled, rank)

    lambda_ = 0.05  
    t = 1  
    m = 1  

    # Keep a copy of M for incremental hint selection.
    M_copy_current = np.copy(M)
    n_queries, n_hints = W_true.shape
    residual_tracker = initialize_residual_tracker(n_queries, n_hints)

    initial_alterations = 1
    additional_alterations = 1

    dprint("\n=== Initial Phase: Performing Alterations ===")
    for i in range(1, initial_alterations + 1):
        dprint(f"\n--- Initial Alteration {i} ---")
        hints, W_tilde_current, M_copy_current, U_best, V_best, selected_hints = LIMEQO(
            W_true=W_true_scaled,
            M=M_copy_current,
            A=A_scaled,
            B=B_scaled,
            k=rank,
            lambda_=lambda_,
            t=1,
            m_select=m
        )
        residuals_temp = W_true_scaled - (A_scaled @ U_best @ V_best.T @ B_scaled.T)
        residual_tracker = update_residual_tracker(residual_tracker, residuals_temp)

    dprint("\n=== Computing X_hat after Initial Alterations ===")
    X_hat_final = A_scaled @ U_best @ V_best.T @ B_scaled.T
    dprint("‚úÖ X_hat after Initial Alterations:")
    dprint(X_hat_final)
    negative_indices = np.where(X_hat_final < 0)
    num_negatives = len(negative_indices[0])
    dprint(f"\nNumber of negative entries in X_hat_final after Initial Alterations: {num_negatives}")

    dprint("\n=== Training Initial Residual Model ===")
    X_hat_final, residual_model, scaler, poly = train_residual_model(
        W_true=W_true_scaled,
        X_hat=X_hat_final,
        A=A_scaled,
        B=B_scaled,
        M=M_copy_current,
        train_observed_only=True
    )
    residuals = W_true_scaled - X_hat_final
    residual_tracker = update_residual_tracker(residual_tracker, residuals)
    X_norm = np.linalg.norm(W_true_scaled)
    relRes = np.linalg.norm(residuals) / X_norm
    dprint(f"\nRelative Residual after Residual Model Adjustment: {relRes:.6e}")

    target_relRes = 0.1
    max_iterations = 5
    current_iteration = 0

    while relRes > target_relRes and current_iteration < max_iterations:
        current_iteration += 1
        dprint(f"\n=== Iterative Residual Correction Phase: Iteration {current_iteration} ===")
        X_hat_final, residual_model, scaler, poly = train_residual_model(
            W_true=W_true_scaled,
            X_hat=X_hat_final,
            A=A_scaled,
            B=B_scaled,
            M=M_copy_current,
            scaler=scaler,
            poly=poly,
            residual_model=residual_model,
            train_observed_only=True
        )
        residuals = W_true_scaled - X_hat_final
        residual_tracker = update_residual_tracker(residual_tracker, residuals)
        relRes = np.linalg.norm(residuals) / X_norm
        dprint(f"Relative Residual after Iteration {current_iteration}: {relRes:.6e}")

    if relRes > target_relRes:
        dprint(f"\n‚ùó Warning: Desired relative residual of {target_relRes} not achieved after {max_iterations} iterations.")
    else:
        dprint(f"\n‚úÖ Desired relative residual of {target_relRes} achieved after {current_iteration} iterations.")

    if relRes > target_relRes:
        dprint("\n=== Verification Phase: Performing Additional Alterations ===")
        for j in range(1, additional_alterations + 1):
            dprint(f"\n--- Additional Alteration {j} ---")
            hints, W_tilde_current, M_copy_current, U_best, V_best, selected_hints = LIMEQO(
                W_true=W_true_scaled,
                M=M_copy_current,
                A=A_scaled,
                B=B_scaled,
                k=rank,
                lambda_=lambda_,
                t=1,
                m_select=m
            )
            X_hat_final = A_scaled @ U_best @ V_best.T @ B_scaled.T
            dprint("‚úÖ X_hat after Additional Alteration:")
            dprint(X_hat_final)

            X_hat_final, residual_model, scaler, poly = train_residual_model(
                W_true=W_true_scaled,
                X_hat=X_hat_final,
                A=A_scaled,
                B=B_scaled,
                M=M_copy_current,
                scaler=scaler,
                poly=poly,
                residual_model=residual_model,
                train_observed_only=True
            )
            residuals = W_true_scaled - X_hat_final
            residual_tracker = update_residual_tracker(residual_tracker, residuals)
            relRes = np.linalg.norm(residuals) / X_norm
            dprint(f"Relative Residual after Alteration {j}: {relRes:.6e}")
            if relRes <= target_relRes:
                dprint("\n‚úÖ Desired relative residual achieved. Stopping additional alterations.")
                break
        if relRes > target_relRes:
            dprint(f"\nReached maximum of {additional_alterations} additional alterations with relative residual: {relRes:.6e}")
    else:
        dprint("\nAll values in X_hat_final are sufficiently accurate after initial alterations and residual modeling.")

    negative_indices = np.where(X_hat_final < 0)
    if len(negative_indices[0]) > 0:
        X_hat_final[negative_indices] = 0
        dprint("\n‚úÖ Set remaining negative entries in X_hat_final to zero.")

    dprint("\n‚úÖ Final X_hat Values (scaled):")
    dprint(X_hat_final)

    X_hat_final_rescaled = X_hat_final * W_scale
    np.save('/Users/raahimlone/New_Data/X_hat_final.npy', X_hat_final_rescaled)
    dprint("\n‚úÖ Final X_hat matrix saved to '/Users/raahimlone/New_Data/X_hat_final.npy'.")

    np.save('/Users/raahimlone/New_Data/U_best.npy', U_best)
    np.save('/Users/raahimlone/New_Data/V_best.npy', V_best)
    dprint("\n‚úÖ U_best and V_best matrices saved successfully.")

    dprint("\n‚úÖ Saving Residual Model and Preprocessing Objects...")
    joblib.dump(residual_model, '/Users/raahimlone/New_Data/residual_model_xgb.pkl')
    joblib.dump(scaler, '/Users/raahimlone/New_Data/residual_model_scaler.pkl')
    joblib.dump(poly, '/Users/raahimlone/New_Data/residual_model_poly.pkl')
    joblib.dump(scaler_A, '/Users/raahimlone/New_Data/scaler_A.pkl')
    joblib.dump(scaler_B, '/Users/raahimlone/New_Data/scaler_B.pkl')
    dprint("‚úÖ Residual model, feature scalers, and preprocessing objects saved successfully.")
