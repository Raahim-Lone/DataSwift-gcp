def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()

import pandas as pd
import numpy as np
from scipy.sparse import coo_matrix, save_npz, load_npz
import os
import argparse
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config
from sklearn.preprocessing import StandardScaler
import logging

def build_feature_matrices(config):
    """
    Build feature matrices and association matrices for Inductive Matrix Completion.

    Parameters:
    - config: Configuration dictionary
    """
    logger = setup_logger(
        __name__,
        config["logging"]["log_file"],
        level=getattr(logging, config["logging"]["log_level"].upper(), logging.INFO)
    )

    # Paths
    csv_file = config["database"]["csv_file"]
    output_path = config["paths"]["processed_features"]
    os.makedirs(output_path, exist_ok=True)

    # Load CSV data
    try:
        csv_data = pd.read_csv(csv_file, na_values=['', ' ', 'nan'], keep_default_na=True)
        logger.info(f"Loaded CSV data from {csv_file} with shape {csv_data.shape}")
    except Exception as e:
        logger.error(f"Error loading CSV file {csv_file}: {e}")
        exit(1)

    required_columns = {"query", "hint", "latency_seconds"}
    if not required_columns.issubset(csv_data.columns):
        logger.error(f"CSV file must contain columns: {required_columns}")
        exit(1)

    # Map queries and hints to indices
    queries = csv_data["query"].unique()
    hints = csv_data["hint"].unique()
    query_to_idx = {q: i for i, q in enumerate(queries)}
    hint_to_idx = {h: i for i, h in enumerate(hints)}

    # Save query and hint mappings
    queries_df = pd.DataFrame({"query": queries})
    hints_df = pd.DataFrame({"hint": hints})
    queries_df.to_csv(os.path.join(output_path, "queries.csv"), index=False)
    hints_df.to_csv(os.path.join(output_path, "hints.csv"), index=False)
    logger.info(f"Saved queries.csv and hints.csv to {output_path}")

    # Load query and hint features
    queries_features_file = config["paths"]["queries_features"]
    hints_features_file = config["paths"]["hints_features"]

    try:
        X_df = pd.read_parquet(queries_features_file) if queries_features_file.endswith('.parquet') else pd.read_csv(queries_features_file)
        logger.info(f"Loaded query features from {queries_features_file} with shape {X_df.shape}")
    except Exception as e:
        logger.error(f"Error loading query features from {queries_features_file}: {e}")
        exit(1)

    try:
        Y_df = pd.read_parquet(hints_features_file) if hints_features_file.endswith('.parquet') else pd.read_csv(hints_features_file)
        logger.info(f"Loaded hint features from {hints_features_file} with shape {Y_df.shape}")
    except Exception as e:
        logger.error(f"Error loading hint features from {hints_features_file}: {e}")
        exit(1)

    # Convert to NumPy arrays
    X = X_df.values
    Y = Y_df.values
    logger.info(f"Converted query features to NumPy array with shape {X.shape}")
    logger.info(f"Converted hint features to NumPy array with shape {Y.shape}")

    # Scale features
    scaler_X = StandardScaler()
    scaler_Y = StandardScaler()
    X_scaled = scaler_X.fit_transform(X)
    Y_scaled = scaler_Y.fit_transform(Y)
    logger.info("Feature matrices X and Y have been scaled using StandardScaler.")

    np.savez(os.path.join(output_path, "X.npz"), X=X_scaled)
    np.savez(os.path.join(output_path, "Y.npz"), Y=Y_scaled)
    logger.info(f"Saved scaled feature matrices as X.npz and Y.npz to {output_path}")

    # Create association matrix W, W_tilde, and mask matrix M
    N_q = len(queries)
    N_h = len(hints)
    W = np.zeros((N_q, N_h))
    W_tilde = np.full((N_q, N_h), np.inf)
    M = np.zeros((N_q, N_h))

    for _, row in csv_data.iterrows():
        i = query_to_idx[row['query']]
        j = hint_to_idx[row['hint']]
        val = row['latency_seconds']
        if pd.notna(val):
            W[i, j] = val
            W_tilde[i, j] = val
            M[i, j] = 1

# Save as Dense Matrices
    np.save(os.path.join(output_path, "W.npy"), W)
    np.save(os.path.join(output_path, "W_tilde.npy"), W_tilde)
    np.save(os.path.join(output_path, "M.npy"), M)

    logger.info(f"Saved association matrices W.npz, W_tilde.npz, and M.npz to {output_path}")
    logger.info(f"Unique values in M: {np.unique(M)}")
    logger.info(f"Number of observed entries: {np.sum(M)} / {N_q * N_h}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build Feature and Association Matrices for IMC")
    parser.add_argument("--config", type=str, default="config/default_config.yaml", help="Path to config file")
    args = parser.parse_args()

    config = load_config(args.config)
    build_feature_matrices(config)

'''
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from src
configure_environment()

import pandas as pd
import numpy as np
from scipy.sparse import coo_matrix, save_npz, load_npz
import os
import argparse
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config
from sklearn.preprocessing import StandardScaler
import logging

def build_feature_matrices(config):
    """
    Build feature matrices and association matrices for Inductive Matrix Completion.

    Parameters:
    - config: Configuration dictionary
    """
    logger = setup_logger(
        __name__,
        config["logging"]["log_file"],
        level=getattr(logging, config["logging"]["log_level"].upper(), logging.INFO)
    )

    # Paths
    csv_file = config["database"]["csv_file"]
    output_path = config["paths"]["processed_features"]
    os.makedirs(output_path, exist_ok=True)

    # Load CSV data
    try:
        csv_data = pd.read_csv(csv_file, na_values=['', ' ', 'nan'], keep_default_na=True)
        logger.info(f"Loaded CSV data from {csv_file} with shape {csv_data.shape}")
    except Exception as e:
        logger.error(f"Error loading CSV file {csv_file}: {e}")
        exit(1)

    required_columns = {"query", "hint", "latency_seconds"}
    if not required_columns.issubset(csv_data.columns):
        logger.error(f"CSV file must contain columns: {required_columns}")
        exit(1)

    # Map queries and hints to indices
    queries = csv_data["query"].unique()
    hints = csv_data["hint"].unique()
    query_to_idx = {q: i for i, q in enumerate(queries)}
    hint_to_idx = {h: i for i, h in enumerate(hints)}

    # Save query and hint mappings
    queries_df = pd.DataFrame({"query": queries})
    hints_df = pd.DataFrame({"hint": hints})
    queries_df.to_csv(os.path.join(output_path, "queries.csv"), index=False)
    hints_df.to_csv(os.path.join(output_path, "hints.csv"), index=False)
    logger.info(f"Saved queries.csv and hints.csv to {output_path}")

    # Load query features from file (CSV or Parquet)
    queries_features_file = config["paths"]["queries_features"]
    if queries_features_file.endswith('.parquet'):
        try:
            X_df = pd.read_parquet(queries_features_file)
            logger.info(f"Loaded query features from {queries_features_file} with shape {X_df.shape}")
        except Exception as e:
            logger.error(f"Error loading query features from {queries_features_file}: {e}")
            exit(1)
    elif queries_features_file.endswith('.csv'):
        try:
            X_df = pd.read_csv(queries_features_file)
            logger.info(f"Loaded query features from {queries_features_file} with shape {X_df.shape}")
        except Exception as e:
            logger.error(f"Error loading query features from {queries_features_file}: {e}")
            exit(1)
    else:
        logger.error("Unsupported file format for queries_features_file. Use .csv or .parquet.")
        exit(1)

    # Load hint features from file (CSV or Parquet)
    hints_features_file = config["paths"]["hints_features"]
    if hints_features_file.endswith('.parquet'):
        try:
            Y_df = pd.read_parquet(hints_features_file)
            logger.info(f"Loaded hint features from {hints_features_file} with shape {Y_df.shape}")
        except Exception as e:
            logger.error(f"Error loading hint features from {hints_features_file}: {e}")
            exit(1)
    elif hints_features_file.endswith('.csv'):
        try:
            Y_df = pd.read_csv(hints_features_file)
            logger.info(f"Loaded hint features from {hints_features_file} with shape {Y_df.shape}")
        except Exception as e:
            logger.error(f"Error loading hint features from {hints_features_file}: {e}")
            exit(1)
    else:
        logger.error("Unsupported file format for hints_features_file. Use .csv or .parquet.")
        exit(1)

    # Convert to NumPy arrays
    X = X_df.values
    Y = Y_df.values
    logger.info(f"Converted query features to NumPy array with shape {X.shape}")
    logger.info(f"Converted hint features to NumPy array with shape {Y.shape}")

    # Consistently scale both X and Y using StandardScaler
    scaler_X = StandardScaler()
    scaler_Y = StandardScaler()

    X_scaled = scaler_X.fit_transform(X)
    Y_scaled = scaler_Y.fit_transform(Y)
    logger.info("Feature matrices X and Y have been scaled using StandardScaler.")

    # Save scaled feature matrices as .npz
    np.savez(os.path.join(output_path, "X.npz"), X=X_scaled)
    np.savez(os.path.join(output_path, "Y.npz"), Y=Y_scaled)
    logger.info(f"Saved scaled feature matrices as X.npz and Y.npz to {output_path}")

    # Create association matrix P and mask matrix M
    N_q = len(queries)
    N_h = len(hints)
    P = np.zeros((N_q, N_h))
    M = np.zeros((N_q, N_h))

    for _, row in csv_data.iterrows():
        i = query_to_idx[row['query']]
        j = hint_to_idx[row['hint']]
        val = row['latency_seconds']
        if pd.notna(val):
            P[i, j] = val
            M[i, j] = 1
        else:
            M[i, j] = 0

    P_sparse = coo_matrix(P)
    M_sparse = coo_matrix(M)
    save_npz(os.path.join(output_path, "P.npz"), P_sparse)
    save_npz(os.path.join(output_path, "M.npz"), M_sparse)
    logger.info(f"Saved association matrices P.npz and M.npz to {output_path}")

    # Convert M_sparse to dense array for verification (optional)
    M_dense = M_sparse.toarray()

    # Check unique values
    logger.info(f"Unique values in M: {np.unique(M_dense)}")  # Should be [0, 1]
    logger.info(f"Number of observed entries: {np.sum(M_dense)} / {N_q * N_h}")
    logger.info("Feature matrices and association matrices have been successfully built and saved.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build Feature and Association Matrices for IMC")
    parser.add_argument("--config", type=str, default="config/default_config.yaml", help="Path to config file")
    args = parser.parse_args()

    config = load_config(args.config)
    build_feature_matrices(config)
'''