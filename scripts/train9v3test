#!/usr/bin/env python3
"""
Full Inference Script Using 25 Features with Integrated Min-Max Scaling (0 to 4) for Inductive Matrix Completion Model
"""
    
import os
import sys
import logging
import psycopg2
import numpy as np
import pandas as pd
import re
import sqlglot
from sqlglot import exp
from sklearn.preprocessing import MinMaxScaler
import joblib

# ---------------- Environment Configuration ----------------

def configure_environment():
    """
    Configure the Python environment by adjusting the system path.
    This ensures that the project root is in the Python path for module imports.
    """
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()

# ---------------- Logging Configuration ----------------

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ---------------- Define Hints and Combination Strings ----------------

HINTS = [
    "enable_hashjoin",
    "enable_indexonlyscan",
    "enable_indexscan",
    "enable_mergejoin",
    "enable_nestloop",
    "enable_seqscan"
]

COMBO_STRS = [
    "hashjoin,indexonlyscan",
    "hashjoin,indexonlyscan,indexscan",
    "hashjoin,indexonlyscan,indexscan,mergejoin",
    "hashjoin,indexonlyscan,indexscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,indexscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,indexscan,nestloop",
    "hashjoin,indexonlyscan,indexscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,indexscan,seqscan",
    "hashjoin,indexonlyscan,mergejoin",
    "hashjoin,indexonlyscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexonlyscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,nestloop",
    "hashjoin,indexonlyscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,seqscan",
    "hashjoin,indexscan",
    "hashjoin,indexscan,mergejoin",
    "hashjoin,indexscan,mergejoin,nestloop",
    "hashjoin,indexscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexscan,mergejoin,seqscan",
    "hashjoin,indexscan,nestloop",
    "hashjoin,indexscan,nestloop,seqscan",
    "hashjoin,indexscan,seqscan",
    "hashjoin,mergejoin,nestloop,seqscan",
    "hashjoin,mergejoin,seqscan",
    "hashjoin,nestloop,seqscan",
    "hashjoin,seqscan",
    "indexonlyscan,indexscan,mergejoin",
    "indexonlyscan,indexscan,mergejoin,nestloop",
    "indexonlyscan,indexscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,indexscan,mergejoin,seqscan",
    "indexonlyscan,indexscan,nestloop",
    "indexonlyscan,indexscan,nestloop,seqscan",
    "indexonlyscan,mergejoin",
    "indexonlyscan,mergejoin,nestloop",
    "indexonlyscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,mergejoin,seqscan",
    "indexonlyscan,nestloop",
    "indexonlyscan,nestloop,seqscan",
    "indexscan,mergejoin",
    "indexscan,mergejoin,nestloop",
    "indexscan,mergejoin,nestloop,seqscan",
    "indexscan,mergejoin,seqscan",
    "indexscan,nestloop",
    "indexscan,nestloop,seqscan",
    "mergejoin,nestloop,seqscan",
    "mergejoin,seqscan",
    "nestloop,seqscan"
]

# ---------------- Original 25-Feature SQL Parsing Functions ----------------

def build_alias_mapping(stmt):
    """
    Build a mapping from table aliases to table names.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    dict
        A dictionary mapping aliases to actual table names.
    """
    alias_mapping = {}
    for table in stmt.find_all(exp.Table):
        alias = table.alias_or_name
        name = table.name
        alias_mapping[alias] = name
        logger.debug(f"Mapping alias '{alias}' to table '{name}'")
    return alias_mapping

def count_predicates(where_clause):
    """
    Count the number of predicate expressions in the WHERE clause.
    Predicate types include EQ, GT, LT, GTE, LTE, NEQ, Like, ILike, In, Between.

    Parameters:
    ----------
    where_clause : sqlglot.Expression or None
        The WHERE clause of the SQL statement.

    Returns:
    -------
    int
        The number of predicate expressions.
    """
    if where_clause is None:
        return 0

    # Define the predicate expression types to count
    predicate_types = (
        exp.EQ,
        exp.GT,
        exp.LT,
        exp.GTE,
        exp.LTE,
        exp.NEQ,
        exp.Like,
        exp.ILike,
        exp.In,
        exp.Between,
    )

    # Use find_all to locate all instances of these predicate types
    predicates = list(where_clause.find_all(*predicate_types))

    return len(predicates)

def count_joins(where_clause, alias_mapping):
    """
    Count the number of unique join conditions in the WHERE clause.
    A join condition is defined as an equality predicate between two different tables.

    Parameters:
    ----------
    where_clause : sqlglot.Expression or None
        The WHERE clause of the SQL statement.
    alias_mapping : dict
        Mapping from table aliases to table names.

    Returns:
    -------
    int
        The number of unique join conditions.
    """
    if where_clause is None:
        return 0

    join_pairs = set()

    # Define only EQ predicates for joins
    join_predicate_types = (exp.EQ,)

    # Iterate over all EQ predicates in the WHERE clause
    for predicate in where_clause.find_all(*join_predicate_types):
        left = predicate.left
        right = predicate.right

        # Check if both sides are columns
        if isinstance(left, exp.Column) and isinstance(right, exp.Column):
            # Extract table aliases or names
            left_table_alias = left.table
            right_table_alias = right.table

            # Get actual table names from alias mapping
            left_table_name = alias_mapping.get(left_table_alias)
            right_table_name = alias_mapping.get(right_table_alias)

            # Log the predicate being processed
            logger.debug(f"Processing predicate: {left.sql()} = {right.sql()} (Tables: {left_table_name}, {right_table_name})")

            # Ensure both tables are present and different
            if left_table_name and right_table_name and left_table_name != right_table_name:
                # Create a sorted tuple to avoid counting (A, B) and (B, A) separately
                pair = tuple(sorted([left_table_name, right_table_name]))
                if pair not in join_pairs:
                    join_pairs.add(pair)
                    logger.debug(f"Identified unique join pair: {pair}")
                else:
                    logger.debug(f"Duplicate join pair detected and ignored: {pair}")
            else:
                logger.debug(f"Ignored predicate as it does not connect two different tables: {left.sql()} = {right.sql()}")

    logger.debug(f"Total unique joins counted: {len(join_pairs)}")
    return len(join_pairs)

def count_aggregate_functions(select_expressions):
    """
    Count the number of aggregate functions in the SELECT clause.

    Parameters:
    ----------
    select_expressions : list of sqlglot.Expression
        The expressions in the SELECT clause.

    Returns:
    -------
    int
        The number of aggregate functions.
    """
    aggregate_functions = {'SUM', 'COUNT', 'AVG', 'MIN', 'MAX', 'MEDIAN', 'MODE'}
    count = 0
    for expr in select_expressions:
        if isinstance(expr, exp.Alias):
            expr = expr.this
        if isinstance(expr, exp.Func) and expr.name.upper() in aggregate_functions:
            count += 1
    return count

def count_logical_operators(where_clause):
    """
    Count the number of logical operators (AND, OR, NOT) in the WHERE clause.

    Parameters:
    ----------
    where_clause : sqlglot.Expression or None
        The WHERE clause of the SQL statement.

    Returns:
    -------
    int
        The number of logical operators.
    """
    if where_clause is None:
        return 0

    logical_ops = (exp.And, exp.Or, exp.Not)
    count = sum(1 for _ in where_clause.find_all(*logical_ops))
    return count

def count_comparison_operators(where_clause):
    """
    Count the number of comparison operators (=, >, <, >=, <=, !=) in the WHERE clause.

    Parameters:
    ----------
    where_clause : sqlglot.Expression or None
        The WHERE clause of the SQL statement.

    Returns:
    -------
    int
        The number of comparison operators.
    """
    if where_clause is None:
        return 0

    comparison_ops = (exp.EQ, exp.GT, exp.LT, exp.GTE, exp.LTE, exp.NEQ)
    count = sum(1 for _ in where_clause.find_all(*comparison_ops))
    return count

def count_group_by_columns(stmt):
    """
    Count the number of columns in the GROUP BY clause.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    int
        The number of GROUP BY columns.
    """
    group = stmt.find(exp.Group)
    if group:
        return len(list(group.find_all(exp.Column)))
    return 0

def count_order_by_columns(stmt):
    """
    Count the number of columns in the ORDER BY clause.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    int
        The number of ORDER BY columns.
    """
    order = stmt.find(exp.Order)
    if order:
        return len(list(order.find_all(exp.Ordered)))
    return 0

def count_nested_subqueries(stmt):
    """
    Count the number of nested subqueries within the main query.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    int
        The number of nested subqueries.
    """
    # Exclude the main select statement
    subqueries = list(stmt.find_all(exp.Select))
    return max(0, len(subqueries) - 1)

def count_correlated_subqueries(stmt):
    """
    Check if there are any correlated subqueries.
    A correlated subquery references columns from the outer query.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    float
        1.0 if there are correlated subqueries, else 0.0
    """
    correlated = False
    for subquery in stmt.find_all(exp.Select):
        if subquery is stmt:
            continue
        outer_tables = {table.alias_or_name for table in stmt.find_all(exp.Table)}
        for column in subquery.find_all(exp.Column):
            if column.table and column.table in outer_tables:
                correlated = True
                break
        if correlated:
            break
    return 1.0 if correlated else 0.0

def count_case_statements(stmt):
    """
    Count the number of CASE statements in the query.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    int
        The number of CASE statements.
    """
    return len(list(stmt.find_all(exp.Case)))

def count_union_operations(stmt):
    """
    Count the number of UNION or UNION ALL operations in the query.

    Parameters:
    ----------
    stmt : sqlglot.Expression
        The parsed SQL statement.

    Returns:
    -------
    int
        The number of UNION operations.
    """
    return len(list(stmt.find_all(exp.Union)))

def parse_sql(query_str):
    """
    Parse SQL using sqlglot with Postgres dialect and extract 25 features.

    Parameters:
    ----------
    query_str : str
        The SQL query string.

    Returns:
    -------
    dict
        A dictionary containing the 25 extracted features.
    """
    try:
        statements = sqlglot.parse(query_str, dialect='postgres')
        if not statements:
            raise ValueError("No statements parsed.")

        stmt = statements[0]

        # Build alias mapping
        alias_mapping = build_alias_mapping(stmt)

        # Extract table names
        tables = [node.alias_or_name for node in stmt.find_all(exp.Table)]
        num_tables = len(tables)
        logger.debug(f"Number of tables: {num_tables}")

        # Extract WHERE predicates
        where_clause = stmt.find(exp.Where)
        num_predicates = count_predicates(where_clause)
        logger.debug(f"Number of predicates: {num_predicates}")

        # Extract JOIN predicates (implicit joins)
        num_joins = count_joins(where_clause, alias_mapping)
        logger.debug(f"Number of unique joins: {num_joins}")

        # Check for GROUP BY clause
        has_group_by = 1.0 if stmt.find(exp.Group) else 0.0
        logger.debug(f"Has GROUP BY: {has_group_by}")

        # Check for ORDER BY clause
        has_order_by = 1.0 if stmt.find(exp.Order) else 0.0
        logger.debug(f"Has ORDER BY: {has_order_by}")

        # Check for subqueries
        num_nested_subqueries = count_nested_subqueries(stmt)
        has_subquery = 1.0 if num_nested_subqueries > 0 else 0.0
        logger.debug(f"Has subquery: {has_subquery}")

        # Query Length Features
        query_length = len(query_str)
        num_tokens = len(list(stmt.walk()))
        logger.debug(f"Query length (chars): {query_length}")
        logger.debug(f"Number of tokens: {num_tokens}")

        # SELECT Clause Features
        select = stmt.find(exp.Select)
        select_expressions = select.expressions if select else []
        num_select_expressions = len(select_expressions)
        has_distinct = 1.0 if select and select.args.get("distinct") else 0.0
        num_aggregate_functions = count_aggregate_functions(select_expressions)
        logger.debug(f"Number of SELECT expressions: {num_select_expressions}")
        logger.debug(f"Has DISTINCT: {has_distinct}")
        logger.debug(f"Number of aggregate functions: {num_aggregate_functions}")

        # WHERE Clause Detailed Features
        num_logical_operators = count_logical_operators(where_clause)
        num_comparison_operators = count_comparison_operators(where_clause)
        logger.debug(f"Number of logical operators: {num_logical_operators}")
        logger.debug(f"Number of comparison operators: {num_comparison_operators}")

        # GROUP BY and ORDER BY Columns
        num_group_by_columns = count_group_by_columns(stmt)
        num_order_by_columns = count_order_by_columns(stmt)
        logger.debug(f"Number of GROUP BY columns: {num_group_by_columns}")
        logger.debug(f"Number of ORDER BY columns: {num_order_by_columns}")

        # Subquery Features
        has_correlated_subqueries = count_correlated_subqueries(stmt)
        logger.debug(f"Has correlated subqueries: {has_correlated_subqueries}")

        # JOIN Type Features (Explicit Joins)
        list_of_joins = list(stmt.find_all(exp.Join))
        num_inner_joins = len([j for j in list_of_joins if j.args.get('kind') and j.args.get('kind').upper() == 'INNER'])
        num_left_joins = len([j for j in list_of_joins if j.args.get('kind') and j.args.get('kind').upper() == 'LEFT'])
        num_right_joins = len([j for j in list_of_joins if j.args.get('kind') and j.args.get('kind').upper() == 'RIGHT'])
        num_full_outer_joins = len([j for j in list_of_joins if j.args.get('kind') and j.args.get('kind').upper() in ['FULL OUTER', 'FULLOUTER', 'FULL_OUTER']])
        logger.debug(f"Number of INNER JOINs: {num_inner_joins}")
        logger.debug(f"Number of LEFT JOINs: {num_left_joins}")
        logger.debug(f"Number of RIGHT JOINs: {num_right_joins}")
        logger.debug(f"Number of FULL OUTER JOINs: {num_full_outer_joins}")

        # Miscellaneous Features
        has_limit = 1.0 if stmt.find(exp.Limit) else 0.0
        has_union = 1.0 if stmt.find(exp.Union) else 0.0
        num_union_operations = count_union_operations(stmt)
        num_case_statements = count_case_statements(stmt)
        logger.debug(f"Has LIMIT: {has_limit}")
        logger.debug(f"Has UNION: {has_union}")
        logger.debug(f"Number of UNION operations: {num_union_operations}")
        logger.debug(f"Number of CASE statements: {num_case_statements}")

        features = {
            "num_tables": num_tables,
            "num_joins": num_joins,
            "num_predicates": num_predicates,
            "has_group_by": has_group_by,
            "has_order_by": has_order_by,
            "has_subquery": has_subquery,
            "query_length": query_length,
            "num_tokens": num_tokens,
            "num_select_expressions": num_select_expressions,
            "has_distinct": has_distinct,
            "num_aggregate_functions": num_aggregate_functions,
            "num_logical_operators": num_logical_operators,
            "num_comparison_operators": num_comparison_operators,
            "num_group_by_columns": num_group_by_columns,
            "num_order_by_columns": num_order_by_columns,
            "num_nested_subqueries": num_nested_subqueries,
            "has_correlated_subqueries": has_correlated_subqueries,
            "num_inner_joins": num_inner_joins,
            "num_left_joins": num_left_joins,
            "num_right_joins": num_right_joins,
            "num_full_outer_joins": num_full_outer_joins,
            "has_limit": has_limit,
            "has_union": has_union,
            "num_union_operations": num_union_operations,
            "num_case_statements": num_case_statements
        }

        logger.debug(f"Parsed SQL features: {features}")
        return features

    except Exception as e:
        logger.error(f"Error parsing SQL: {e}")
        # Return a default feature set with zeros
        return { 
            "num_tables": 0,
            "num_joins": 0,
            "num_predicates": 0,
            "has_group_by": 0.0,
            "has_order_by": 0.0,
            "has_subquery": 0.0,
            "query_length": 0,
            "num_tokens": 0,
            "num_select_expressions": 0,
            "has_distinct": 0.0,
            "num_aggregate_functions": 0,
            "num_logical_operators": 0,
            "num_comparison_operators": 0,
            "num_group_by_columns": 0,
            "num_order_by_columns": 0,
            "num_nested_subqueries": 0,
            "has_correlated_subqueries": 0.0,
            "num_inner_joins": 0,
            "num_left_joins": 0,
            "num_right_joins": 0,
            "num_full_outer_joins": 0,
            "has_limit": 0.0,
            "has_union": 0.0,
            "num_union_operations": 0,
            "num_case_statements": 0
        }

# ---------------- Feature Preparation ----------------

def prepare_features(feats_dict):
    """
    Prepare features for prediction by extracting raw feature values.

    Parameters:
    ----------
    feats_dict : dict
        Dictionary containing raw feature values.

    Returns:
    -------
    numpy.ndarray
        Array of raw feature values ordered correctly (shape: 1, 25).
    """
    feature_order = [
        "num_tables",
        "num_joins",
        "num_predicates",
        "has_group_by",
        "has_order_by",
        "has_subquery",
        "query_length",
        "num_tokens",
        "num_select_expressions",
        "has_distinct",
        "num_aggregate_functions",
        "num_logical_operators",
        "num_comparison_operators",
        "num_group_by_columns",
        "num_order_by_columns",
        "num_nested_subqueries",
        "has_correlated_subqueries",
        "num_inner_joins",
        "num_left_joins",
        "num_right_joins",
        "num_full_outer_joins",
        "has_limit",
        "has_union",
        "num_union_operations",
        "num_case_statements"
    ]
    raw_features = [feats_dict.get(key, 0.0) for key in feature_order]
    return np.array(raw_features, dtype=np.float32).reshape(1, -1)

# ---------------- Hint Matrix Construction ----------------

def build_hint_matrix_from_combos(combo_strs, hint_list):
    """
    Build a hint matrix from combination strings and a list of hints.

    Parameters:
    ----------
    combo_strs : list of str
        List of comma-separated hint combinations.
    hint_list : list of str
        List of all possible hints.

    Returns:
    -------
    numpy.ndarray
        Hint matrix of shape (N, D), where N is the number of combinations and D is the number of hints.
    """
    N = len(combo_strs)
    D = len(hint_list)
    Y = np.zeros((N, D), dtype=float)
    for i, c_str in enumerate(combo_strs):
        items = [x.strip() for x in c_str.split(",") if x.strip()]
        for item in items:
            enable_str = "enable_" + item
            if enable_str in hint_list:
                j = hint_list.index(enable_str)
                Y[i, j] = 1.0
    return Y

# ---------------- Inductive Matrix Completion Model Class ----------------

class InductiveMatrixCompletionModel:
    """
    Inductive Matrix Completion Model with Residuals and All Hint Combinations Cost Predictions.
    """
    def __init__(self, W, H, residual_model, poly):
        """
        Initialize the IMC model with model matrices and residual components.

        Parameters:
        ----------
        W : numpy.ndarray
            Matrix W from IMC model.
        H : numpy.ndarray
            Matrix H from IMC model.
        residual_model : sklearn estimator
            Trained residual model (e.g., XGBoost).
        poly : sklearn transformer
            Polynomial feature transformer.
        """
        self.Z = W @ H.T
        self.residual_model = residual_model
        self.poly = poly

    def predict_all_combos(self, x_feat, Y):
        """
        Predict costs for all hint combinations across multiple queries.

        Parameters:
        ----------
        x_feat : numpy.ndarray
            Raw feature vectors for the queries (shape: N, n_features).
        Y : numpy.ndarray
            Hint combination matrix (shape: M, D).

        Returns:
        -------
        numpy.ndarray
            Predicted costs for each combination and query (shape: N*M,).
        """
        # Predict initial costs using IMC
        xZ = x_feat @ self.Z  # Shape: (N, D)

        initial_costs = xZ @ Y.T  # Shape: (N, M)

        # ------------------------------
        # Commented out XGBoost residual predictions
        # ------------------------------
        # Prepare features for residual prediction
        # num_combos = Y.shape[0]
        # N = x_feat.shape[0]
        # x_feat_repeated = np.repeat(x_feat, num_combos, axis=0)  # Shape: (N*M, n_features)
        # Y_expanded = np.tile(Y, (N, 1))  # Shape: (N*M, D)

        # # Concatenate features
        # features = np.hstack((x_feat_repeated, Y_expanded))  # Shape: (N*M, n_features + D)

        # # Apply polynomial features
        # features_poly = self.poly.transform(features)  # Shape: (N*M, num_poly_features)

        # # Predict residuals
        # residuals_pred = self.residual_model.predict(features_poly)  # Shape: (N*M,)

        # # Adjust initial costs with predicted residuals
        # adjusted_costs = initial_costs.flatten() + residuals_pred  # Shape: (N*M,)
        # return adjusted_costs

        # Since residuals are removed, return initial_costs as flattened array
        return initial_costs.flatten()

# ---------------- PostgreSQL Hint Application Functions ----------------

def apply_plan_hints(cursor, plan_vector):
    """
    Apply hints to the PostgreSQL session based on the plan vector.

    Parameters:
    ----------
    cursor : psycopg2.cursor
        The database cursor.
    plan_vector : numpy.ndarray
        Binary vector indicating which hints to enable (shape: D,).
    """
    for i, hint_name in enumerate(HINTS):
        val = plan_vector[i]
        cmd = f"SET {hint_name} TO {'ON' if val >= 0.5 else 'OFF'};"
        cursor.execute(cmd)

def reset_all_hints(cursor):
    """
    Reset all hints to their default settings.

    Parameters:
    ----------
    cursor : psycopg2.cursor
        The database cursor.
    """
    cursor.execute("RESET ALL;")

# ---------------- EXPLAIN ANALYZE Parsing Function ----------------

def parse_explain_analyze_output(explain_output):
    """
    Parse EXPLAIN ANALYZE output and extract execution time.

    Parameters:
    ----------
    explain_output : list of tuple
        The output from EXPLAIN ANALYZE.

    Returns:
    -------
    float
        The total execution time in seconds. Returns infinity if parsing fails.
    """
    total_time = None
    for row in explain_output:
        line = row[0]
        match_ms = re.search(r"Execution Time:\s+([\d.]+)\s+ms", line)
        if match_ms:
            total_time_ms = float(match_ms.group(1))
            total_time = total_time_ms / 1000.0  # Convert ms to seconds
            break
    if total_time is None:
        for row in explain_output:
            line = row[0]
            match_s = re.search(r"Execution Time:\s+([\d.]+)\s+s", line)
            if match_s:
                total_time_s = float(match_s.group(1))
                total_time = total_time_s
                break
    if total_time is None:
        logger.warning("Could not parse Execution Time from EXPLAIN ANALYZE output.")
        return float("inf")
    return total_time

# ---------------- Query Execution Function ----------------

def run_query_postgres_once_explain_analyze(query_str, plan_vector, pg_host, pg_db, pg_user, pg_password, port=5432):
    """
    Execute a query with given hints and retrieve execution time using EXPLAIN ANALYZE.

    Parameters:
    ----------
    query_str : str
        The SQL query string.
    plan_vector : numpy.ndarray
        Binary vector indicating which hints to enable (shape: D,).
    pg_host : str
        PostgreSQL host address.
    pg_db : str
        PostgreSQL database name.
    pg_user : str
        PostgreSQL username.
    pg_password : str
        PostgreSQL password.
    port : int, optional
        PostgreSQL port number, by default 5432.

    Returns:
    -------
    float
        The execution time in seconds.
    """
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host, dbname=pg_db, user=pg_user,
            password=pg_password, port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            apply_plan_hints(cur, plan_vector)
            cur.execute(f"EXPLAIN ANALYZE {query_str}")
            explain_output = cur.fetchall()
            reset_all_hints(cur)
            return parse_explain_analyze_output(explain_output)
    except Exception as e:
        logger.error(f"Error executing EXPLAIN ANALYZE: {e}")
        return float("inf")
    finally:
        if conn:
            conn.close()

# ---------------- Main Inference Function ----------------

def main():
    """
    Main function to perform inference on SQL queries, predict optimal hints,
    execute queries with selected hints, and save the results.
    """
    # Configuration Parameters
    MODEL_DIR = "/Users/raahimlone/New_Data"
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results_imc_25feat.csv"
    PG_HOST = "localhost"
    PG_DB = "IMDB"
    PG_USER = "postgres"
    PG_PASSWORD = "raahimhere"
    PG_PORT = 6543

    # ---------------- Load IMC Model Components ----------------
    try:
        W = np.load(os.path.join(MODEL_DIR, "U_best.npy"))  # Ensure this matches the first script's output
        H = np.load(os.path.join(MODEL_DIR, "V_best.npy"))
        logger.info("✅ Inductive Matrix Completion Model Loaded.")
    except Exception as e:
        logger.error(f"Model load error: {e}")
        return

    # ---------------- Load Residual Model Components ----------------
    try:
        # Commented out loading of the residual model and polynomial transformer
        # residual_model = joblib.load(os.path.join(MODEL_DIR, "residual_model_xgb.pkl"))
        # poly = joblib.load(os.path.join(MODEL_DIR, "residual_model_poly.pkl"))
        logger.info("✅ Residual Model and Polynomial Transformer Loading Commented Out.")
    except FileNotFoundError:
        logger.error("Residual model or polynomial transformer not found. Ensure they are saved correctly.")
        return
    except Exception as e:
        logger.error(f"Error loading residual model components: {e}")
        return

    # ---------------- Initialize IMC Model with Residual Components ----------------
    try:
        # Commented out residual_model and poly as they are not loaded
        # imc_model = InductiveMatrixCompletionModel(W, H, residual_model, poly)
        # Instead, initialize without residuals
        imc_model = InductiveMatrixCompletionModel(W, H, residual_model=None, poly=None)
        logger.info("✅ Inductive Matrix Completion Model Initialized without Residuals.")
    except Exception as e:
        logger.error(f"Error initializing IMC model without residuals: {e}")
        return

    # ---------------- Build Hint Matrix ----------------
    Y = build_hint_matrix_from_combos(COMBO_STRS, HINTS)
    logger.info(f"Hint matrix built with shape: {Y.shape}")
    
    # ---------------- Feature Collection Phase ----------------
    logger.info(f"Starting feature extraction from SQL queries in {QUERIES_DIR}")
    all_features = []
    filenames = []

    for fname in os.listdir(QUERIES_DIR):
        if not fname.endswith(".sql"):
            continue

        full_path = os.path.join(QUERIES_DIR, fname)
        try:
            with open(full_path, 'r', encoding="utf-8") as f:
                query_str = f.read().strip()

            if not query_str:
                logger.warning(f"{fname} - Empty SQL file.")
                continue

            # Parse and extract features
            feats = parse_sql(query_str)
            if not feats:
                logger.warning(f"{fname} - Failed to extract features.")
                continue

            all_features.append(feats)
            filenames.append(fname)

            logger.debug(f"Extracted features for {fname}: {feats}")

        except Exception as e:
            logger.error(f"Error reading or parsing {fname}: {e}")

    if not all_features:
        logger.warning("No query features extracted. Exiting.")
        print("\nNo results to display.")
        return

    # ---------------- Scaling Phase ----------------
    logger.info("Converting extracted features to DataFrame for scaling.")
    df_features = pd.DataFrame(all_features)

    # Handle missing columns by filling with zeros (optional)
    df_features.fillna(0, inplace=True)

    logger.info(f"Extracted features for {df_features.shape[0]} queries with {df_features.shape[1]} features.")

    # ---------------- Load the Pre-fitted IMC Feature Scaler ----------------
    try:
        scaler_imc = joblib.load(os.path.join(MODEL_DIR, "imc_feature_scaler.pkl"))
        logger.info("✅ Loaded IMC Feature Scaler (MinMaxScaler).")
    except FileNotFoundError:
        logger.error("IMC Feature Scaler not found. Ensure 'imc_feature_scaler.pkl' exists in MODEL_DIR.")
        return
    except Exception as e:
        logger.error(f"Error loading IMC Feature Scaler: {e}")
        return

    # ---------------- Apply the Loaded Scaler to the Features ----------------
    try:
        scaled_features = scaler_imc.transform(df_features)
        logger.info("✅ Applied Min-Max scaling to the features using the loaded scaler.")
    except Exception as e:
        logger.error(f"Error during feature scaling: {e}")
        return

    # Convert scaled features back to DataFrame for consistency
    df_scaled_features = pd.DataFrame(scaled_features, columns=df_features.columns)

    # ---------------- Prediction Phase ----------------
    logger.info("Starting cost prediction for all hint combinations.")

    # Convert scaled features to NumPy array
    X_scaled = df_scaled_features.to_numpy()  # Shape: (N, 25)

    # Make predictions
    adjusted_costs = imc_model.predict_all_combos(X_scaled, Y)  # Shape: (N*M,)

    logger.info("✅ Completed cost predictions.")

    # ---------------- Hint Selection Phase ----------------
    logger.info("Selecting best hint combinations based on predicted costs.")
    N_queries = X_scaled.shape[0]
    M_combos = Y.shape[0]
    adjusted_costs_reshaped = adjusted_costs.reshape(N_queries, M_combos)  # Shape: (N, M)
    best_indices = np.argmin(adjusted_costs_reshaped, axis=1)  # Shape: (N,)
    best_combos = [COMBO_STRS[idx] for idx in best_indices]
    best_Y_vectors = Y[best_indices]  # Shape: (N, D)

    # ---------------- Query Execution Phase ----------------
    logger.info("Starting query execution with selected hints.")

    results = []

    for i, fname in enumerate(filenames):
        query_str = ""
        full_path = os.path.join(QUERIES_DIR, fname)
        try:
            with open(full_path, 'r', encoding="utf-8") as f:
                query_str = f.read().strip()

            if not query_str:
                logger.warning(f"{fname} - Empty SQL file.")
                continue

            # Retrieve the best hint combination
            best_idx = best_indices[i]
            best_combo = best_combos[i]
            best_Y = best_Y_vectors[i]

            # Execute the query with the selected hints and measure latency
            latency = run_query_postgres_once_explain_analyze(
                query_str, best_Y, PG_HOST, PG_DB, PG_USER, PG_PASSWORD, PG_PORT
            )

            # Extract enabled hints
            enabled_hints = [HINTS[j].replace("enable_", "") for j in range(len(HINTS)) if best_Y[j] >= 0.5]

            # Retrieve the predicted cost
            predicted_cost = adjusted_costs_reshaped[i, best_idx]

            results.append({
                "filename": fname,
                "best_idx": best_idx,
                "best_combo": best_combo,
                "predicted_cost": predicted_cost,
                "latency": latency,
                "hints": ",".join(enabled_hints)
            })

            logger.info(f"Processed {fname}: Best Hints - {enabled_hints}, Predicted Cost - {predicted_cost:.4f}, Latency - {latency:.4f}s")

        except Exception as e:
            logger.error(f"Error executing query {fname}: {e}")

    # ---------------- Save Results to CSV ----------------
    try:
        if not results:
            logger.warning("No results to save. The results list is empty.")
            print("\nNo results to display.")
            return

        df_results = pd.DataFrame(results)

        # Reorder columns for better readability
        columns_order = ["filename", "best_idx", "best_combo", "predicted_cost", "latency", "hints"]
        df_results = df_results[columns_order]

        df_results.to_csv(OUTPUT_CSV, index=False)
        logger.info(f"✅ Results saved to {OUTPUT_CSV}")
        print("\nResults Summary:")
        print(df_results.describe())

    except Exception as e:
        logger.error(f"Error saving results to CSV: {e}")

# ---------------- Entry Point ----------------

if __name__ == "__main__":
    main()
