#!/usr/bin/env python3
"""
Enhanced Inference Script with 25 Structural SQL Features + 1 Semantic Feature
Using Integrated Min-Max Scaling (0 to 4) for Inductive Matrix Completion Model
"""

import os
import sys
import logging
import psycopg2
import numpy as np
import pandas as pd
import re
import sqlglot
from sqlglot import exp
from sklearn.preprocessing import MinMaxScaler
import joblib

# For semantic embedding:
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None
    print("WARNING: `sentence_transformers` library not found. Semantic embeddings will be skipped.")

# ---------------- Environment Configuration ----------------

def configure_environment():
    """
    Configure the Python environment by adjusting the system path.
    This ensures that the project root is in the Python path for module imports.
    """
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()

# ---------------- Logging Configuration ----------------

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ---------------- Define Hints and Combination Strings ----------------

HINTS = [
    "enable_hashjoin",
    "enable_indexonlyscan",
    "enable_indexscan",
    "enable_mergejoin",
    "enable_nestloop",
    "enable_seqscan"
]

COMBO_STRS = [
    "hashjoin,indexonlyscan",
    "hashjoin,indexonlyscan,indexscan",
    "hashjoin,indexonlyscan,indexscan,mergejoin",
    "hashjoin,indexonlyscan,indexscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,indexscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,indexscan,nestloop",
    "hashjoin,indexonlyscan,indexscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,indexscan,seqscan",
    "hashjoin,indexonlyscan,mergejoin",
    "hashjoin,indexonlyscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexonlyscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,nestloop",
    "hashjoin,indexonlyscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,seqscan",
    "hashjoin,indexscan",
    "hashjoin,indexscan,mergejoin",
    "hashjoin,indexscan,mergejoin,nestloop",
    "hashjoin,indexscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexscan,mergejoin,seqscan",
    "hashjoin,indexscan,nestloop",
    "hashjoin,indexscan,nestloop,seqscan",
    "hashjoin,indexscan,seqscan",
    "hashjoin,mergejoin,nestloop,seqscan",
    "hashjoin,mergejoin,seqscan",
    "hashjoin,nestloop,seqscan",
    "hashjoin,seqscan",
    "indexonlyscan,indexscan,mergejoin",
    "indexonlyscan,indexscan,mergejoin,nestloop",
    "indexonlyscan,indexscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,indexscan,mergejoin,seqscan",
    "indexonlyscan,indexscan,nestloop",
    "indexonlyscan,indexscan,nestloop,seqscan",
    "indexonlyscan,mergejoin",
    "indexonlyscan,mergejoin,nestloop",
    "indexonlyscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,mergejoin,seqscan",
    "indexonlyscan,nestloop",
    "indexonlyscan,nestloop,seqscan",
    "indexscan,mergejoin",
    "indexscan,mergejoin,nestloop",
    "indexscan,mergejoin,nestloop,seqscan",
    "indexscan,mergejoin,seqscan",
    "indexscan,nestloop",
    "indexscan,nestloop,seqscan",
    "mergejoin,nestloop,seqscan",
    "mergejoin,seqscan",
    "nestloop,seqscan"
]

# ---------------- SEMANTIC EMBEDDING FUNCTION ----------------

# To avoid dimension mismatch, we demonstrate a simple approach:
# we take the mean of the embedding vector and use it as a single float feature.
# In a real scenario, you'd typically keep the entire embedding or a PCA-reduced version
# and ensure your IMC model has been trained with that dimension.
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
_ST_MODEL = None

def _load_semantic_model():
    global _ST_MODEL
    if SentenceTransformer is not None and _ST_MODEL is None:
        try:
            _ST_MODEL = SentenceTransformer(EMBEDDING_MODEL_NAME)
            logger.info(f"✅ Loaded semantic embedding model: {EMBEDDING_MODEL_NAME}")
        except Exception as e:
            logger.error(f"Failed to load SentenceTransformer model: {e}")
            _ST_MODEL = None

def get_semantic_embedding(query_str):
    """
    Produce a single float from the query string by taking the mean
    of a sentence-transformer embedding. If not available, returns 0.0.
    """
    if _ST_MODEL is None:
        return 0.0
    embedding = _ST_MODEL.encode(query_str)  # shape: (embedding_dim,)
    return float(np.mean(embedding))

# ---------------- Original 25-Feature SQL Parsing Functions ----------------

def build_alias_mapping(stmt):
    """
    Build a mapping from table aliases to table names.
    """
    alias_mapping = {}
    for table in stmt.find_all(exp.Table):
        alias = table.alias_or_name
        name = table.name
        alias_mapping[alias] = name
        logger.debug(f"Mapping alias '{alias}' to table '{name}'")
    return alias_mapping

def count_predicates(where_clause):
    """
    Count the number of predicate expressions in the WHERE clause.
    """
    if where_clause is None:
        return 0

    # Define the predicate expression types to count
    predicate_types = (
        exp.EQ,
        exp.GT,
        exp.LT,
        exp.GTE,
        exp.LTE,
        exp.NEQ,
        exp.Like,
        exp.ILike,
        exp.In,
        exp.Between,
    )

    predicates = list(where_clause.find_all(*predicate_types))
    return len(predicates)

def count_joins(where_clause, alias_mapping):
    """
    Count the number of unique join conditions in the WHERE clause.
    A join condition is an equality predicate between two different tables.
    """
    if where_clause is None:
        return 0

    join_pairs = set()
    join_predicate_types = (exp.EQ,)

    for predicate in where_clause.find_all(*join_predicate_types):
        left = predicate.left
        right = predicate.right
        if isinstance(left, exp.Column) and isinstance(right, exp.Column):
            left_table_alias = left.table
            right_table_alias = right.table
            left_table_name = alias_mapping.get(left_table_alias)
            right_table_name = alias_mapping.get(right_table_alias)

            logger.debug(f"Processing predicate: {left.sql()} = {right.sql()} "
                         f"(Tables: {left_table_name}, {right_table_name})")

            if left_table_name and right_table_name and left_table_name != right_table_name:
                pair = tuple(sorted([left_table_name, right_table_name]))
                if pair not in join_pairs:
                    join_pairs.add(pair)
                    logger.debug(f"Identified unique join pair: {pair}")
                else:
                    logger.debug(f"Duplicate join pair detected and ignored: {pair}")

    logger.debug(f"Total unique joins counted: {len(join_pairs)}")
    return len(join_pairs)

def count_aggregate_functions(select_expressions):
    """
    Count the number of aggregate functions in the SELECT clause.
    """
    aggregate_functions = {'SUM', 'COUNT', 'AVG', 'MIN', 'MAX', 'MEDIAN', 'MODE'}
    count = 0
    for expr in select_expressions:
        if isinstance(expr, exp.Alias):
            expr = expr.this
        if isinstance(expr, exp.Func) and expr.name.upper() in aggregate_functions:
            count += 1
    return count

def count_logical_operators(where_clause):
    """
    Count the number of logical operators (AND, OR, NOT) in the WHERE clause.
    """
    if where_clause is None:
        return 0
    logical_ops = (exp.And, exp.Or, exp.Not)
    return sum(1 for _ in where_clause.find_all(*logical_ops))

def count_comparison_operators(where_clause):
    """
    Count the number of comparison operators (=, >, <, >=, <=, !=) in the WHERE clause.
    """
    if where_clause is None:
        return 0
    comparison_ops = (exp.EQ, exp.GT, exp.LT, exp.GTE, exp.LTE, exp.NEQ)
    return sum(1 for _ in where_clause.find_all(*comparison_ops))

def count_group_by_columns(stmt):
    """
    Count the number of columns in the GROUP BY clause.
    """
    group = stmt.find(exp.Group)
    if group:
        return len(list(group.find_all(exp.Column)))
    return 0

def count_order_by_columns(stmt):
    """
    Count the number of columns in the ORDER BY clause.
    """
    order = stmt.find(exp.Order)
    if order:
        return len(list(order.find_all(exp.Ordered)))
    return 0

def count_nested_subqueries(stmt):
    """
    Count the number of nested subqueries within the main query.
    """
    subqueries = list(stmt.find_all(exp.Select))
    return max(0, len(subqueries) - 1)

def count_correlated_subqueries(stmt):
    """
    Check if there are any correlated subqueries (references columns from the outer query).
    Returns 1.0 if correlated subqueries exist, else 0.0.
    """
    correlated = False
    outer_tables = {table.alias_or_name for table in stmt.find_all(exp.Table)}

    for subquery in stmt.find_all(exp.Select):
        if subquery is stmt:
            continue
        for column in subquery.find_all(exp.Column):
            if column.table and column.table in outer_tables:
                correlated = True
                break
        if correlated:
            break

    return 1.0 if correlated else 0.0

def count_case_statements(stmt):
    """
    Count the number of CASE statements in the query.
    """
    return len(list(stmt.find_all(exp.Case)))

def count_union_operations(stmt):
    """
    Count the number of UNION or UNION ALL operations in the query.
    """
    return len(list(stmt.find_all(exp.Union)))

def parse_sql(query_str):
    """
    Parse SQL using sqlglot (PostgreSQL dialect) and extract 25 structural features + 1 semantic feature.
    """
    try:
        statements = sqlglot.parse(query_str, dialect='postgres')
        if not statements:
            raise ValueError("No statements parsed.")

        stmt = statements[0]
        alias_mapping = build_alias_mapping(stmt)

        # Basic table info
        tables = [node.alias_or_name for node in stmt.find_all(exp.Table)]
        num_tables = len(tables)

        # WHERE analysis
        where_clause = stmt.find(exp.Where)
        num_predicates = count_predicates(where_clause)
        num_joins = count_joins(where_clause, alias_mapping)

        # GROUP BY / ORDER BY
        has_group_by = 1.0 if stmt.find(exp.Group) else 0.0
        has_order_by = 1.0 if stmt.find(exp.Order) else 0.0

        # Subqueries
        num_nested_subqueries = count_nested_subqueries(stmt)
        has_subquery = 1.0 if num_nested_subqueries > 0 else 0.0

        # Query length
        query_length = len(query_str)
        num_tokens = len(list(stmt.walk()))

        # SELECT clause details
        select = stmt.find(exp.Select)
        select_expressions = select.expressions if select else []
        num_select_expressions = len(select_expressions)
        has_distinct = 1.0 if select and select.args.get("distinct") else 0.0
        num_aggregate_functions = count_aggregate_functions(select_expressions)

        # Logical + comparison operators
        num_logical_operators = count_logical_operators(where_clause)
        num_comparison_operators = count_comparison_operators(where_clause)

        # GROUP BY & ORDER BY columns
        num_group_by_columns = count_group_by_columns(stmt)
        num_order_by_columns = count_order_by_columns(stmt)

        # Correlated subqueries
        has_correlated_subqueries = count_correlated_subqueries(stmt)

        # JOIN types
        list_of_joins = list(stmt.find_all(exp.Join))
        num_inner_joins = len([j for j in list_of_joins if j.args.get('kind', '').upper() == 'INNER'])
        num_left_joins = len([j for j in list_of_joins if j.args.get('kind', '').upper() == 'LEFT'])
        num_right_joins = len([j for j in list_of_joins if j.args.get('kind', '').upper() == 'RIGHT'])
        num_full_outer_joins = len([
            j for j in list_of_joins
            if j.args.get('kind', '').replace(" ", "").upper() in ['FULLOUTER', 'FULL_OUTER']
        ])

        # Misc
        has_limit = 1.0 if stmt.find(exp.Limit) else 0.0
        has_union = 1.0 if stmt.find(exp.Union) else 0.0
        num_union_operations = count_union_operations(stmt)
        num_case_statements = count_case_statements(stmt)

        # Build the feature dictionary (25 structural features)
        feats = {
            "num_tables": num_tables,
            "num_joins": num_joins,
            "num_predicates": num_predicates,
            "has_group_by": has_group_by,
            "has_order_by": has_order_by,
            "has_subquery": has_subquery,
            "query_length": query_length,
            "num_tokens": num_tokens,
            "num_select_expressions": num_select_expressions,
            "has_distinct": has_distinct,
            "num_aggregate_functions": num_aggregate_functions,
            "num_logical_operators": num_logical_operators,
            "num_comparison_operators": num_comparison_operators,
            "num_group_by_columns": num_group_by_columns,
            "num_order_by_columns": num_order_by_columns,
            "num_nested_subqueries": num_nested_subqueries,
            "has_correlated_subqueries": has_correlated_subqueries,
            "num_inner_joins": num_inner_joins,
            "num_left_joins": num_left_joins,
            "num_right_joins": num_right_joins,
            "num_full_outer_joins": num_full_outer_joins,
            "has_limit": has_limit,
            "has_union": has_union,
            "num_union_operations": num_union_operations,
            "num_case_statements": num_case_statements
        }

        # Add 1 semantic feature (requires a loaded SentenceTransformer model)
        _load_semantic_model()
        semantic_value = get_semantic_embedding(query_str)
        feats["semantic_embed_mean"] = semantic_value

        logger.debug(f"Parsed SQL features: {feats}")
        return feats

    except Exception as e:
        logger.error(f"Error parsing SQL: {e}")
        # Return a default feature set with zeros, plus semantic feature=0
        empty_feats = {
            "num_tables": 0,
            "num_joins": 0,
            "num_predicates": 0,
            "has_group_by": 0.0,
            "has_order_by": 0.0,
            "has_subquery": 0.0,
            "query_length": 0,
            "num_tokens": 0,
            "num_select_expressions": 0,
            "has_distinct": 0.0,
            "num_aggregate_functions": 0,
            "num_logical_operators": 0,
            "num_comparison_operators": 0,
            "num_group_by_columns": 0,
            "num_order_by_columns": 0,
            "num_nested_subqueries": 0,
            "has_correlated_subqueries": 0.0,
            "num_inner_joins": 0,
            "num_left_joins": 0,
            "num_right_joins": 0,
            "num_full_outer_joins": 0,
            "has_limit": 0.0,
            "has_union": 0.0,
            "num_union_operations": 0,
            "num_case_statements": 0,
            "semantic_embed_mean": 0.0
        }
        return empty_feats

# ---------------- Feature Preparation ----------------

def prepare_features(feats_dict):
    """
    Prepare features for prediction by extracting raw feature values.
    Now includes the semantic feature as the 26th dimension.
    """
    feature_order = [
        "num_tables",
        "num_joins",
        "num_predicates",
        "has_group_by",
        "has_order_by",
        "has_subquery",
        "query_length",
        "num_tokens",
        "num_select_expressions",
        "has_distinct",
        "num_aggregate_functions",
        "num_logical_operators",
        "num_comparison_operators",
        "num_group_by_columns",
        "num_order_by_columns",
        "num_nested_subqueries",
        "has_correlated_subqueries",
        "num_inner_joins",
        "num_left_joins",
        "num_right_joins",
        "num_full_outer_joins",
        "has_limit",
        "has_union",
        "num_union_operations",
        "num_case_statements",
        # New semantic feature appended here
        "semantic_embed_mean"
    ]
    raw_features = [feats_dict.get(key, 0.0) for key in feature_order]
    return np.array(raw_features, dtype=np.float32).reshape(1, -1)

# ---------------- Hint Matrix Construction ----------------

def build_hint_matrix_from_combos(combo_strs, hint_list):
    """
    Build a hint matrix from combination strings and a list of hints.
    """
    N = len(combo_strs)
    D = len(hint_list)
    Y = np.zeros((N, D), dtype=float)
    for i, c_str in enumerate(combo_strs):
        items = [x.strip() for x in c_str.split(",") if x.strip()]
        for item in items:
            enable_str = "enable_" + item
            if enable_str in hint_list:
                j = hint_list.index(enable_str)
                Y[i, j] = 1.0
    return Y

# ---------------- Inductive Matrix Completion Model Class ----------------

class InductiveMatrixCompletionModel:
    """
    Inductive Matrix Completion Model with Residuals and All Hint Combinations Cost Predictions.
    """
    def __init__(self, W, H, residual_model, poly):
        """
        Initialize the IMC model with model matrices and residual components.

        Parameters:
        ----------
        W : numpy.ndarray
            Matrix W from IMC model (shape: [n_query_features, k]).
        H : numpy.ndarray
            Matrix H from IMC model (shape: [n_hint_features, k]).
        residual_model : sklearn estimator
            Trained residual model (e.g., XGBoost or similar).
        poly : sklearn transformer
            Polynomial feature transformer (or similar).
        """
        # Z = W @ H.T => shape: [n_query_features, n_hint_features]
        self.Z = W @ H.T
        self.residual_model = residual_model
        self.poly = poly

    def predict_all_combos(self, x_feat, Y):
        """
        Predict costs for all hint combinations across multiple queries.

        Parameters:
        ----------
        x_feat : np.ndarray
            Feature vectors for the queries, shape: (N, n_query_features).
        Y : np.ndarray
            Hint combination matrix, shape: (M, n_hint_features).

        Returns:
        -------
        np.ndarray
            Predicted costs for each combination and query, shape: (N*M,).
        """
        # (N, n_query_features) x (n_query_features, n_hint_features) => (N, n_hint_features)
        xZ = x_feat @ self.Z  
        # Then multiply (N, n_hint_features) by Y.T => shape: (N, M)
        initial_costs = xZ @ Y.T

        # Prepare features for residual prediction
        N = x_feat.shape[0]
        M = Y.shape[0]

        # Repeat query features for each hint combination
        x_feat_repeated = np.repeat(x_feat, M, axis=0)  # shape: (N*M, n_query_features)

        # Tile hint combos for each query
        Y_expanded = np.tile(Y, (N, 1))  # shape: (N*M, n_hint_features)

        # Concatenate
        features = np.hstack((x_feat_repeated, Y_expanded))  # shape: (N*M, n_query_features + n_hint_features)

        # Apply polynomial transform
        features_poly = self.poly.transform(features)  # shape: (N*M, ?)

        # Predict residuals
        residuals_pred = self.residual_model.predict(features_poly)  # shape: (N*M,)

        # Adjust initial costs with residuals
        adjusted_costs = initial_costs.flatten() + residuals_pred  # shape: (N*M,)
        return adjusted_costs

# ---------------- PostgreSQL Hint Application Functions ----------------

def apply_plan_hints(cursor, plan_vector):
    """
    Apply hints to the PostgreSQL session based on the plan vector.
    """
    for i, hint_name in enumerate(HINTS):
        val = plan_vector[i]
        cmd = f"SET {hint_name} TO {'ON' if val >= 0.5 else 'OFF'};"
        cursor.execute(cmd)

def reset_all_hints(cursor):
    """
    Reset all hints to their default settings.
    """
    cursor.execute("RESET ALL;")

# ---------------- EXPLAIN ANALYZE Parsing Function ----------------

def parse_explain_analyze_output(explain_output):
    """
    Parse EXPLAIN ANALYZE output and extract execution time in seconds.
    Returns infinity if no time is found.
    """
    total_time = None
    for row in explain_output:
        line = row[0]
        match_ms = re.search(r"Execution Time:\s+([\d.]+)\s+ms", line)
        if match_ms:
            total_time_ms = float(match_ms.group(1))
            total_time = total_time_ms / 1000.0
            break

    if total_time is None:
        for row in explain_output:
            line = row[0]
            match_s = re.search(r"Execution Time:\s+([\d.]+)\s+s", line)
            if match_s:
                total_time = float(match_s.group(1))
                break

    if total_time is None:
        logger.warning("Could not parse Execution Time from EXPLAIN ANALYZE output.")
        return float("inf")
    return total_time

# ---------------- Query Execution Function ----------------

def run_query_postgres_once_explain_analyze(query_str, plan_vector, pg_host, pg_db, pg_user, pg_password, port=5432):
    """
    Execute a query with given hints and retrieve execution time using EXPLAIN ANALYZE.
    """
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host, dbname=pg_db, user=pg_user,
            password=pg_password, port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            apply_plan_hints(cur, plan_vector)
            cur.execute(f"EXPLAIN ANALYZE {query_str}")
            explain_output = cur.fetchall()
            execution_time = parse_explain_analyze_output(explain_output)
            return execution_time
    except Exception as e:
        logger.error(f"Error executing EXPLAIN ANALYZE: {e}")
        return float("inf")
    finally:
        if conn:
            # Ensure we reset hints no matter what
            try:
                with conn.cursor() as cur:
                    reset_all_hints(cur)
            except Exception as e:
                logger.error(f"Error resetting hints: {e}")
            conn.close()

# ---------------- Main Inference Function ----------------

def main():
    """
    Main function to perform inference on SQL queries, predict optimal hints,
    execute queries with selected hints, and save the results.
    """
    # Configuration Parameters
    MODEL_DIR = "/Users/raahimlone/New_Data"
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results_imc_25feat.csv"
    PG_HOST = "localhost"
    PG_DB = "IMDB"
    PG_USER = "postgres"
    PG_PASSWORD = "raahimhere"
    PG_PORT = 6543

    # ---------------- Load IMC Model Components ----------------
    try:
        W = np.load(os.path.join(MODEL_DIR, "U_best.npy"))  # shape: (26, K) if you have 26 features now
        H = np.load(os.path.join(MODEL_DIR, "V_best.npy"))  # shape: (len(HINTS), K)
        logger.info("✅ Inductive Matrix Completion Model Loaded.")
    except Exception as e:
        logger.error(f"Model load error: {e}")
        return

    # ---------------- Load Residual Model Components ----------------
    try:
        residual_model = joblib.load(os.path.join(MODEL_DIR, "residual_model_xgb.pkl"))
        poly = joblib.load(os.path.join(MODEL_DIR, "residual_model_poly.pkl"))
        logger.info("✅ Residual Model and Polynomial Transformer Loaded.")
    except FileNotFoundError:
        logger.error("Residual model or polynomial transformer not found. Ensure they are saved correctly.")
        return
    except Exception as e:
        logger.error(f"Error loading residual model components: {e}")
        return

    # ---------------- Initialize IMC Model with Residual Components ----------------
    try:
        imc_model = InductiveMatrixCompletionModel(W, H, residual_model, poly)
        logger.info("✅ Inductive Matrix Completion Model with Residuals Initialized.")
    except Exception as e:
        logger.error(f"Error initializing IMC model with residuals: {e}")
        return

    # ---------------- Build Hint Matrix ----------------
    Y = build_hint_matrix_from_combos(COMBO_STRS, HINTS)
    logger.info(f"Hint matrix built with shape: {Y.shape}")

    # ---------------- Feature Collection Phase ----------------
    logger.info(f"Starting feature extraction from SQL queries in {QUERIES_DIR}")
    all_features = []
    filenames = []

    for fname in os.listdir(QUERIES_DIR):
        if not fname.endswith(".sql"):
            continue

        full_path = os.path.join(QUERIES_DIR, fname)
        try:
            with open(full_path, 'r', encoding="utf-8") as f:
                query_str = f.read().strip()

            if not query_str:
                logger.warning(f"{fname} - Empty SQL file.")
                continue

            # Parse and extract features (25 structural + 1 semantic)
            feats = parse_sql(query_str)
            if not feats:
                logger.warning(f"{fname} - Failed to extract features.")
                continue

            all_features.append(feats)
            filenames.append(fname)

            logger.debug(f"Extracted features for {fname}: {feats}")

        except Exception as e:
            logger.error(f"Error reading or parsing {fname}: {e}")

    if not all_features:
        logger.warning("No query features extracted. Exiting.")
        print("\nNo results to display.")
        return

    # ---------------- Scaling Phase ----------------
    logger.info("Converting extracted features to DataFrame for scaling.")
    df_features = pd.DataFrame(all_features)

    # Handle missing columns by filling with zeros (optional)
    df_features.fillna(0, inplace=True)

    logger.info(f"Extracted features for {df_features.shape[0]} queries with {df_features.shape[1]} features.")

    # Initialize Min-Max Scaler with range [0, 4]
    scaler = MinMaxScaler(feature_range=(0, 4))
    logger.info("Fitting Min-Max Scaler on the extracted features.")

    # Fit the scaler and transform the features
    scaled_features = scaler.fit_transform(df_features)
    logger.info("Applied Min-Max scaling to the features.")

    # Convert scaled features back to DataFrame for consistency
    df_scaled_features = pd.DataFrame(scaled_features, columns=df_features.columns)

    # ---------------- Prediction Phase ----------------
    logger.info("Starting cost prediction for all hint combinations.")

    # Convert scaled features to NumPy array (N, 26)
    X_scaled = df_scaled_features.to_numpy()

    # Make predictions: shape => (N*M,)
    adjusted_costs = imc_model.predict_all_combos(X_scaled, Y)

    logger.info("Completed cost predictions.")

    # ---------------- Hint Selection Phase ----------------
    logger.info("Selecting best hint combinations based on predicted costs.")
    N_queries = X_scaled.shape[0]
    M_combos = Y.shape[0]

    adjusted_costs_reshaped = adjusted_costs.reshape(N_queries, M_combos)  # shape: (N, M)
    best_indices = np.argmin(adjusted_costs_reshaped, axis=1)  # shape: (N,)
    best_combos = [COMBO_STRS[idx] for idx in best_indices]
    best_Y_vectors = Y[best_indices]  # shape: (N, len(HINTS))

    # ---------------- Query Execution Phase ----------------
    logger.info("Starting query execution with selected hints.")
    results = []

    for i, fname in enumerate(filenames):
        query_str = ""
        full_path = os.path.join(QUERIES_DIR, fname)
        try:
            with open(full_path, 'r', encoding="utf-8") as f:
                query_str = f.read().strip()

            if not query_str:
                logger.warning(f"{fname} - Empty SQL file.")
                continue

            best_idx = best_indices[i]
            best_combo = best_combos[i]
            best_Y = best_Y_vectors[i]

            # Execute query with selected hints
            latency = run_query_postgres_once_explain_analyze(
                query_str, best_Y, PG_HOST, PG_DB, PG_USER, PG_PASSWORD, PG_PORT
            )

            # Extract enabled hints
            enabled_hints = [
                HINTS[j].replace("enable_", "")
                for j in range(len(HINTS))
                if best_Y[j] >= 0.5
            ]

            predicted_cost = adjusted_costs_reshaped[i, best_idx]

            results.append({
                "filename": fname,
                "best_idx": best_idx,
                "best_combo": best_combo,
                "predicted_cost": predicted_cost,
                "latency": latency,
                "hints": ",".join(enabled_hints)
            })

            logger.info(
                f"Processed {fname}: Best Hints - {enabled_hints}, "
                f"Predicted Cost - {predicted_cost:.4f}, Latency - {latency:.4f}s"
            )

        except Exception as e:
            logger.error(f"Error executing query {fname}: {e}")

    # ---------------- Save Results to CSV ----------------
    try:
        if not results:
            logger.warning("No results to save. The results list is empty.")
            print("\nNo results to display.")
            return

        df_results = pd.DataFrame(results)
        columns_order = ["filename", "best_idx", "best_combo", "predicted_cost", "latency", "hints"]
        df_results = df_results[columns_order]

        df_results.to_csv(OUTPUT_CSV, index=False)
        logger.info(f"✅ Results saved to {OUTPUT_CSV}")
        print("\nResults Summary:")
        print(df_results.describe())

    except Exception as e:
        logger.error(f"Error saving results to CSV: {e}")

# ---------------- Entry Point ----------------

if __name__ == "__main__":
    main()
