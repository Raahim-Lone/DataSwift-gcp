#!/usr/bin/env python3
"""
Full Script Extended for Query Embeddings + Structural Features, 
with Min-Max Scaling (0 to 4).
"""

import os
import sys
import logging
import argparse
import pandas as pd
import numpy as np

# 1) Import the SentenceTransformer class
from sentence_transformers import SentenceTransformer

from sklearn.preprocessing import MinMaxScaler
import joblib

# ---------------- Configure environment ----------------
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()

# Import from your local modules AFTER environment config
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config

# Import your parse_sql function (with structural features) from the correct location
from src.data_io.sql_parser3 import parse_sql

def main():
    parser = argparse.ArgumentParser(description="Parse SQL queries with structural + semantic embeddings.")
    parser.add_argument("--config", type=str, default="config/default_config.yaml", 
                        help="Path to configuration YAML file.")
    args = parser.parse_args()

    # Load your YAML config
    config = load_config(args.config)

    # Setup logger
    logger = setup_logger(
        __name__,
        config["logging"]["log_file"],
        level=getattr(logging, config["logging"]["log_level"].upper(), logging.INFO),
        max_bytes=config["logging"].get("max_bytes", 10485760),
        backup_count=config["logging"].get("backup_count", 5)
    )

    # The path containing .sql files
    query_path = config["database"]["query_path"]
    # The folder where processed features will be saved
    processed_path = config["paths"]["processed_features"]

    logger.info(f"Parsing queries from {query_path}")

    # 2) Initialize your sentence transformer model
    #    You can pick another model from https://www.sbert.net/docs/pretrained_models.html
    embed_model = SentenceTransformer('all-MiniLM-L6-v2')

    all_feature_dicts = []
    filenames = [f for f in os.listdir(query_path) if f.endswith(".sql")]

    if not filenames:
        logger.warning("No .sql files found in query_path.")
        sys.exit(0)

    for fname in filenames:
        full_path = os.path.join(query_path, fname)
        with open(full_path, "r", encoding="utf-8") as f:
            query_str = f.read().strip()

            if not query_str:
                logger.warning(f"Empty query in file: {fname}")
                continue

            # (A) Structural features
            feats = parse_sql(query_str)
            if not feats:
                logger.warning(f"Failed to extract structural features from file: {fname}")
                continue

            # (B) Text embedding
            embedding_vector = embed_model.encode(query_str, convert_to_numpy=True)
            # embedding_vector is typically length 384 for 'all-MiniLM-L6-v2'

            # Convert embedding to dict so we can merge with structural feats
            # We'll label each dimension embed_0, embed_1, ..., embed_383
            embedding_size = len(embedding_vector)
            embedding_features = {
                f"embed_{i}": float(embedding_vector[i]) for i in range(embedding_size)
            }

            # Combine structural feats + embedding feats into one dict
            # This can create a large dimension if your embedding is 384 or 768
            combined_feats = {**feats, **embedding_features}

            all_feature_dicts.append(combined_feats)

    if not all_feature_dicts:
        logger.warning("No query features extracted (structural + embeddings).")
        sys.exit(0)

    # Convert list of dicts to DataFrame
    df_features = pd.DataFrame(all_feature_dicts)
    df_features.fillna(0, inplace=True)

    logger.info(
        f"Extracted combined (structural + embedding) features for {df_features.shape[0]} queries, "
        f"with {df_features.shape[1]} total feature dimensions."
    )

    # 3) Min-Max Scaler with range [0, 4] (applied to ALL features: structural + embedding)
    scaler = MinMaxScaler(feature_range=(0, 4))
    logger.info("Fitting Min-Max Scaler on the extracted features.")

    # Fit & transform
    scaled_features = scaler.fit_transform(df_features)
    df_scaled_features = pd.DataFrame(scaled_features, columns=df_features.columns)

    # 4) Save everything
    os.makedirs(processed_path, exist_ok=True)

    # Optionally as Parquet
    parquet_output_file = os.path.join(processed_path, "X_scaled_embeddings.parquet")
    df_scaled_features.to_parquet(parquet_output_file, index=False)
    logger.info(f"Saved scaled query features (with embeddings) to {parquet_output_file}")

    # Also as NPZ
    npz_output_file = os.path.join(processed_path, "X_scaled_embeddings.npz")
    np.savez(npz_output_file, data=df_scaled_features.to_numpy())
    logger.info(f"Saved scaled query features (with embeddings) to {npz_output_file}")

    # Save the scaler
    scaler_output_file = os.path.join(processed_path, "scaler_embeddings.joblib")
    joblib.dump(scaler, scaler_output_file)
    logger.info(f"Saved Min-Max scaler (for embeddings) to {scaler_output_file}")

    # Save column names
    columns_output_file = os.path.join(processed_path, "feature_columns_embeddings.txt")
    with open(columns_output_file, "w") as f:
        for column in df_scaled_features.columns:
            f.write(f"{column}\n")
    logger.info(f"Saved feature column names to {columns_output_file}")

if __name__ == "__main__":
    main()
