#!/usr/bin/env python3
"""
execute_queries.py
A simplified script to execute SQL queries from a specified directory against a PostgreSQL database
and measure their execution latency using EXPLAIN ANALYZE.
"""

import os
import sys
import logging
import psycopg2
import re
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def run_query_postgres_explain_analyze(query_str, pg_host, pg_db, pg_user, pg_password, port=5432):
    """
    Executes EXPLAIN ANALYZE on the given query and returns the total execution time in seconds.
    """
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host,
            dbname=pg_db,
            user=pg_user,
            password=pg_password,
            port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            explain_query = f"EXPLAIN ANALYZE {query_str}"
            cur.execute(explain_query)
            explain_output = cur.fetchall()

            # Parse the execution time from EXPLAIN ANALYZE output
            execution_time = parse_explain_analyze_output(explain_output)
            return execution_time
    except Exception as e:
        logger.error(f"Error executing EXPLAIN ANALYZE: {e}")
        return float("inf")  # Return a large number to indicate failure
    finally:
        if conn:
            conn.close()

def parse_explain_analyze_output(explain_output):
    """
    Parses the EXPLAIN ANALYZE output to extract the total execution time in seconds.
    """
    total_time = None
    # EXPLAIN ANALYZE output is a list of tuples with one string element each
    for row in explain_output:
        line = row[0]
        # Look for the line that starts with "Execution Time:"
        match = re.search(r"Execution Time:\s+([\d.]+) ms", line)
        if match:
            total_time_ms = float(match.group(1))
            total_time = total_time_ms / 1000.0  # Convert to seconds
            break
    if total_time is None:
        logger.warning("Could not find Execution Time in EXPLAIN ANALYZE output.")
        return float("inf")  # Indicate failure to parse
    return total_time

def main():
    # Configuration
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"  # Update this path
    OUTPUT_CSV = "/Users/raahimlone/rahhh/Data_Gathering/results.csv"    # Update this path

    PG_HOST = "localhost"
    PG_DB = "IMDB"      # Update with your database name
    PG_USER = "postgres"    # Update with your PostgreSQL username
    PG_PASSWORD = "raahimlhere"  # Update with your PostgreSQL password
    PG_PORT = 6543              # Update if your PostgreSQL runs on a different port

    # Validate Queries Directory
    if not os.path.isdir(QUERIES_DIR):
        logger.error(f"Queries directory not found: {QUERIES_DIR}")
        sys.exit(1)

    # Gather SQL Files
    sql_files = [f for f in os.listdir(QUERIES_DIR) if f.endswith(".sql")]
    sql_files.sort()
    logger.info(f"Found {len(sql_files)} SQL files to process.")

    if not sql_files:
        logger.warning("No SQL files found to process.")
        sys.exit(0)

    results = []

    for fname in sql_files:
        fpath = os.path.join(QUERIES_DIR, fname)
        logger.info(f"Processing {fname}...")
        try:
            with open(fpath, "r", encoding="utf-8") as ff:
                query_str = ff.read().strip()
            if not query_str:
                logger.warning(f"{fname} => Empty query.")
                results.append({
                    "filename": fname,
                    "latency_s": None,
                    "error": "Empty query."
                })
                continue

            # Execute EXPLAIN ANALYZE
            latency = run_query_postgres_explain_analyze(
                query_str=query_str,
                pg_host=PG_HOST,
                pg_db=PG_DB,
                pg_user=PG_USER,
                pg_password=PG_PASSWORD,
                port=PG_PORT
            )

            if latency == float("inf"):
                logger.warning(f"{fname} => Failed to measure latency.")
                results.append({
                    "filename": fname,
                    "latency_s": None,
                    "error": "Failed to execute query or parse latency."
                })
                continue

            logger.info(
                f"{fname} => latency={latency:.4f}s"
            )
            results.append({
                "filename": fname,
                "latency_s": latency,
                "error": None
            })

        except Exception as e:
            logger.error(f"Error processing {fname}: {e}")
            results.append({
                "filename": fname,
                "latency_s": None,
                "error": str(e)
            })

    # Save Results to CSV
    if OUTPUT_CSV and OUTPUT_CSV.strip():
        try:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_CSV, index=False)
            logger.info(f"Results saved to {OUTPUT_CSV}")
        except Exception as e:
            logger.error(f"Failed to save results to CSV: {e}")

    # Print Summary
    if results:
        print("\n=== Query Execution Results ===")
        for r in results:
            if r["error"]:
                print(f"{r['filename']} => Error: {r['error']}")
            else:
                print(
                    f"{r['filename']} => Latency: {r['latency_s']:.4f} seconds"
                )
    else:
        print("No results to display.")

    logger.info("All queries processed.")

if __name__ == "__main__":
    main()
