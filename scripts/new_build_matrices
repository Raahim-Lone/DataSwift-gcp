#!/usr/bin/env python3
"""
Script to build feature matrices and association matrices for Inductive Matrix Completion.

Changes in this version:
1. The CSV dataset is read from '/Users/raahimlone/rahhh/output.csv' (full dataset).
2. Query features are loaded from 'X_scalednew.parquet' (instead of the previous file).
"""
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()


import os
import sys
import pandas as pd
import numpy as np
from scipy.sparse import coo_matrix, save_npz
import argparse
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config
from sklearn.preprocessing import MinMaxScaler  # Using MinMaxScaler
import logging

def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

configure_environment()

def build_feature_matrices(config):
    """
    Build feature matrices and association matrices for Inductive Matrix Completion.

    Parameters:
    - config: Configuration dictionary
    """
    logger = setup_logger(
        __name__,
        config["logging"]["log_file"],
        level=getattr(logging, config["logging"]["log_level"].upper(), logging.INFO)
    )

    # Paths
    # Override the CSV file to use the full dataset from the new location.
    csv_file = "/Users/raahimlone/rahhh/output.csv"
    output_path = config["paths"]["processed_features"]
    os.makedirs(output_path, exist_ok=True)

    # Load CSV data
    try:
        csv_data = pd.read_csv(csv_file, na_values=['', ' ', 'nan'], keep_default_na=True)
        logger.info(f"Loaded CSV data from {csv_file} with shape {csv_data.shape}")
    except Exception as e:
        logger.error(f"Error loading CSV file {csv_file}: {e}")
        exit(1)

    required_columns = {"query", "hint", "latency_postgresql_seconds"}
    if not required_columns.issubset(csv_data.columns):
        logger.error(f"CSV file must contain columns: {required_columns}")
        exit(1)

    # Map queries and hints to indices
    queries = csv_data["query"].unique()
    hints = csv_data["hint"].unique()
    query_to_idx = {q: i for i, q in enumerate(queries)}
    hint_to_idx = {h: i for i, h in enumerate(hints)}

    # Save query and hint mappings
    queries_df = pd.DataFrame({"query": queries})
    hints_df = pd.DataFrame({"hint": hints})
    queries_df.to_csv(os.path.join(output_path, "queries.csv"), index=False)
    hints_df.to_csv(os.path.join(output_path, "hints.csv"), index=False)
    logger.info(f"Saved queries.csv and hints.csv to {output_path}")

    # Load query and hint features
    # Load query features from X_scalednew.parquet instead of the previous file.
    queries_features_file = os.path.join(output_path, "X_scalednew.parquet")
    hints_features_file = config["paths"]["hints_features"]

    try:
        if queries_features_file.endswith('.parquet'):
            X_df = pd.read_parquet(queries_features_file)
        else:
            X_df = pd.read_csv(queries_features_file)
        logger.info(f"Loaded query features from {queries_features_file} with shape {X_df.shape}")
    except Exception as e:
        logger.error(f"Error loading query features from {queries_features_file}: {e}")
        exit(1)

    try:
        if hints_features_file.endswith('.parquet'):
            Y_df = pd.read_parquet(hints_features_file)
        else:
            Y_df = pd.read_csv(hints_features_file)
        logger.info(f"Loaded hint features from {hints_features_file} with shape {Y_df.shape}")
    except Exception as e:
        logger.error(f"Error loading hint features from {hints_features_file}: {e}")
        exit(1)

    # Convert to NumPy arrays
    X = X_df.values
    Y = Y_df.values
    logger.info(f"Converted query features to NumPy array with shape {X.shape}")
    logger.info(f"Converted hint features to NumPy array with shape {Y.shape}")

    # Scale features using Min-Max Scaler
    scaler_X = MinMaxScaler(feature_range=(0, 10))  # Using Min-Max Scaling
    scaler_Y = MinMaxScaler(feature_range=(0, 10))  # Using Min-Max Scaling

    X_scaled = scaler_X.fit_transform(X)
    Y_scaled = scaler_Y.fit_transform(Y)
    logger.info("Feature matrices X and Y have been scaled using MinMaxScaler to [0, 1].")

    # Save scaled feature matrices as .npz
    np.savez(os.path.join(output_path, "X.npz"), X=X_scaled)
    np.savez(os.path.join(output_path, "Y.npz"), Y=Y_scaled)
    logger.info(f"Saved scaled feature matrices as X.npz and Y.npz to {output_path}")

    # Create association matrix W and mask matrix M
    N_q = len(queries)
    N_h = len(hints)
    W = np.zeros((N_q, N_h))  # Workload matrix with 0 for unobserved entries
    M = np.zeros((N_q, N_h))  # Mask matrix indicating observed entries

    for _, row in csv_data.iterrows():
        i = query_to_idx[row['query']]
        j = hint_to_idx[row['hint']]
        val = row['latency_postgresql_seconds']
        if pd.notna(val):
            W[i, j] = val  # Set observed latency
            M[i, j] = 1    # Mark as observed
        else:
            W[i, j] = 0
            M[i, j] = 0

    # Save W as a dense matrix
    np.save(os.path.join(output_path, "W.npy"), W)
    logger.info(f"Saved workload matrix W.npy to {output_path}")

    # Save mask matrix M as a sparse matrix
    M_sparse = coo_matrix(M)
    save_npz(os.path.join(output_path, "M.npz"), M_sparse)
    logger.info(f"Saved mask matrix M.npz to {output_path}")

    # New Addition: Save M as a Dense Matrix (omega.npy)
    try:
        omega_path = os.path.join(output_path, "omega.npy")
        np.save(omega_path, M)
        logger.info(f"Saved dense mask matrix as omega.npy to {output_path}")
    except Exception as e:
        logger.error(f"Error saving omega.npy: {e}")
        exit(1)

    # Verification logs
    logger.info(f"Unique values in M: {np.unique(M)}")  # Should be [0, 1]
    logger.info(f"Number of observed entries: {int(np.sum(M))} / {N_q * N_h}")
    logger.info("Feature matrices and association matrices have been successfully built and saved.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build Feature and Association Matrices for IMC")
    parser.add_argument("--config", type=str, default="config/default_config.yaml", help="Path to config file")
    args = parser.parse_args()

    config = load_config(args.config)
    build_feature_matrices(config)
