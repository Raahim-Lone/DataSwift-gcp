# parse_queries_debug.py

#!/usr/bin/env python3
"""
Full Script Combining:
1) Reading queries -> extracting exactly 60 features from parse_sql.py
2) Manual min–max scaling => x_feat_60 shape (60,).
3) IMC model with W(60,5) & H(6,5).
4) Polynomial feature expansion => 2,000+ features for the residual XGBoost model.
5) Query execution in PostgreSQL using EXPLAIN ANALYZE for latency measurements.
6) Optional: training an XGBoost residual model on real latencies vs. predicted cost.

DEBUG ENHANCEMENTS:
- Log the beginning of query strings to confirm correct file reading
- Print out the raw feature dictionary (from parse_sql_60_features)
- Print out the scaled feature vector (x_feat_60)
"""

def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from `src`
configure_environment()

import os
import sys
import time
import logging
import psycopg2
import numpy as np
import pandas as pd
import joblib
import re  # for parsing EXPLAIN ANALYZE output

# For polynomial expansion, scaling, and XGBoost (residual model)
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
import xgboost as xgb  # For residual modeling

# Import our parse_sql_60_features function
from src.data_io.sql_parser import parse_sql_60_features, FEATURE_SCHEMA_60

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

assert len(FEATURE_SCHEMA_60) == 60, "We want exactly 60 features."

class MaskedALSNonnegative:
    """
    W: shape (60,5)
    H: shape (6,5)
    """
    def __init__(self, W, H):
        # Ensure nonnegative (just for safety).
        self.W = np.clip(W, 0, None)  # (60,5)
        self.H = np.clip(H, 0, None)  # (6,5)
        self.logger = logging.getLogger(__name__)

    def predict_single(self, x_feat_60, y_feat_6):
        """
        x_feat_60: shape(60,)
        y_feat_6 : shape(6,)

        We first get:
            q_latent = x_feat_60 @ W  => shape (5,)
            h_latent = y_feat_6  @ H  => shape (5,)

        Then the final predicted cost is dot(q_latent, h_latent).
        """
        q_latent = x_feat_60 @ self.W      # (60,) x (60,5) => (5,)
        h_latent = y_feat_6 @ self.H       # (6,)  x (6,5)  => (5,)
        cost_pred = float(np.dot(q_latent, h_latent))  # scalar
        return cost_pred

HINTS = [
    "enable_hashjoin",
    "enable_indexonlyscan",
    "enable_indexscan",
    "enable_mergejoin",
    "enable_nestloop",
    "enable_seqscan"
]

# NOTE: Removed empty string from combos if you do NOT want a "no-hints" plan
COMBO_STRS = [
    "hashjoin,indexonlyscan",
    "hashjoin,indexonlyscan,indexscan",
    "hashjoin,indexonlyscan,indexscan,mergejoin",
    "hashjoin,indexonlyscan,indexscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,indexscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,indexscan,nestloop",
    "hashjoin,indexonlyscan,indexscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,indexscan,seqscan",
    "hashjoin,indexonlyscan,mergejoin",
    "hashjoin,indexonlyscan,mergejoin,nestloop",
    "hashjoin,indexonlyscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexonlyscan,mergejoin,seqscan",
    "hashjoin,indexonlyscan,nestloop",
    "hashjoin,indexonlyscan,nestloop,seqscan",
    "hashjoin,indexonlyscan,seqscan",
    "hashjoin,indexscan",
    "hashjoin,indexscan,mergejoin",
    "hashjoin,indexscan,mergejoin,nestloop",
    "hashjoin,indexscan,mergejoin,nestloop,seqscan",
    "hashjoin,indexscan,mergejoin,seqscan",
    "hashjoin,indexscan,nestloop",
    "hashjoin,indexscan,nestloop,seqscan",
    "hashjoin,indexscan,seqscan",
    "hashjoin,mergejoin,nestloop,seqscan",
    "hashjoin,mergejoin,seqscan",
    "hashjoin,nestloop,seqscan",
    "hashjoin,seqscan",
    "indexonlyscan,indexscan,mergejoin",
    "indexonlyscan,indexscan,mergejoin,nestloop",
    "indexonlyscan,indexscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,indexscan,mergejoin,seqscan",
    "indexonlyscan,indexscan,nestloop",
    "indexonlyscan,indexscan,nestloop,seqscan",
    "indexonlyscan,mergejoin",
    "indexonlyscan,mergejoin,nestloop",
    "indexonlyscan,mergejoin,nestloop,seqscan",
    "indexonlyscan,mergejoin,seqscan",
    "indexonlyscan,nestloop",
    "indexonlyscan,nestloop,seqscan",
    "indexscan,mergejoin",
    "indexscan,mergejoin,nestloop",
    "indexscan,mergejoin,nestloop,seqscan",
    "indexscan,mergejoin,seqscan",
    "indexscan,nestloop",
    "indexscan,nestloop,seqscan",
    "mergejoin,nestloop,seqscan",
    "mergejoin,seqscan",
    "nestloop,seqscan"
]

def build_hint_matrix_from_combos(combo_strs, hint_list):
    N = len(combo_strs)
    D = len(hint_list)
    Y = np.zeros((N, D), dtype=int)
    for i, c_str in enumerate(combo_strs):
        if not c_str.strip():
            continue
        items = [x.strip() for x in c_str.split(",")]
        for item in items:
            enable_str = "enable_" + item
            if enable_str in hint_list:
                j = hint_list.index(enable_str)
                Y[i, j] = 1
    return Y

def apply_plan_hints(cursor, plan_vector):
    for i, hint_name in enumerate(HINTS):
        val = plan_vector[i]
        cmd = f"SET {hint_name} TO {'ON' if val >= 0.5 else 'OFF'};"
        cursor.execute(cmd)

def reset_all_hints(cursor):
    cursor.execute("RESET ALL;")

def run_query_postgres_once_explain_analyze(query_str, plan_vector, pg_host, pg_db, pg_user, pg_password, port=5432):
    conn = None
    try:
        conn = psycopg2.connect(
            host=pg_host, dbname=pg_db, user=pg_user,
            password=pg_password, port=port
        )
        conn.autocommit = True
        with conn.cursor() as cur:
            apply_plan_hints(cur, plan_vector)
            explain_query = f"EXPLAIN ANALYZE {query_str}"
            cur.execute(explain_query)
            explain_output = cur.fetchall()
            reset_all_hints(cur)
            execution_time = parse_explain_analyze_output(explain_output)
            return execution_time
    except Exception as e:
        logger.error(f"Error executing EXPLAIN ANALYZE: {e}")
        return float("inf")
    finally:
        if conn:
            conn.close()

def parse_explain_analyze_output(explain_output):
    total_time = None
    for row in explain_output:
        line = row[0]
        match = re.search(r"Execution Time:\s+([\d.]+)\s+ms", line)
        if match:
            total_time_ms = float(match.group(1))
            total_time = total_time_ms / 1000.0
            break
    if total_time is None:
        logger.warning("Could not find Execution Time in EXPLAIN ANALYZE output.")
        return float("inf")
    return total_time

def prepare_single_query_features(x_feat_60, y_feat_6, poly, scaler):
    combined = np.hstack([x_feat_60, y_feat_6]).reshape(1, -1)
    feats_poly = poly.transform(combined)
    feats_scaled = scaler.transform(feats_poly)
    return feats_scaled

def train_xgboost_residual_model(X_residual, Y_residual):
    logger.info("Training XGBoost on residuals (actual_latency - predicted) ...")
    poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
    scaler = StandardScaler()

    X_poly = poly.fit_transform(X_residual)
    X_scaled = scaler.fit_transform(X_poly)

    xgb_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=500,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        verbosity=1,
        random_state=42
    )
    xgb_model.fit(X_scaled, Y_residual)
    logger.info("Residual model training complete.")
    return xgb_model, poly, scaler

def main():
    MODEL_DIR = "/Users/raahimlone/New_Data"
    QUERIES_DIR = "/Users/raahimlone/rahhh/Data_Gathering/raw_sql_queries"
    OUTPUT_CSV = "/Users/raahimlone/rahhh/results.csv"

    PG_HOST = "localhost"
    PG_DB = "IMDB"
    PG_USER = "postgres"
    PG_PASSWORD = "raahimlhere"
    PG_PORT = 6543

    # 1) Load IMC model: W(60,5) & H(6,5)
    try:
        W_path = os.path.join(MODEL_DIR, "U_best.npy")  
        H_path = os.path.join(MODEL_DIR, "V_best.npy")  
        W = np.load(W_path)  # shape (60,5)
        H = np.load(H_path)  # shape (6,5)
        logger.info(f"Loaded W: {W.shape}, H: {H.shape}")
    except Exception as e:
        logger.error(f"Could not load IMC model: {e}")
        return

    model_imc = MaskedALSNonnegative(W, H)

    # 2) Build Y combos => shape(N,6)
    Y = build_hint_matrix_from_combos(COMBO_STRS, HINTS)
    logger.info(f"Y combos => shape={Y.shape}")

    residual_model = None
    poly = None
    scaler = None

    X_residual_data = []
    y_residual_data = []

    # 4) Manual min–max scaling dictionary
    MANUAL_MAX = {
        "num_tables": 10,
        "total_joins": 50,
        "num_inner_joins": 50,
        "num_left_joins": 50,
        "num_right_joins": 50,
        "num_full_joins": 50,
        "num_cross_joins": 50,
        "num_semi_joins": 50,
        "num_anti_joins": 50,
        "num_other_joins": 50,
        "num_union": 10,
        "num_intersect": 10,
        "num_except": 10,
        "num_agg_count": 50,
        "num_agg_sum": 50,
        "num_agg_avg": 50,
        "num_agg_min": 50,
        "num_agg_max": 50,
        "num_agg_other": 50,
        "num_distinct": 10,
        "num_limit": 10,
        "num_offset": 10,
        "func_unknown": 50,  # etc...
    }

    def manual_min_max_scale(feats_dict_60):
        scaled = []
        for key in FEATURE_SCHEMA_60:
            raw_val = feats_dict_60.get(key, 0)
            max_val = MANUAL_MAX.get(key, 50)  # default=50
            if max_val > 0:
                scaled.append(min(raw_val, max_val) / max_val)
            else:
                scaled.append(0.0)
        return np.array(scaled, dtype=float)

    if not os.path.isdir(QUERIES_DIR):
        logger.error(f"Queries dir not found: {QUERIES_DIR}")
        return

    sql_files = [f for f in os.listdir(QUERIES_DIR) if f.endswith(".sql")]
    sql_files.sort()
    logger.info(f"Found {len(sql_files)} SQL files to process.")

    results = []
    for fname in sql_files:
        fpath = os.path.join(QUERIES_DIR, fname)
        logger.info(f"Processing {fname}...")

        try:
            with open(fpath, "r", encoding="utf-8") as ff:
                query_str = ff.read().strip()

            # DEBUG #1: Log a snippet of the query
            logger.info(f"[DEBUG] Query text for {fname} (first ~120 chars):\n{query_str[:120]!r}")

            if not query_str:
                logger.warning(f"{fname} => empty query.")
                continue

            # === Parse => EXACT 60 features
            feats_60_dict = parse_sql_60_features(query_str)

            # DEBUG #2: Log the returned feature dictionary
            logger.info(f"[DEBUG] Raw feature dict for {fname} => {feats_60_dict}")

            # === Scale => x_feat_60 (shape (60,))
            x_feat_60 = manual_min_max_scale(feats_60_dict)

            # DEBUG #3: Log the scaled feature vector
            logger.info(f"[DEBUG] Scaled features for {fname} => {x_feat_60}")

            # Evaluate each plan & pick the row-wise minimum predicted cost
            best_idx = None
            best_cost = float("inf")
            for i in range(Y.shape[0]):
                y_feat_6 = Y[i, :]
                cost_imc = model_imc.predict_single(x_feat_60, y_feat_6)

                # If we had a trained residual model:
                # big_feats = prepare_single_query_features(x_feat_60, y_feat_6, poly, scaler)
                # residual_pred = residual_model.predict(big_feats)[0]
                # final_cost = cost_imc + residual_pred

                final_cost = cost_imc

                if final_cost < best_cost:
                    best_cost = final_cost
                    best_idx = i

            if best_idx is None:
                logger.warning(f"{fname} => no feasible plan found!")
                continue

            chosen_plan = Y[best_idx, :]
            latency = run_query_postgres_once_explain_analyze(
                query_str=query_str,
                plan_vector=chosen_plan,
                pg_host=PG_HOST,
                pg_db=PG_DB,
                pg_user=PG_USER,
                pg_password=PG_PASSWORD,
                port=PG_PORT
            )
            if latency == float("inf") or latency <= 0:
                logger.warning(f"{fname} => invalid or failed latency measurement: {latency}")
                continue

            # Save data to train residual model later
            combined_66 = np.hstack([x_feat_60, chosen_plan])  # shape (66,)
            residual_val = latency - best_cost
            X_residual_data.append(combined_66)
            y_residual_data.append(residual_val)

            # Decode chosen hints
            enabled_hints = []
            for j, hint_name in enumerate(HINTS):
                if chosen_plan[j] == 1:
                    short = hint_name.replace("enable_", "")
                    enabled_hints.append(short)
            hint_str = ",".join(enabled_hints) if enabled_hints else "???"

            logger.info(
                f"{fname} => best_idx={best_idx}, cost={best_cost:.4f}, "
                f"latency={latency:.4f}s, hints=[{hint_str}]"
            )
            results.append({
                "filename": fname,
                "best_index": best_idx,
                "cost_estimate": best_cost,
                "latency_s": latency,
                "hints": hint_str
            })

        except Exception as e:
            logger.error(f"Error processing {fname}: {e}")
            results.append({"filename": fname, "error": str(e)})

    # (Optional) Train XGBoost residual model if we have enough data
    if len(X_residual_data) > 3:
        X_residual_data = np.array(X_residual_data)
        y_residual_data = np.array(y_residual_data, dtype=float)

        logger.info(f"Training residual model on {len(X_residual_data)} data points...")
        residual_model, poly, scaler = train_xgboost_residual_model(X_residual_data, y_residual_data)

        # Save model artifacts
        model_path = os.path.join(MODEL_DIR, "residual_model_trained.pkl")
        poly_path  = os.path.join(MODEL_DIR, "residual_model_poly_trained.pkl")
        scal_path  = os.path.join(MODEL_DIR, "residual_model_scaler_trained.pkl")

        joblib.dump(residual_model, model_path)
        joblib.dump(poly,        poly_path)
        joblib.dump(scaler,      scal_path)

        logger.info(
            f"Residual model + poly + scaler saved to:\n {model_path}\n {poly_path}\n {scal_path}"
        )
    else:
        logger.info("Not enough data to train residual model (need more queries).")

    # 7) Save results
    if OUTPUT_CSV and OUTPUT_CSV.strip():
        try:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_CSV, index=False)
            logger.info(f"Results saved => {OUTPUT_CSV}")
        except Exception as e:
            logger.error(f"Failed saving CSV => {e}")

    if results:
        print("\n=== Query Optimization Results ===")
        for r in results:
            if "error" in r:
                print(f"{r['filename']} => Error: {r['error']}")
            else:
                print(
                    f"{r['filename']} => best_index={r['best_index']}, "
                    f"cost={r['cost_estimate']:.4f}, latency={r['latency_s']:.4f}s, "
                    f"hints=[{r['hints']}]"
                )
    else:
        print("No results to display.")

    logger.info("All queries processed.")

    # Optional: visualize the Y matrix
    import matplotlib.pyplot as plt
    def visualize_hint_matrix(Y, best_idx):
        plt.figure(figsize=(10, 6))
        plt.imshow(Y, cmap='gray', aspect='auto')
        plt.colorbar(label='Hint Enabled (0=OFF, 1=ON)')
        plt.title('Hint Combination Matrix (Y)')
        plt.xlabel('Hint Types')
        plt.ylabel('Plan Index')
        plt.axhline(y=best_idx, color='red', linestyle='--', label='Best Plan')
        plt.legend()
        plt.show()

if __name__ == "__main__":
    main()
