# parse_queries.py
#!/usr/bin/env python3
"""
Full Script Modified for 6 Features with Min-Max Scaling (0 to 4)
"""

import os
import sys
import logging
import pandas as pd
import argparse
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import joblib

# Configure environment
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from `src`
configure_environment()

# Import after configuring the environment
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config
from src.data_io.sql_parser3 import parse_sql


def main():
    parser = argparse.ArgumentParser(description="Parse SQL queries and extract features with Min-Max scaling.")
    parser.add_argument("--config", type=str, default="config/default_config.yaml", help="Path to configuration YAML file.")
    args = parser.parse_args()

    # Load configuration
    config = load_config(args.config)

    # Setup logger
    logger = setup_logger(
        __name__,
        config["logging"]["log_file"],
        level=getattr(logging, config["logging"]["log_level"].upper(), logging.INFO),
        max_bytes=config["logging"].get("max_bytes", 10485760),
        backup_count=config["logging"].get("backup_count", 5)
    )

    query_path = config["database"]["query_path"]
    processed_path = config["paths"]["processed_features"]

    logger.info(f"Parsing queries from {query_path}")
    all_features = []
    filenames = [f for f in os.listdir(query_path) if f.endswith(".sql")]

    if not filenames:
        logger.warning("No .sql files found in query_path.")
        sys.exit(0)

    for fname in filenames:
        full_path = os.path.join(query_path, fname)
        with open(full_path, "r", encoding="utf-8") as f:
            query_str = f.read().strip()
            if not query_str:
                logger.warning(f"Empty query in file: {fname}")
                continue
            feats = parse_sql(query_str)
            if not feats:
                logger.warning(f"Failed to extract features from file: {fname}")
                continue
            all_features.append(feats)

    if not all_features:
        logger.warning("No query features extracted.")
        sys.exit(0)

    # Convert list of dicts to DataFrame
    df_features = pd.DataFrame(all_features)

    # Handle missing columns by filling with zeros (optional)
    df_features.fillna(0, inplace=True)

    # Log the shape of the features DataFrame
    logger.info(f"Extracted features for {df_features.shape[0]} queries with {df_features.shape[1]} features.")

    # Initialize Min-Max Scaler with range [0, 4]
    scaler = MinMaxScaler(feature_range=(0, 4))
    logger.info("Fitting Min-Max Scaler on the extracted features.")

    # Fit the scaler and transform the features
    scaled_features = scaler.fit_transform(df_features)
    logger.info("Applied Min-Max scaling to the features.")

    # Convert scaled features back to DataFrame for saving
    df_scaled_features = pd.DataFrame(scaled_features, columns=df_features.columns)

    os.makedirs(processed_path, exist_ok=True)

    # Save scaled features as Parquet
    parquet_output_file = os.path.join(processed_path, "X_scaled.parquet")
    df_scaled_features.to_parquet(parquet_output_file, index=False)
    logger.info(f"Saved scaled query features to {parquet_output_file}")

    # Save scaled features as NPZ
    npz_output_file = os.path.join(processed_path, "X_scaled.npz")
    np.savez(npz_output_file, data=df_scaled_features.to_numpy())
    logger.info(f"Saved scaled query features to {npz_output_file}")

    # Save scaler for future use (optional but recommended)
    scaler_output_file = os.path.join(processed_path, "scaler.joblib")
    joblib.dump(scaler, scaler_output_file)
    logger.info(f"Saved Min-Max scaler to {scaler_output_file}")

    # Optional: Save column names for future reference
    columns_output_file = os.path.join(processed_path, "feature_columns.txt")
    with open(columns_output_file, "w") as f:
        for column in df_scaled_features.columns:
            f.write(f"{column}\n")
    logger.info(f"Saved feature column names to {columns_output_file}")


if __name__ == "__main__":
    main()

'''
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from `src`
configure_environment()


import logging
import os
import pandas as pd
import argparse
import numpy as np
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config
from src.data_io.sql_parser2 import parse_sql


def main():
    parser = argparse.ArgumentParser(description="Parse SQL queries and extract features.")
    parser.add_argument("--config", type=str, default="config/default_config.yaml", help="Path to configuration YAML file.")
    args = parser.parse_args()

    config = load_config(args.config)
    logger = setup_logger(__name__, config["logging"]["log_file"],
                          level=getattr(logging, config["logging"]["log_level"].upper(), logging.INFO),
                          max_bytes=config["logging"].get("max_bytes", 10485760),
                          backup_count=config["logging"].get("backup_count", 5))

    query_path = config["database"]["query_path"]
    processed_path = config["paths"]["processed_features"]

    logger.info(f"Parsing queries from {query_path}")
    all_features = []
    filenames = [f for f in os.listdir(query_path) if f.endswith(".sql")]

    if not filenames:
        logger.warning("No .sql files found in query_path.")
        exit(0)

    for fname in filenames:
        full_path = os.path.join(query_path, fname)
        with open(full_path, "r", encoding="utf-8") as f:
            query_str = f.read().strip()
            if not query_str:
                logger.warning(f"Empty query in file: {fname}")
                continue
            feats = parse_sql(query_str)
            if not feats:
                logger.warning(f"Failed to extract features from file: {fname}")
                continue
            all_features.append(feats)

    if not all_features:
        logger.warning("No query features extracted.")
        exit(0)

    # Convert list of dicts to DataFrame
    df_features = pd.DataFrame(all_features)

    # Handle missing columns by filling with zeros (optional)
    df_features.fillna(0, inplace=True)

    # Log the shape of the features DataFrame
    logger.info(f"Extracted features for {df_features.shape[0]} queries with {df_features.shape[1]} features.")

    os.makedirs(processed_path, exist_ok=True)

    # Save as Parquet
    parquet_output_file = os.path.join(processed_path, "X.parquet")
    df_features.to_parquet(parquet_output_file, index=False)
    logger.info(f"Saved query features to {parquet_output_file}")

    # Save as NPZ
    npz_output_file = os.path.join(processed_path, "X.npz")
    np.savez(npz_output_file, data=df_features.to_numpy())
    logger.info(f"Saved query features to {npz_output_file}")

    # Optional: Save column names for future reference
    columns_output_file = os.path.join(processed_path, "feature_columns.txt")
    with open(columns_output_file, "w") as f:
        for column in df_features.columns:
            f.write(f"{column}\n")
    logger.info(f"Saved feature column names to {columns_output_file}")

if __name__ == "__main__":
    main()
'''
'''
def configure_environment():
    """Dynamically set up sys.path to include the project root."""
    import sys
    import os

    # Dynamically determine the project root and add it to sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    scripts_path = os.path.join(project_root, "scripts")

    # Remove conflicting paths
    if scripts_path in sys.path:
        sys.path.remove(scripts_path)

    # Add project root to sys.path
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

# Call environment setup before importing anything from `src`
configure_environment()

import os
import pandas as pd
import argparse
import logging
from src.utils.logger import setup_logger
from src.utils.config_manager import load_config
from src.data_io.sql_parser import parse_sql
import numpy as np  # Import NumPy for saving as npz

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config/default_config.yaml")
    args = parser.parse_args()

    config = load_config(args.config)
    logger = setup_logger(__name__, config["logging"]["log_file"],
                          level=getattr(logging, config["logging"]["log_level"]),
                          max_bytes=config["logging"].get("max_bytes", 10485760),
                          backup_count=config["logging"].get("backup_count",5))

    query_path = config["database"]["query_path"]
    processed_path = config["paths"]["processed_features"]

    logger.info(f"Parsing queries from {query_path}")
    all_features = []
    filenames = [f for f in os.listdir(query_path) if f.endswith(".sql")]

    if not filenames:
        logger.warning("No .sql files found in query_path.")
        exit(0)

    for fname in filenames:
        full_path = os.path.join(query_path, fname)
        with open(full_path, "r", encoding="utf-8") as f:
            query_str = f.read().strip()
            if not query_str:
                continue
            feats = parse_sql(query_str)
            vector = [
                feats["num_tables"],
                feats["num_joins"],
                feats["num_predicates"],
                feats["has_group_by"],
                feats["has_order_by"],
                feats["has_subquery"]
            ]
            all_features.append(vector)

    if not all_features:
        logger.warning("No query features extracted.")
        exit(0)

    columns = ["num_tables","num_joins","num_predicates","has_group_by","has_order_by","has_subquery"]
    df_features = pd.DataFrame(all_features, columns=columns)
    os.makedirs(processed_path, exist_ok=True)
    
    # Save as Parquet (optional, you can remove this if not needed)
    parquet_output_file = os.path.join(processed_path, "X.parquet")
    df_features.to_parquet(parquet_output_file, index=False)
    logger.info(f"Saved query features to {parquet_output_file}")

    # Save as Parquet (optional, you can remove this if not needed)
    parquet_output_file = os.path.join(processed_path, "X.parquet")
    df_features.to_parquet(parquet_output_file, index=False)
    logger.info(f"Saved query features to {parquet_output_file}")

    # Save as NPZ
    npz_output_file = os.path.join(processed_path, "X.npz")
    np.savez(npz_output_file, data=df_features.to_numpy())
    logger.info(f"Saved query features to {npz_output_file}")
    import os
    import numpy as np

    # Define the output path for NPZ
    npz_output_file = "/Users/raahimlone/New_Data/X.npz"

    # Ensure the directory exists
    os.makedirs(os.path.dirname(npz_output_file), exist_ok=True)

    # Save as NPZ
    np.savez(npz_output_file, data=df_features.to_numpy())
    logger.info(f"Saved query features to {npz_output_file}")
    import numpy as np
    loaded = np.load('/Users/raahimlone/New_Data/X.npz')
    print(loaded['data'])
'''